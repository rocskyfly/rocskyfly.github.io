<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>情感分析</title>
      <link href="/2017/09/17/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/"/>
      <url>/2017/09/17/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<p><strong>数据集说明：</strong>数据集来源于电影中的台词文本。文件positive.txt, negative.txt分别存储有5331条正面情感的台词文本数据，331条负面情感的台词文本数据。<br>&lt;/br&gt;<br><strong>程序说明：</strong>采用nltk处理文本数据，采用深层神经网络算法，训练模型，实现文本情感分析。在实践中有现成的NLP库可以处理，譬如TextBlob&lt;/br&gt;<br><strong>算法理论请参照：</strong><a href="https://rocskyfly.github.io/categories/4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">神经网络算法</a>&lt;/br&gt;<br><strong>Ipynb演示文件：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/SentimentAnalysis/SentimentNeuralNetwork.ipynb">Ipynb文件</a>&lt;/br&gt;<br><strong>Python代码：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/SentimentAnalysis/SentimentNeuralNetwork.py">Python代码</a>&lt;/br&gt;</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">import</span> codecs</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_lexicon</span>(<span class="params">pos, neg, hm_lines=<span class="number">10000000</span>, word_frequency_min=<span class="number">50</span>, word_frequency_max=<span class="number">1000</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    通过解析pos和neg文本，生成一个字典</span></span><br><span class="line"><span class="string">    :param pos: 积极文本。</span></span><br><span class="line"><span class="string">    :param neg: 消极文本。</span></span><br><span class="line"><span class="string">    :param hm_lines: 支持的最大行数。</span></span><br><span class="line"><span class="string">    :param word_frequency_min: 可放入字典中的单词在文本中出现的最小次数。</span></span><br><span class="line"><span class="string">    :param word_frequency_max: 可放入字典中的单词在文本中出现的最大次数。</span></span><br><span class="line"><span class="string">    :return: 返回生成的字典</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    lexicon = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> text_file <span class="keyword">in</span> [pos, neg]:</span><br><span class="line">        <span class="keyword">with</span> codecs.open(text_file, <span class="string">&#x27;r&#x27;</span>, <span class="string">&quot;latin-1&quot;</span>) <span class="keyword">as</span> f:  <span class="comment"># 输入文件是以latin编码的，需要用响应的解码器</span></span><br><span class="line">            lines = f.readlines()</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> lines[:hm_lines]:</span><br><span class="line">                all_words = word_tokenize(line.lower())  <span class="comment"># word_tokenize是分词器</span></span><br><span class="line">                lexicon += list(all_words)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 词规范化(还原词本身)：如把broken替换为break，buses替换成bus</span></span><br><span class="line">    lexicon = [WordNetLemmatizer().lemmatize(index) <span class="keyword">for</span> index <span class="keyword">in</span> lexicon]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Counter类的目的是用来跟踪值出现的次数。它是一个无序的容器类型，以字典的键值对形式存储，其中元素作为key，其计数作为value</span></span><br><span class="line">    w_counts = Counter(lexicon)</span><br><span class="line"></span><br><span class="line">    l2 = []</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> w_counts:</span><br><span class="line">        <span class="keyword">if</span> w_counts[w] <span class="keyword">in</span> range(word_frequency_min, word_frequency_max):</span><br><span class="line">            l2.append(w)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">&quot;The lexicon size is: %s&quot;</span> % len(l2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> l2</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_handling</span>(<span class="params">sample, hm_lines, lexicon, classification</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将sample文本，逐行转换成特征向量。</span></span><br><span class="line"><span class="string">    :param sample: 输入文本</span></span><br><span class="line"><span class="string">    :param hm_lines: 支持的最大行数。</span></span><br><span class="line"><span class="string">    :param lexicon: 字典</span></span><br><span class="line"><span class="string">    :param classification: 文本分类列表，如[1, 0]代表积极</span></span><br><span class="line"><span class="string">    :return: 返回特征向量集</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    features_set = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> codecs.open(sample, <span class="string">&#x27;r&#x27;</span>, <span class="string">&quot;latin-1&quot;</span>) <span class="keyword">as</span> sam:</span><br><span class="line">        lines = sam.readlines()</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines[:hm_lines]:</span><br><span class="line">            current_words = [WordNetLemmatizer().lemmatize(index)</span><br><span class="line">                             <span class="keyword">for</span> index <span class="keyword">in</span> word_tokenize(line.lower())]</span><br><span class="line">            features = np.zeros(len(lexicon))</span><br><span class="line">            <span class="keyword">for</span> w <span class="keyword">in</span> current_words:</span><br><span class="line">                <span class="keyword">if</span> w.lower() <span class="keyword">in</span> lexicon:</span><br><span class="line">                    index = lexicon.index(w.lower())</span><br><span class="line">                    features[index] += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            features = list(features)</span><br><span class="line"></span><br><span class="line">            features_set.append([features, classification])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> features_set</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_features_and_labels_set</span>(<span class="params">pos, neg, hm_lines=<span class="number">10000000</span>, word_frequency_min=<span class="number">50</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   word_frequency_max=<span class="number">1000</span>, test_size=<span class="number">0.2</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   dataset_name=<span class="string">&#x27;sentiment_dataset.pickle&#x27;</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    将文本处理成机器学习算法可以使用的特征向量数据集。</span></span><br><span class="line"><span class="string">    :param pos: 积极文本。</span></span><br><span class="line"><span class="string">    :param neg: 消极文本。</span></span><br><span class="line"><span class="string">    :param hm_lines: 支持的最大行数。</span></span><br><span class="line"><span class="string">    :param word_frequency_min: 可放入字典中的单词在文本中出现的最小次数。</span></span><br><span class="line"><span class="string">    :param word_frequency_max: 可放入字典中的单词在文本中出现的最大次数。</span></span><br><span class="line"><span class="string">    :param test_size: 测试集的比例大小设置</span></span><br><span class="line"><span class="string">    :param dataset_name: 获取数据集，将其保存的名称</span></span><br><span class="line"><span class="string">    :return: x_train, y_train, x_test, y_test</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(dataset_name):</span><br><span class="line">        <span class="keyword">print</span> <span class="string">&quot;loading dataset...&quot;</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">with</span> open(dataset_name, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> dataset:</span><br><span class="line">                <span class="keyword">return</span> pickle.load(dataset)</span><br><span class="line">        <span class="keyword">except</span> Exception, e:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">&quot;try to created again, due to: &#x27;%s&#x27;&quot;</span> % e</span><br><span class="line"></span><br><span class="line">    features = []</span><br><span class="line"></span><br><span class="line">    lexicon = create_lexicon(pos, neg, hm_lines, word_frequency_min, word_frequency_max)</span><br><span class="line">    features += sample_handling(pos, hm_lines, lexicon, [<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">    features += sample_handling(neg, hm_lines, lexicon, [<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    random.shuffle(features)</span><br><span class="line"></span><br><span class="line">    features = np.array(features)</span><br><span class="line"></span><br><span class="line">    test_size = int(test_size * len(features))</span><br><span class="line"></span><br><span class="line">    x_train = list(features[:, <span class="number">0</span>][:-test_size])</span><br><span class="line">    y_train = list(features[:, <span class="number">1</span>][:-test_size])</span><br><span class="line">    x_test = list(features[:, <span class="number">0</span>][-test_size:])</span><br><span class="line">    y_test = list(features[:, <span class="number">1</span>][-test_size:])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> open(dataset_name, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> dataset:</span><br><span class="line">        pickle.dump([x_train, y_train, x_test, y_test], dataset)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> x_train, y_train, x_test, y_test</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span>(<span class="params">shape</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;权重在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。&quot;&quot;&quot;</span></span><br><span class="line">    initial = tf.truncated_normal(shape=shape, stddev=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span>(<span class="params">shape</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;ReLu使用一个较小的正数来初始化偏置，以避免神经元输出恒为0&quot;&quot;&quot;</span></span><br><span class="line">    initial = tf.constant(shape=shape, value=<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neural_network_model</span>(<span class="params">data, sizes=list(<span class="params"></span>)</span>):</span></span><br><span class="line">    num_layer = len(sizes)</span><br><span class="line">    layer_parameters = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(num_layer - <span class="number">1</span>):</span><br><span class="line">        parameters = &#123;<span class="string">&#x27;weight&#x27;</span>: weight_variable([sizes[index], sizes[index + <span class="number">1</span>]]),</span><br><span class="line">                      <span class="string">&#x27;bias&#x27;</span>: bias_variable([sizes[index + <span class="number">1</span>]])&#125;</span><br><span class="line">        layer_parameters.append(parameters)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> range(len(layer_parameters)):</span><br><span class="line">        data = tf.add(tf.matmul(data, layer_parameters[index][<span class="string">&#x27;weight&#x27;</span>]),</span><br><span class="line">                      layer_parameters[index][<span class="string">&#x27;bias&#x27;</span>])</span><br><span class="line">        data = tf.nn.relu(data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_neural_network</span>(<span class="params">sizes=(<span class="params"><span class="number">433</span>, <span class="number">2</span></span>), batch_size=<span class="number">128</span>, hm_epochs=<span class="number">10</span></span>):</span></span><br><span class="line">    x = tf.placeholder(tf.float32, [<span class="literal">None</span>, len(train_x[<span class="number">0</span>])])</span><br><span class="line">    y = tf.placeholder(tf.float32, [<span class="literal">None</span>, len(train_y[<span class="number">0</span>])])</span><br><span class="line"></span><br><span class="line">    prediction = neural_network_model(x, list(sizes))</span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=y))</span><br><span class="line">    optimizer = tf.train.AdamOptimizer().minimize(cost)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(hm_epochs):</span><br><span class="line">            epoch_loss = <span class="number">0</span></span><br><span class="line">            i = <span class="number">0</span></span><br><span class="line">            <span class="keyword">while</span> i &lt; len(train_x):</span><br><span class="line">                start = i</span><br><span class="line">                end = i + batch_size</span><br><span class="line">                batch_x = np.array(train_x[start:end])</span><br><span class="line">                batch_y = np.array(train_y[start:end])</span><br><span class="line">                _, c = sess.run([optimizer, cost], feed_dict=&#123;x: batch_x, y: batch_y&#125;)</span><br><span class="line">                epoch_loss += c</span><br><span class="line"></span><br><span class="line">                i += batch_size</span><br><span class="line"></span><br><span class="line">            print(<span class="string">&#x27;Epoch&#x27;</span>, epoch + <span class="number">1</span>, <span class="string">&#x27;completed out of&#x27;</span>, hm_epochs, <span class="string">&#x27;loss:&#x27;</span>, epoch_loss)</span><br><span class="line"></span><br><span class="line">        correct_predictions = tf.equal(tf.argmax(prediction, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_predictions, np.float32))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">print</span> <span class="string">&#x27;Accuracy:&#x27;</span>, accuracy.eval(&#123;x: np.array(test_x), y: np.array(test_y)&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    train_x, train_y, test_x, test_y = create_features_and_labels_set(<span class="string">&#x27;positive.txt&#x27;</span>, <span class="string">&#x27;negative.txt&#x27;</span>)</span><br><span class="line">    train_neural_network(sizes=(<span class="number">433</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">2</span>), batch_size=<span class="number">128</span>, hm_epochs=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning Project </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基因算法求解函数极值</title>
      <link href="/2017/09/10/GeneticAlgorithm/"/>
      <url>/2017/09/10/GeneticAlgorithm/</url>
      
        <content type="html"><![CDATA[<p><strong>程序说明：</strong>求解函数 f(x) = x + 10sin(5x) + 7cos(4x) 在区间[0,9]的最大值<br>分析：假如设定求解的精度为小数点后4位，可以将x的解空间划分为 (9-0)×(1e+4)=90000个等分。\(2^{16} &lt; 90000 &lt; 2^{17} \)，需要17位二进制数来表示这些解。一个解的编码就是一个17位的二进制串。&lt;/br&gt;<br><strong>算法理论请参照：</strong><a href="https://rocskyfly.github.io/2017/08/19/5.5%20%E8%87%AA%E5%8A%A8%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习自动化</a>&lt;/br&gt;<br><strong>Ipynb演示文件：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/GeneticAlgorithm/FindFunctionExtreme.ipynb">Ipynb文件</a>&lt;/br&gt;<br><strong>Python代码：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/GeneticAlgorithm/FindFunctionExtreme.py">Python代码</a>&lt;/br&gt;</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GA</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, length, count</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        :param length: 染色体长度</span></span><br><span class="line"><span class="string">        :param count: 种群染数量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.length = length</span><br><span class="line">        self.count = count</span><br><span class="line">        self.population = self.gen_population()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">evolve</span>(<span class="params">self, retain_rate=<span class="number">0.2</span>, random_select_rate=<span class="number">0.5</span>, mutation_rate=<span class="number">0.01</span></span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        进化:对当前一代种群依次进行选择、交叉并生成新一代种群，然后对新一代种群进行变异。</span></span><br><span class="line"><span class="string">        :param retain_rate: 在种群中择优选取一定比率的样本作为选择父母的样本空间。</span></span><br><span class="line"><span class="string">        :param random_select_rate: 在种群中随机选择非优质染色体作为父母样本空间的一部分的随机比例。</span></span><br><span class="line"><span class="string">        :param mutation_rate: 变异率，一般使用0.5％-1％</span></span><br><span class="line"><span class="string">        :return: </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        parents = self.selection(retain_rate, random_select_rate)</span><br><span class="line">        self.crossover(parents)</span><br><span class="line">        self.mutation(mutation_rate)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gen_population</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        获取初始种群:（一个含有count个长度为length的染色体的列表）</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> [self.gen_chromosome() <span class="keyword">for</span> _ <span class="keyword">in</span> xrange(self.count)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">gen_chromosome</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        随机生成长度为length的染色体，每个基因的取值是0或1这里用一个bit表示一个基因</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        chromosome = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(self.length):</span><br><span class="line">            chromosome |= (<span class="number">1</span> &lt;&lt; i) * random.randint(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> chromosome</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fitness</span>(<span class="params">self, chromosome</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        计算适应度，将染色体解码为0~9之间数字，代入函数计算因为是求最大值，所以数值越大，适应度越高</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        x_fit = self.decode(chromosome)</span><br><span class="line">        <span class="keyword">return</span> x_fit + <span class="number">10</span> * math.sin(<span class="number">5</span> * x_fit) + <span class="number">7</span> * math.cos(<span class="number">4</span> * x_fit)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">selection</span>(<span class="params">self, retain_rate, random_select_rate</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        选择:先对适应度从大到小排序，选出存活的染色体,再进行随机选择，选出适应度虽然小，但是幸存下来的个体</span></span><br><span class="line"><span class="string">        :param retain_rate: 在种群中择优选取一定比率的样本作为选择父母的样本空间。</span></span><br><span class="line"><span class="string">        :param random_select_rate: 在种群中随机选择非优质染色体作为父母样本空间的一部分的随机比例。</span></span><br><span class="line"><span class="string">        :return: 父母的样本空间</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 对适应度从大到小进行排序。</span></span><br><span class="line">        graded = [(self.fitness(chromosome), chromosome) <span class="keyword">for</span> chromosome <span class="keyword">in</span> self.population]</span><br><span class="line">        graded = [individual[<span class="number">1</span>] <span class="keyword">for</span> individual <span class="keyword">in</span> sorted(graded, reverse=<span class="literal">True</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 选出适应性很强的前retain_rate的染色体，作为选择父母的样本空间。</span></span><br><span class="line">        retain_length = int(len(graded) * retain_rate)</span><br><span class="line">        parents = graded[:retain_length]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 选出适应性不强，但是幸存的染色体</span></span><br><span class="line">        <span class="keyword">for</span> chromosome <span class="keyword">in</span> graded[retain_length:]:</span><br><span class="line">            <span class="keyword">if</span> random.random() &lt; random_select_rate:</span><br><span class="line">                parents.append(chromosome)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> parents</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">crossover</span>(<span class="params">self, parents</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        染色体的交叉，繁殖生成新一代的种群</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        children = []  <span class="comment"># 新出生的孩子，最终会被加入存活下来的父母之中，形成新一代的种群。</span></span><br><span class="line">        target_count = len(self.population) - len(parents)  <span class="comment"># 需要繁殖的孩子的量</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">while</span> len(children) &lt; target_count:</span><br><span class="line">            male = random.randint(<span class="number">0</span>, len(parents) - <span class="number">1</span>)</span><br><span class="line">            female = random.randint(<span class="number">0</span>, len(parents) - <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> male != female:</span><br><span class="line">                cross_pos = random.randint(<span class="number">0</span>, self.length)  <span class="comment"># 随机选取交叉点</span></span><br><span class="line">                mask = <span class="number">0</span>  <span class="comment"># 生成掩码，方便位操作</span></span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> xrange(cross_pos):</span><br><span class="line">                    mask |= (<span class="number">1</span> &lt;&lt; i)</span><br><span class="line">                male = parents[male]</span><br><span class="line">                female = parents[female]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 单点交叉：孩子将获得父亲在交叉点前的基因和母亲在交叉点后（包括交叉点）的基因</span></span><br><span class="line">                child = ((male &amp; mask) | female &amp; ~mask) &amp; ((<span class="number">1</span> &lt;&lt; self.length) - <span class="number">1</span>)</span><br><span class="line">                children.append(child)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 经过繁殖后，孩子和父母的数量与原始种群数量相等，更新种群。</span></span><br><span class="line">        self.population = parents + children</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mutation</span>(<span class="params">self, rate</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">        变异:对种群中的所有个体，随机改变某个个体中的某个基因</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> xrange(len(self.population)):</span><br><span class="line">            <span class="keyword">if</span> random.random() &lt; rate:</span><br><span class="line">                j = random.randint(<span class="number">0</span>, self.length - <span class="number">1</span>)</span><br><span class="line">                self.population[i] ^= <span class="number">1</span> &lt;&lt; j</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decode</span>(<span class="params">self, chromosome</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        解码染色体，将二进制转化为属于[0, 9]的实数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> chromosome * <span class="number">9.0</span> / (<span class="number">2</span> ** self.length - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">result</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;获得当前代的最优值，这里取的是函数取最大值时x的值。&quot;&quot;&quot;</span></span><br><span class="line">        graded = [(self.fitness(chromosome), chromosome) <span class="keyword">for</span> chromosome <span class="keyword">in</span> self.population]</span><br><span class="line">        graded = [individual[<span class="number">1</span>] <span class="keyword">for</span> individual <span class="keyword">in</span> sorted(graded, reverse=<span class="literal">True</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.decode(graded[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    ga = GA(<span class="number">17</span>, <span class="number">300</span>)  <span class="comment"># 染色体长度为17， 种群数量为300</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> xrange(<span class="number">100</span>):  <span class="comment"># 100次进化迭代</span></span><br><span class="line">        ga.evolve()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> ga.result()  <span class="comment"># 参考结果为：7.85672650701</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning Project </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>猫狗图像识别</title>
      <link href="/2017/09/06/CatsVsDogsRecognization/"/>
      <url>/2017/09/06/CatsVsDogsRecognization/</url>
      
        <content type="html"><![CDATA[<p><strong>数据集说明：</strong>数据集来自Kaggle猫狗图像识别比赛。训练集共有25000张已被标注的大小不尽相同的猫和狗的图片，猫狗图片数量各一半，图片命名规则为:[dog|cat].index.jpg<br>测试集共有12500张未被标注的大小不尽相同的猫和狗的图片。图片命名规则为:index.jpg<br>&lt;/br&gt;<br><strong>程序说明：</strong>采用卷积神经网络算法，由Tflearn库，实现猫和狗的识别&lt;/br&gt;<br><strong>算法理论请参照：</strong><a href="https://rocskyfly.github.io/categories/4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">神经网络算法</a>&lt;/br&gt;<br><strong>Ipynb演示文件：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/CatsVsDogsRecognization/TflearnCnnDogsVsCats.ipynb">Ipynb文件</a>&lt;/br&gt;<br><strong>Python代码：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/CatsVsDogsRecognization/TflearnCnnDogsVsCats.py">Python代码</a>&lt;/br&gt;</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> shuffle</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tflearn</span><br><span class="line"><span class="keyword">from</span> tflearn.layers.conv <span class="keyword">import</span> conv_2d, max_pool_2d</span><br><span class="line"><span class="keyword">from</span> tflearn.layers.core <span class="keyword">import</span> input_data, dropout, fully_connected</span><br><span class="line"><span class="keyword">from</span> tflearn.layers.estimator <span class="keyword">import</span> regression</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#设置图片大小：长度和宽度</span></span><br><span class="line">IMG_LENGTH = <span class="number">50</span></span><br><span class="line">IMG_WIDTH = <span class="number">50</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">label_img</span>(<span class="params">img</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    获取训练集每张图片的标签。</span></span><br><span class="line"><span class="string">    :param img: 输入为图片文件名</span></span><br><span class="line"><span class="string">    :return: 输出为标签，猫为[1,0]，狗为[0,1]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    word_label = img.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">-3</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> word_label == <span class="string">&#x27;cat&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">    <span class="keyword">elif</span> word_label == <span class="string">&#x27;dog&#x27;</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="number">0</span>, <span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_train_dataset</span>(<span class="params">dest, img_length, img_width, train_dataset_name</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    训练集数据预处理，获取训练集加标签的数据。</span></span><br><span class="line"><span class="string">    :param dest: 训练集原始数据所在的目录</span></span><br><span class="line"><span class="string">    :param img_length: 重新设置图片的长度</span></span><br><span class="line"><span class="string">    :param img_width: 重新设置图片的宽度</span></span><br><span class="line"><span class="string">    :param train_dataset_name: 保存处理后的数据集的名称</span></span><br><span class="line"><span class="string">    :return: 训练集数据list，单个元素为[np.array(img), np.array(label)]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    training_data = []</span><br><span class="line">    <span class="keyword">for</span> img <span class="keyword">in</span> tqdm(os.listdir(dest)):  <span class="comment"># tqdm进度条，用户只需要封装任意的迭代器tqdm(iterator)</span></span><br><span class="line">        label = label_img(img)</span><br><span class="line">        path = os.path.join(dest, img)</span><br><span class="line"></span><br><span class="line">        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)</span><br><span class="line">        img_std = cv2.resize(img, (img_length, img_width))</span><br><span class="line"></span><br><span class="line">        training_data.append([np.array(img_std), np.array(label)])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将序列的所有元素随机排序</span></span><br><span class="line">    shuffle(training_data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将training_data保存为文件，以备下次使用</span></span><br><span class="line">    np.save(train_dataset_name, training_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> training_data</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_test_dataset</span>(<span class="params">dest, img_length, img_width, test_dataset_name</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    测试集数据预处理，获取测试集数据。</span></span><br><span class="line"><span class="string">    :param dest: 训练集原始数据所在的目录</span></span><br><span class="line"><span class="string">    :param img_length: 重新设置图片的长度</span></span><br><span class="line"><span class="string">    :param img_width: 重新设置图片的宽度</span></span><br><span class="line"><span class="string">    :param test_dataset_name: 保存处理后的数据集的名称</span></span><br><span class="line"><span class="string">    :return: 测试集数据list，单个元素为[np.array(img), img_num]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    testing_data = []</span><br><span class="line">    <span class="keyword">for</span> img <span class="keyword">in</span> tqdm(os.listdir(dest)):</span><br><span class="line">        img_num = img.split(<span class="string">&#x27;.&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">        path = os.path.join(dest, img)</span><br><span class="line"></span><br><span class="line">        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)</span><br><span class="line">        img_std = cv2.resize(img, (img_length, img_width))</span><br><span class="line"></span><br><span class="line">        testing_data.append([np.array(img_std), img_num])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将testing_data保存为文件，以备下次使用</span></span><br><span class="line">    np.save(test_dataset_name, testing_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> testing_data</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dataset_preprocessing</span>(<span class="params">train_dir, test_dir, img_length=IMG_LENGTH, img_width=IMG_WIDTH</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    获取并预处理训练集和测试集数据</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    train_dataset_name = <span class="string">&#x27;train_dataset-&#123;&#125;x&#123;&#125;.npy&#x27;</span>.format(img_length, img_width)</span><br><span class="line">    test_dataset_name = <span class="string">&#x27;test_dataset-&#123;&#125;x&#123;&#125;.npy&#x27;</span>.format(img_length, img_width)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(train_dataset_name):</span><br><span class="line">        train_dataset = np.load(train_dataset_name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        train_dataset = create_train_dataset(train_dir, img_length, img_width, train_dataset_name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(test_dataset_name):</span><br><span class="line">        test_dataset = np.load(test_dataset_name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        test_dataset = create_test_dataset(test_dir, img_length, img_width, test_dataset_name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_dataset, test_dataset</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    定义卷积神经网络模型</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, classifiers=<span class="number">2</span>, lr=<span class="number">1e-3</span>, img_length=IMG_LENGTH, img_width=IMG_WIDTH</span>):</span></span><br><span class="line">        self.classifiers = classifiers</span><br><span class="line">        self.lr = lr</span><br><span class="line">        self.img_length = img_length</span><br><span class="line">        self.img_width = img_width</span><br><span class="line">        self.log_dir = <span class="string">&#x27;log&#x27;</span></span><br><span class="line">        self.model_name = <span class="string">&#x27;dogsvscats-&#123;&#125;-&#123;&#125;.model&#x27;</span>.format(lr, <span class="string">&#x27;6conv-basic&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        tf.reset_default_graph()</span><br><span class="line">        convnet = input_data(shape=[<span class="literal">None</span>, self.img_length, self.img_width, <span class="number">1</span>], name=<span class="string">&#x27;input&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        convnet = conv_2d(convnet, nb_filter=<span class="number">32</span>, filter_size=<span class="number">5</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        convnet = max_pool_2d(convnet, kernel_size=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">        convnet = conv_2d(convnet, nb_filter=<span class="number">64</span>, filter_size=<span class="number">5</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        convnet = max_pool_2d(convnet, kernel_size=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">        convnet = conv_2d(convnet, nb_filter=<span class="number">32</span>, filter_size=<span class="number">2</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        convnet = max_pool_2d(convnet, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        convnet = conv_2d(convnet, nb_filter=<span class="number">64</span>, filter_size=<span class="number">2</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        convnet = max_pool_2d(convnet, kernel_size=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        convnet = conv_2d(convnet, nb_filter=<span class="number">32</span>, filter_size=<span class="number">2</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        convnet = max_pool_2d(convnet, kernel_size=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        convnet = conv_2d(convnet, nb_filter=<span class="number">64</span>, filter_size=<span class="number">2</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">        convnet = max_pool_2d(convnet, kernel_size=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        convnet = fully_connected(convnet, n_units=<span class="number">1024</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        convnet = dropout(convnet, keep_prob=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">        convnet = fully_connected(convnet, classifiers, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">        convnet = regression(convnet, optimizer=<span class="string">&#x27;adam&#x27;</span>, learning_rate=self.lr, loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>,</span><br><span class="line">                             name=<span class="string">&#x27;targets&#x27;</span>)</span><br><span class="line">        <span class="comment"># convnet = regression(convnet, optimizer=&#x27;adam&#x27;, learning_rate=learn_rate, loss=&#x27;binary_crossentropy&#x27;,</span></span><br><span class="line">        <span class="comment">#                      name=&#x27;targets&#x27;)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 指定tensorboard_dir，可以将运行中生成的结构化数据放在此目录下，为tensorboard可视化提供数据,命令：</span></span><br><span class="line">        <span class="comment"># tensorboard --logdir full path of tensorboard_dir</span></span><br><span class="line">        self.network = tflearn.DNN(convnet, tensorboard_dir=self.log_dir)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, train_dataset</span>):</span></span><br><span class="line">        <span class="comment"># split out training and testing data，有标签数据分为训练集和测试集</span></span><br><span class="line">        train = train_dataset[:<span class="number">-2500</span>]</span><br><span class="line">        test = train_dataset[<span class="number">-2500</span>:]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># separate my features and labels　特征，类别分离</span></span><br><span class="line">        x = np.array([i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> train]).reshape(<span class="number">-1</span>, self.img_length, self.img_width, <span class="number">1</span>)</span><br><span class="line">        y = [i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> train]</span><br><span class="line">        <span class="comment"># print &quot;x shape is:%s\t y size is:%s&quot; % (x.shape, len(y))</span></span><br><span class="line"></span><br><span class="line">        test_x = np.array([i[<span class="number">0</span>] <span class="keyword">for</span> i <span class="keyword">in</span> test]).reshape(<span class="number">-1</span>, self.img_length, self.img_width, <span class="number">1</span>)</span><br><span class="line">        test_y = [i[<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> test]</span><br><span class="line">        <span class="comment"># print &quot;test x shape is:%s\t test y size is:%s&quot; % (test_x.shape, len(test_y))</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># run_id for tensorboard</span></span><br><span class="line">        self.network.fit(&#123;<span class="string">&#x27;input&#x27;</span>: x&#125;, &#123;<span class="string">&#x27;targets&#x27;</span>: y&#125;, n_epoch=<span class="number">5</span>,</span><br><span class="line">                         validation_set=(&#123;<span class="string">&#x27;input&#x27;</span>: test_x&#125;, &#123;<span class="string">&#x27;targets&#x27;</span>: test_y&#125;),</span><br><span class="line">                         snapshot_step=<span class="number">500</span>, show_metric=<span class="literal">True</span>, run_id=self.model_name)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.network</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.network.save(self.model_name)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">if</span> os.path.exists(<span class="string">&#x27;&#123;&#125;.meta&#x27;</span>.format(self.model_name)):</span><br><span class="line">            self.network.load(self.model_name)</span><br><span class="line">            <span class="keyword">print</span> <span class="string">&quot;model loaded!&quot;</span></span><br><span class="line">            <span class="keyword">return</span> self.network</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">print</span> <span class="string">&quot;no existed model, need training&quot;</span></span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, dataset</span>):</span></span><br><span class="line">        <span class="keyword">if</span> <span class="number">0</span> &lt; len(dataset) &lt;= <span class="number">12</span>:</span><br><span class="line">            fig = plt.figure()</span><br><span class="line"></span><br><span class="line">            <span class="comment"># predict first 12 data in test data</span></span><br><span class="line">            <span class="keyword">for</span> num, data <span class="keyword">in</span> enumerate(dataset):  <span class="comment"># enumerate将其组成一个索引序列，利用它可以同时获得索引和值</span></span><br><span class="line">                img_data = data[<span class="number">0</span>]</span><br><span class="line">                img_index = data[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 将画布分割成3行4列，图像画在从左到右从上到下的第num+1块</span></span><br><span class="line">                y = fig.add_subplot(<span class="number">3</span>, <span class="number">4</span>, num + <span class="number">1</span>)</span><br><span class="line">                orig = img_data</span><br><span class="line">                data = img_data.reshape(self.img_length, self.img_width, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">                model_out = self.network.predict([data])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> np.argmax(model_out) == <span class="number">1</span>:</span><br><span class="line">                    str_label = <span class="string">&#x27;Dog-&#x27;</span> + img_index</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    str_label = <span class="string">&#x27;Cat-&#x27;</span> + img_index</span><br><span class="line"></span><br><span class="line">                y.imshow(orig, cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">                plt.title(str_label)</span><br><span class="line">                y.axes.get_xaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line">                y.axes.get_yaxis().set_visible(<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">            plt.show()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">with</span> open(<span class="string">&#x27;result.csv&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                f.write(<span class="string">&#x27;id,label\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">with</span> open(<span class="string">&#x27;result.csv&#x27;</span>, <span class="string">&#x27;a&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                <span class="keyword">for</span> data <span class="keyword">in</span> tqdm(dataset):</span><br><span class="line">                    img_num = data[<span class="number">1</span>]</span><br><span class="line">                    img_data = data[<span class="number">0</span>]</span><br><span class="line">                    data = img_data.reshape(self.img_length, self.img_width, <span class="number">1</span>)</span><br><span class="line">                    model_out = self.network.predict([data])[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">                    f.write(<span class="string">&#x27;&#123;&#125;,&#123;&#125;\n&#x27;</span>.format(img_num, model_out[<span class="number">1</span>]))</span><br><span class="line">            <span class="keyword">print</span> <span class="string">&quot;Result show in file result.csv&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    train_dir_name = <span class="string">&#x27;./train&#x27;</span></span><br><span class="line">    test_dir_name = <span class="string">&#x27;./test&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取训练数据集及测试数据集</span></span><br><span class="line">    train_data, test_data = dataset_preprocessing(train_dir_name, test_dir_name)</span><br><span class="line"></span><br><span class="line">    model = CNN()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> model.load():</span><br><span class="line">        model.fit(train_data)</span><br><span class="line">        model.save()</span><br><span class="line"></span><br><span class="line">    model.predict(test_data)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning Project </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>手写数字图片识别（mnist）</title>
      <link href="/2017/09/02/MnistRecognization/"/>
      <url>/2017/09/02/MnistRecognization/</url>
      
        <content type="html"><![CDATA[<p><strong>数据集说明：</strong>MNIST是一个入门级的计算机视觉数据集，它包含各种手写数字图片，每一张图片有对应的标签标记为数字几。数据集分为两部分,每个MNIST数据单元包含手写数字的图片“xs”和一个对应的标签“ys”:</p><a id="more"></a><ul><li>60000行的训练数据集：图片为mnist.train.images、标签为mnist.train.labels.</li><li>10000行的测试数据集：图片为mnist. test.images、标签为mnist. test.labels。</li></ul><p>图片“xs”包含28<em>28个像素点，用一个数字数组来表示这张图片，将数组展开成一个向量，长度是 28</em>28=784（会丢弃图片的二维结构信息），数据集的图片就是在784维向量空间里面的点</p><p>mnist.train.images 为张量[60000, 784]，mnist. test.images为张量[10000, 784] ，第一个维度用来索引图片，第二个维度用来索引每张图片中的像素点（向量中的每个元素都表示某张图片里的某个像素的强度值，值介于0和1之间）。</p><p>mnist.train.labels为张量[60000, 10] ，图片的标签是一个one-hot向量除了某一位的数字是1以外其余各维度数字都是0（表示0到9的数字）。<br>&lt;/br&gt;<br><strong>程序说明：</strong>通过采用多种神经网络算法，实现手写数字的识别。&lt;/br&gt;</p><h1 id="1-自实现的神经网络"><a href="#1-自实现的神经网络" class="headerlink" title="1.自实现的神经网络"></a><font color=blue>1.自实现的神经网络</font></h1><p><strong>算法理论请参照：</strong><a href="https://rocskyfly.github.io/categories/4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">神经网络算法</a>&lt;/br&gt;<br><strong>Ipynb演示文件：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/MnistRecognization/NeuralNetworkByRocky.ipynb">Ipynb文件</a>&lt;/br&gt;<br><strong>Python代码：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/MnistRecognization/NeuralNetworkByRocky.py">Python代码</a>&lt;/br&gt;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;实现S型函数方法&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-z))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_prime</span>(<span class="params">z</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;# 实现sigmoid_prime，计算σ函数的导数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> sigmoid(z) * (<span class="number">1</span> - sigmoid(z))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuadraticCost</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;二次代价函数&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fn</span>(<span class="params">a, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回代价函数的整体误差&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">0.5</span> * np.linalg.norm(a - y) ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">delta</span>(<span class="params">z, a, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回二次代价函数输出层的误差：z为带权输入，a为激活值（实际输出值），y为实际值&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> (a - y) * sigmoid_prime(z)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CrossEntropyCost</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;交叉熵代价函数&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fn</span>(<span class="params">a, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;回代价函数的整体误差，np.nan_to_num 确保将nan转换成0.0&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> np.sum(np.nan_to_num(-y * np.log(a) - (<span class="number">1</span> - y) * np.log(<span class="number">1</span> - a)))</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">delta</span>(<span class="params">z, a, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回二次代价函数输出层的误差：z为带权输入，a为激活值（实际输出值），y为实际值&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> a - y</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span>(<span class="params">object</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, sizes=list(<span class="params"></span>), cost=CrossEntropyCost, eta=<span class="number">3.0</span>, mini_batch_size=<span class="number">25</span>, epochs=<span class="number">20</span>, lmbda=<span class="number">0.02</span></span>):</span></span><br><span class="line">        <span class="comment"># sizes为长度为3的列表，表示神经网络的层数为3。</span></span><br><span class="line">        self.num_layers = len(sizes)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 若创建⼀个在第⼀层有2个神经元，第⼆层有3个神经元，最后层有1个神经元Network对象为：sizes=[2,3,1]</span></span><br><span class="line">        self.sizes = sizes</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 神经网络代价函数</span></span><br><span class="line">        self.cost = cost</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 随机初始化偏置：np.random.randn ⽣成均值为 0，标准差为1 的⾼斯分布。</span></span><br><span class="line">        self.biases = [np.random.randn(y, <span class="number">1</span>) <span class="keyword">for</span> y <span class="keyword">in</span> sizes[<span class="number">1</span>:]]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 随机初始化权重：⽣成均值为 0，标准差为1/sqrt(input) 的⾼斯分布；zip函数接受任意多个序列作为参数，返回一个tuple列表。</span></span><br><span class="line">        self.weights = [np.random.randn(y, x) / np.sqrt(x) <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(sizes[:<span class="number">-1</span>], sizes[<span class="number">1</span>:])]</span><br><span class="line"></span><br><span class="line">        self.eta = eta  <span class="comment"># eta代表学习率。</span></span><br><span class="line">        self.mini_batch_size = mini_batch_size  <span class="comment"># mini_batch_size代表每次迭代的数据数量。</span></span><br><span class="line">        self.epochs = epochs  <span class="comment"># epochs表示迭代次数。</span></span><br><span class="line">        self.lmbda = lmbda  <span class="comment"># lmbda为L2正则化参数.</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, train_data, test_data=None</span>):</span></span><br><span class="line">        self.sgd(train_data, test_data)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, x_predictions</span>):</span></span><br><span class="line">        predictions = [np.argmax(self.feed_forward(x.reshape(<span class="number">784</span>, <span class="number">1</span>))) <span class="keyword">for</span> x <span class="keyword">in</span> x_predictions]</span><br><span class="line">        <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span>(<span class="params">self, test_data</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;评估算法：通过前向传播算法获取测试数据集的网络的输出值，</span></span><br><span class="line"><span class="string">        将输出值与测试数据集的标签进行比对，获取准确率&quot;&quot;&quot;</span></span><br><span class="line">        n_test = len(test_data)</span><br><span class="line">        test_results = [(np.argmax(self.feed_forward(x.reshape(<span class="number">784</span>, <span class="number">1</span>))), np.argmax(y))</span><br><span class="line">                        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_data]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> sum(int(x == y) <span class="keyword">for</span> (x, y) <span class="keyword">in</span> test_results) / float(n_test)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">feed_forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;前向传播算法：对每⼀层计算神经元的激活值：σ(wx + b)&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            x = sigmoid(np.dot(w, x) + b)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sgd</span>(<span class="params">self, train_data, test_data</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;实现小批量随机梯度下降算法：train_data是训练数据集列表，每个元素为(x, y)；test_data用来评估</span></span><br><span class="line"><span class="string">        每次小批量迭代后的准确率；&quot;&quot;&quot;</span></span><br><span class="line">        n = len(train_data)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(self.epochs):</span><br><span class="line">            <span class="comment"># 随机地将训练数据打乱，然后将它切分成每个大小为mini_batch_size的⼩批量数据集。</span></span><br><span class="line">            <span class="keyword">import</span> random</span><br><span class="line">            random.shuffle(train_data)</span><br><span class="line">            mini_batches = [train_data[k:(k + self.mini_batch_size)] <span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">0</span>, n,</span><br><span class="line">                                                                                     self.mini_batch_size)]</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 每⼀个 mini_batch应⽤⼀次梯度下降算法，根据单次梯度下降的迭代更新⽹络的权重和偏置。</span></span><br><span class="line">            <span class="keyword">for</span> mini_batch <span class="keyword">in</span> mini_batches:</span><br><span class="line">                self.update_mini_batch(mini_batch, n)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> test_data:</span><br><span class="line">                <span class="keyword">print</span> <span class="string">&quot;Epoch %s/%s accuracy: %s&quot;</span> % (j, self.epochs, self.score(test_data))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_mini_batch</span>(<span class="params">self, mini_batch, train_dataset_length</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;对每个mini_batch应用一次梯度下降，更新网络权重和偏置&quot;&quot;&quot;</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]  <span class="comment"># zeros返回来一个给定形状和类型的,用0填充的数组.</span></span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 对mini_batch中的每对训练数据应用反向传播算法，获取每对训练数据在每层上的代价函数的梯度和。</span></span><br><span class="line">        <span class="keyword">for</span> x, y <span class="keyword">in</span> mini_batch:</span><br><span class="line">            delta_nabla_b, delta_nabla_w = self.back_prop(x.reshape(<span class="number">784</span>, <span class="number">1</span>), y.reshape(<span class="number">10</span>, <span class="number">1</span>))</span><br><span class="line">            nabla_b = [nb + dnb <span class="keyword">for</span> nb, dnb <span class="keyword">in</span> zip(nabla_b, delta_nabla_b)]</span><br><span class="line">            nabla_w = [nw + dnw <span class="keyword">for</span> nw, dnw <span class="keyword">in</span> zip(nabla_w, delta_nabla_w)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 应用小批量随机梯度下降算法，更新网络权重和偏置。</span></span><br><span class="line">        self.weights = [(<span class="number">1</span> - self.eta * self.lmbda / train_dataset_length) * w - (self.eta / len(mini_batch)) * nw</span><br><span class="line">                        <span class="keyword">for</span> w, nw <span class="keyword">in</span> zip(self.weights, nabla_w)]</span><br><span class="line">        self.biases = [b - (self.eta / len(mini_batch)) * nb</span><br><span class="line">                       <span class="keyword">for</span> b, nb <span class="keyword">in</span> zip(self.biases, nabla_b)]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">back_prop</span>(<span class="params">self, x, y</span>):</span></span><br><span class="line">        <span class="string">&quot;&quot;&quot;反向传播算法：逐层获取代价函数关于权重和偏置的偏导数。&quot;&quot;&quot;</span></span><br><span class="line">        nabla_b = [np.zeros(b.shape) <span class="keyword">for</span> b <span class="keyword">in</span> self.biases]</span><br><span class="line">        nabla_w = [np.zeros(w.shape) <span class="keyword">for</span> w <span class="keyword">in</span> self.weights]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 前向传播：计算并保存每层神经元的的带权输入以及激活值。</span></span><br><span class="line">        activation = x</span><br><span class="line">        activations = [x]  <span class="comment"># list to store all the activations, layer by layer</span></span><br><span class="line">        zs = []  <span class="comment"># list to store all the z vectors, layer by layer</span></span><br><span class="line">        <span class="keyword">for</span> b, w <span class="keyword">in</span> zip(self.biases, self.weights):</span><br><span class="line">            z = np.dot(w, activation) + b</span><br><span class="line">            zs.append(z)</span><br><span class="line">            activation = sigmoid(z)</span><br><span class="line">            activations.append(activation)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播：首先计算并保存最后输出层的误差（偏置偏导数）以及权重偏导数。</span></span><br><span class="line">        delta = self.cost.delta(zs[<span class="number">-1</span>], activations[<span class="number">-1</span>], y)</span><br><span class="line">        nabla_b[<span class="number">-1</span>] = delta</span><br><span class="line">        nabla_w[<span class="number">-1</span>] = np.dot(delta, activations[<span class="number">-2</span>].transpose())</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播：自倒数第二层开始，逐层计算并保存输出误差（偏置偏导数）以及权重偏导数。</span></span><br><span class="line">        <span class="keyword">for</span> l <span class="keyword">in</span> xrange(<span class="number">2</span>, self.num_layers):</span><br><span class="line">            z = zs[-l]</span><br><span class="line">            delta = np.dot(self.weights[-l + <span class="number">1</span>].transpose(), delta) * sigmoid_prime(z)</span><br><span class="line">            nabla_b[-l] = delta</span><br><span class="line">            nabla_w[-l] = np.dot(delta, activations[-l - <span class="number">1</span>].transpose())</span><br><span class="line">        <span class="keyword">return</span> nabla_b, nabla_w</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">load</span>(<span class="params">self, filename=<span class="string">&#x27;NeuralNetwork.pickle&#x27;</span></span>):</span></span><br><span class="line">        <span class="keyword">with</span> open(filename, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            data = pickle.load(f)</span><br><span class="line">            self.weights = data[<span class="string">&quot;weights&quot;</span>]</span><br><span class="line">            self.biases = data[<span class="string">&quot;biaes&quot;</span>]</span><br><span class="line">            self.sizes = data[<span class="string">&quot;sizes&quot;</span>]</span><br><span class="line">            self.cost = data[<span class="string">&quot;cost&quot;</span>]</span><br><span class="line">            self.eta = data[<span class="string">&quot;eta&quot;</span>]</span><br><span class="line">            self.mini_batch_size = data[<span class="string">&quot;mini_batch_size&quot;</span>]</span><br><span class="line">            self.epochs = data[<span class="string">&quot;epochs&quot;</span>]</span><br><span class="line">        <span class="keyword">print</span> <span class="string">&quot;Load model successfully!&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span>(<span class="params">self, filename=<span class="string">&#x27;NeuralNetwork.pickle&#x27;</span></span>):</span></span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">&quot;weights&quot;</span>: self.weights,</span><br><span class="line">            <span class="string">&quot;biaes&quot;</span>: self.biases,</span><br><span class="line">            <span class="string">&quot;sizes&quot;</span>: self.sizes,</span><br><span class="line">            <span class="string">&quot;cost&quot;</span>: self.cost,</span><br><span class="line">            <span class="string">&quot;eta&quot;</span>: self.eta,</span><br><span class="line">            <span class="string">&quot;mini_batch_size&quot;</span>: self.mini_batch_size,</span><br><span class="line">            <span class="string">&quot;epochs&quot;</span>: self.epochs,</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">import</span> pickle</span><br><span class="line">        <span class="keyword">with</span> open(filename, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            pickle.dump(data, f)</span><br><span class="line">        <span class="keyword">print</span> <span class="string">&quot;Save mode successfully!&quot;</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;数据集说明：minist 训练图片数据集总有55000条，测试图片数据集共有10000条&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 读取mnist数据集</span></span><br><span class="line">    <span class="keyword">from</span> tensorflow.contrib.learn.python.learn.datasets.mnist <span class="keyword">import</span> read_data_sets</span><br><span class="line">    mnist = read_data_sets(<span class="string">&quot;MNIST_data/&quot;</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># zip函数用于将可迭代的对象作为参数，将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表。</span></span><br><span class="line">    train_data_set = zip(mnist.train.images, mnist.train.labels)</span><br><span class="line">    test_data_set = zip(mnist.test.images, mnist.test.labels)</span><br><span class="line"></span><br><span class="line">    cls = Network([<span class="number">784</span>, <span class="number">400</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">    cls.fit(train_data_set, test_data_set)</span><br><span class="line"></span><br><span class="line">    cls.save()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">&quot;Accuracy: %s&quot;</span> % cls.score(test_data_set)</span><br></pre></td></tr></table></figure><h1 id="2-Tensorflow实现的简单Softmax网络"><a href="#2-Tensorflow实现的简单Softmax网络" class="headerlink" title="2.Tensorflow实现的简单Softmax网络"></a><font color=blue>2.Tensorflow实现的简单Softmax网络</font></h1><p><strong>算法理论请参照：</strong><a href="https://rocskyfly.github.io/categories/4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">神经网络算法</a>&lt;/br&gt;<br><strong>Ipynb演示文件：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/MnistRecognization/TensorflowSoftmaxSimpleNN.ipynb">Ipynb文件</a>&lt;/br&gt;<br><strong>Python代码：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/MnistRecognization/TensorflowSoftmaxSimpleNN.py">Python代码</a>&lt;/br&gt;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 小批量随机梯度下降迭代次数</span></span><br><span class="line">hm_epochs = <span class="number">20</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 小批量随机梯度下降每次迭代的数据数量</span></span><br><span class="line">batch_size = <span class="number">25</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 读取mnist数据集</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.contrib.learn.python.learn.datasets.mnist <span class="keyword">import</span> read_data_sets</span><br><span class="line">mnist = read_data_sets(<span class="string">&quot;MNIST_data/&quot;</span>, one_hot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># x占位符：张量[None, 784]表示第一维可以有任意多个输入，第二位表示长度为784的图片</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 权重变量</span></span><br><span class="line">W = tf.Variable(tf.zeros([<span class="number">784</span>, <span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 偏置变量</span></span><br><span class="line">b = tf.Variable(tf.zeros([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出层运用softmax算法</span></span><br><span class="line">y = tf.nn.softmax(tf.matmul(x, W) + b)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 占位符：表示x的实际标签值</span></span><br><span class="line">y_ = tf.placeholder(<span class="string">&quot;float&quot;</span>, [<span class="literal">None</span>, <span class="number">10</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义操作：softmax的交叉熵代价函数</span></span><br><span class="line">cross_entropy = -tf.reduce_sum(y_ * tf.log(y))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义操作：选择梯度下降算法优化器，学习率为0.01，代价函数为softmax的交叉熵代价函数</span></span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.01</span>).minimize(cross_entropy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义操作：获取预测准确的值，得tf.argmax返回最大的那个数值所在的下标。</span></span><br><span class="line">correct_predictions = tf.equal(tf.argmax(y, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义操作：初始化所有变量</span></span><br><span class="line">init = tf.global_variables_initializer()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个会话对象，启动图执行图中的所有操作。</span></span><br><span class="line"><span class="keyword">with</span> tf.Session <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># 执行操作：初始化所有变量</span></span><br><span class="line">    sess.run(init)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(hm_epochs):</span><br><span class="line">        epoch_loss = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> range(int(mnist.train.num_examples / float(batch_size))):</span><br><span class="line">            <span class="comment"># 获取大小为batch_size的小批量数据</span></span><br><span class="line">            batch_xs, batch_ys = mnist.train.next_batch(batch_size)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 执行操作：执行优化和交叉熵</span></span><br><span class="line">            _, c = sess.run([optimizer, cross_entropy], feed_dict=&#123;x: batch_xs, y_: batch_ys&#125;)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 累积误差</span></span><br><span class="line">            epoch_loss += c</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">print</span> <span class="string">&quot;Epoch %d/%d, loss: %s&quot;</span> % (epoch + <span class="number">1</span>, hm_epochs, epoch_loss)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 执行操作：找出测试数据集中那些预测正确的标签。</span></span><br><span class="line">    correct_predictions = sess.run(correct_predictions, feed_dict=&#123;x: mnist.test.images, y_: mnist.test.labels&#125;)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确定正确预测项的比例，把布尔值转换成浮点数，然后取平均值。</span></span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_predictions, <span class="string">&#x27;float&#x27;</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">print</span> <span class="string">&quot;Accuracy: &quot;</span>, accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;)</span><br></pre></td></tr></table></figure><h1 id="3-Tensorflow实现的DNN-Softmax网络"><a href="#3-Tensorflow实现的DNN-Softmax网络" class="headerlink" title="3.Tensorflow实现的DNN Softmax网络"></a><font color=blue>3.Tensorflow实现的DNN Softmax网络</font></h1><p><strong>算法理论请参照：</strong><a href="https://rocskyfly.github.io/categories/4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">神经网络算法</a>&lt;/br&gt;<br><strong>Ipynb演示文件：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/MnistRecognization/TensorflowSoftmaxDNN.ipynb">Ipynb文件</a>&lt;/br&gt;<br><strong>Python代码：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/MnistRecognization/TensorflowSoftmaxDNN.py">Python代码</a>&lt;/br&gt;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neural_network_mode</span>(<span class="params">x_train, sizes=list(<span class="params">(<span class="params"><span class="number">784</span>, <span class="number">400</span>, <span class="number">10</span></span>)</span>)</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;定义神经网络模型&quot;&quot;&quot;</span></span><br><span class="line">    num_layers = len(sizes)</span><br><span class="line">    activation = x_train</span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> range(num_layers - <span class="number">1</span>):</span><br><span class="line">        layer = &#123;<span class="string">&#x27;weights&#x27;</span>: tf.Variable(tf.random_normal([sizes[layer], sizes[layer + <span class="number">1</span>]])),</span><br><span class="line">                 <span class="string">&#x27;biases&#x27;</span>: tf.Variable(tf.random_normal([sizes[layer + <span class="number">1</span>]]))&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment"># input_data * weights + biases; define activation function</span></span><br><span class="line">        activation = tf.add(tf.matmul(activation, layer[<span class="string">&#x27;weights&#x27;</span>]), layer[<span class="string">&#x27;biases&#x27;</span>])</span><br><span class="line">        activation = tf.nn.sigmoid(activation)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> activation</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_neural_network</span>(<span class="params">x_train, y_train, sizes, batch_size=<span class="number">25</span>, hm_epochs=<span class="number">20</span></span>):</span></span><br><span class="line">    output = neural_network_mode(x_train, sizes)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义操作：softmax的交叉熵代价函数。</span></span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=y_train))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义操作：选择AdamOptimizer（A Method for Stochastic Optimization）。</span></span><br><span class="line">    optimizer = tf.train.AdamOptimizer().minimize(cost)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义操作：初始化所有变量。</span></span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 创建一个会话对象，启动图执行图中的所有操作。</span></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="comment"># 执行操作：初始化所有变量</span></span><br><span class="line">        sess.run(init)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># train neural network</span></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(hm_epochs):</span><br><span class="line">            epoch_loss = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> range(int(mnist.train.num_examples / float(batch_size))):</span><br><span class="line">                <span class="comment"># 获取大小为batch_size的小批量数据</span></span><br><span class="line">                epoch_x, epoch_y = mnist.train.next_batch(batch_size)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 执行操作：执行优化和交叉熵</span></span><br><span class="line">                _, c = sess.run([optimizer, cost], feed_dict=&#123;x: epoch_x, y: epoch_y&#125;)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 累积误差</span></span><br><span class="line">                epoch_loss += c</span><br><span class="line"></span><br><span class="line">            <span class="keyword">print</span> <span class="string">&quot;Epoch %d/%d, loss: %f&quot;</span> % (epoch + <span class="number">1</span>, hm_epochs, epoch_loss)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 找出测试数据集中那些预测正确的标签。</span></span><br><span class="line">        correct_predictions = tf.equal(tf.argmax(output, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 确定正确预测项的比例，把布尔值转换成浮点数，然后取平均值。</span></span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct_predictions, <span class="string">&#x27;float&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">print</span> <span class="string">&quot;Accuracy: &quot;</span>, accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># 读取mnist数据集</span></span><br><span class="line">    <span class="keyword">from</span> tensorflow.contrib.learn.python.learn.datasets.mnist <span class="keyword">import</span> read_data_sets</span><br><span class="line">    mnist = read_data_sets(<span class="string">&quot;MNIST_data/&quot;</span>, one_hot=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义神经网络结构</span></span><br><span class="line">    layers = [<span class="number">784</span>, <span class="number">600</span>, <span class="number">10</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># height x width</span></span><br><span class="line">    x = tf.placeholder(<span class="string">&#x27;float&#x27;</span>, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">    y = tf.placeholder(<span class="string">&#x27;float&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    train_neural_network(x, y, layers)</span><br></pre></td></tr></table></figure><h1 id="4-Tensorflow实现的卷积神经网络"><a href="#4-Tensorflow实现的卷积神经网络" class="headerlink" title="4.Tensorflow实现的卷积神经网络"></a><font color=blue>4.Tensorflow实现的卷积神经网络</font></h1><p><strong>算法理论请参照：</strong><a href="https://rocskyfly.github.io/categories/4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">神经网络算法</a>&lt;/br&gt;<br><strong>Ipynb演示文件：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/MnistRecognization/TensorflowCNN.ipynb">Ipynb文件</a>&lt;/br&gt;<br><strong>Python代码：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/MnistRecognization/TensorflowCNN.py">Python代码</a>&lt;/br&gt;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#卷积函数：使用1步长（stride size），使用左右补零填充边距操作，使得输入和输出的像素相同。</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span>(<span class="params">xs, w</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(input=xs, filter=w, strides=[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#池化函数：采用最大值池化，ksize为池化窗口的大小2x2，strides为移动的步长</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span>(<span class="params">xs</span>):</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(xs, ksize=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], strides=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>], padding=<span class="string">&#x27;SAME&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#权重初始化函数：在初始化时应该加入少量的噪声来打破对称性以及避免0梯度。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span>(<span class="params">shape</span>):</span></span><br><span class="line">    initial = tf.truncated_normal(shape=shape, stddev=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#偏置初始化函数：ReLU使用一个较小的正数来初始化偏置，以避免神经元输出恒为0</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span>(<span class="params">shape</span>):</span></span><br><span class="line">    initial = tf.constant(shape=shape, value=<span class="number">0.1</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 卷积网络定义：网络共分为六层，输入层 28×28、第一卷积层5×5、第一池化层2×2、第二卷积层5×5、第二池化层2×2、全连接层1024、输出层10</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convolution_neural_network</span>(<span class="params">xs, n_classes=<span class="number">10</span></span>):</span></span><br><span class="line">    weights = &#123;</span><br><span class="line">        <span class="string">&#x27;w_conv1&#x27;</span>: weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">32</span>]),  <span class="comment"># 第一层卷积：使用5x5的卷积核，获得32个特征，输入的通道为1，输出通道为32。</span></span><br><span class="line">        <span class="string">&#x27;w_conv2&#x27;</span>: weight_variable([<span class="number">5</span>, <span class="number">5</span>, <span class="number">32</span>, <span class="number">64</span>]),  <span class="comment"># 第二层卷积：使用5x5的卷积核，获得64个特征，输入的通道为32，输出通道为64。</span></span><br><span class="line">        <span class="string">&#x27;w_fc&#x27;</span>: weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>]),  <span class="comment"># 全连接层：输入图片的尺寸减小到7x7，将其输入至1024个神经元的全连接层。</span></span><br><span class="line">        <span class="string">&#x27;w_out&#x27;</span>: weight_variable([<span class="number">1024</span>, n_classes])</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    bias = &#123;</span><br><span class="line">        <span class="string">&#x27;b_conv1&#x27;</span>: bias_variable([<span class="number">32</span>]),</span><br><span class="line">        <span class="string">&#x27;b_conv2&#x27;</span>: bias_variable([<span class="number">64</span>]),</span><br><span class="line">        <span class="string">&#x27;b_fc&#x27;</span>: bias_variable([<span class="number">1024</span>]),</span><br><span class="line">        <span class="string">&#x27;b_out&#x27;</span>: bias_variable([n_classes])</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将原始输入图片变成一个思维向量，第一维表示任意个输入，第二第三维分别对应图片的宽和高，</span></span><br><span class="line">    <span class="comment"># 最后一维代表图片输出通道数（颜色）。由于输入为灰度图所以输出通道数为1，若为rgb彩色图，则为3。</span></span><br><span class="line">    xs = tf.reshape(xs, shape=[<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 第一卷积层：卷积后加上偏置项，然后输入ReLU激活神经元。</span></span><br><span class="line">    conv1 = tf.nn.relu(conv2d(xs, weights[<span class="string">&#x27;w_conv1&#x27;</span>]) + bias[<span class="string">&#x27;b_conv1&#x27;</span>])</span><br><span class="line">    pool1 = max_pool_2x2(conv1)  <span class="comment"># 第一池化层</span></span><br><span class="line">    conv2 = tf.nn.relu(conv2d(pool1, weights[<span class="string">&#x27;w_conv2&#x27;</span>]) + bias[<span class="string">&#x27;b_conv2&#x27;</span>])  <span class="comment"># 第二卷积层</span></span><br><span class="line">    pool2 = max_pool_2x2(conv2)  <span class="comment"># 第二池化层</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 全连接层：将池化层输出的张量，转换成二维向量，第一维表示可以任意输入，第二维表示64个7x7的图片的一维向量。</span></span><br><span class="line">    fc_input = tf.reshape(pool2, [<span class="number">-1</span>, <span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>])</span><br><span class="line">    fc_output = tf.nn.relu(tf.matmul(fc_input, weights[<span class="string">&#x27;w_fc&#x27;</span>]) + bias[<span class="string">&#x27;b_fc&#x27;</span>])</span><br><span class="line">    fc_output = tf.nn.dropout(fc_output, keep_prob)  <span class="comment"># dropout：为了减少过拟合，以keep_prob的概率，随机丢弃一些神经元输出。</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 输出层</span></span><br><span class="line">    output = tf.matmul(fc_output, weights[<span class="string">&#x27;w_out&#x27;</span>]) + bias[<span class="string">&#x27;b_out&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练卷积网络</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train_neural_network</span>(<span class="params">xs, ys, n_classes=<span class="number">10</span>, batch_size=<span class="number">128</span>, hm_epochs=<span class="number">10</span>, keep_rate=<span class="number">0.8</span></span>):</span></span><br><span class="line">    output = convolution_neural_network(xs, n_classes)</span><br><span class="line">    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=output, labels=ys))</span><br><span class="line">    optimizer = tf.train.AdamOptimizer().minimize(cost)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">        sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> epoch <span class="keyword">in</span> range(hm_epochs):</span><br><span class="line">            epoch_loss = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> range(int(mnist.train.num_examples / batch_size)):</span><br><span class="line">                epoch_x, epoch_y = mnist.train.next_batch(batch_size)</span><br><span class="line">                _, c = sess.run([optimizer, cost], feed_dict=&#123;x: epoch_x, y: epoch_y, keep_prob: keep_rate&#125;)</span><br><span class="line">                epoch_loss += c</span><br><span class="line"></span><br><span class="line">            <span class="keyword">print</span> <span class="string">&#x27;Epoch&#x27;</span>, epoch, <span class="string">&#x27;completed out of&#x27;</span>, hm_epochs, <span class="string">&#x27;loss:&#x27;</span>, epoch_loss</span><br><span class="line"></span><br><span class="line">        correct = tf.equal(tf.argmax(output, <span class="number">1</span>), tf.argmax(y, <span class="number">1</span>))</span><br><span class="line">        accuracy = tf.reduce_mean(tf.cast(correct, <span class="string">&#x27;float&#x27;</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">print</span> <span class="string">&#x27;Accuracy:&#x27;</span>, accuracy.eval(&#123;x: mnist.test.images, y: mnist.test.labels, keep_prob: keep_rate&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.contrib.learn.python.learn.datasets.mnist <span class="keyword">import</span> read_data_sets</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    mnist = read_data_sets(<span class="string">&quot;MNIST_data/&quot;</span>, one_hot=<span class="literal">True</span>)  <span class="comment"># 读取mnist数据集</span></span><br><span class="line"></span><br><span class="line">    x = tf.placeholder(<span class="string">&#x27;float&#x27;</span>, [<span class="literal">None</span>, <span class="number">784</span>])</span><br><span class="line">    y = tf.placeholder(<span class="string">&#x27;float&#x27;</span>)</span><br><span class="line">    keep_prob = tf.placeholder(<span class="string">&#x27;float&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    train_neural_network(x, y, n_classes=<span class="number">10</span>, batch_size=<span class="number">128</span>, hm_epochs=<span class="number">10</span>, keep_rate=<span class="number">0.8</span>)</span><br></pre></td></tr></table></figure><h1 id="5-Tflearn实现的卷积神经网络"><a href="#5-Tflearn实现的卷积神经网络" class="headerlink" title="5.Tflearn实现的卷积神经网络"></a><font color=blue>5.Tflearn实现的卷积神经网络</font></h1><p><strong>算法理论请参照：</strong><a href="https://rocskyfly.github.io/categories/4-%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">神经网络算法</a>&lt;/br&gt;<br><strong>Ipynb演示文件：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/MnistRecognization/TflearnCNN.ipynb">Ipynb文件</a>&lt;/br&gt;<br><strong>Python代码：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/MnistRecognization/TflearnCNN.py">Python代码</a>&lt;/br&gt;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> tflearn</span><br><span class="line"><span class="keyword">from</span> tflearn.layers.conv <span class="keyword">import</span> conv_2d, max_pool_2d</span><br><span class="line"><span class="keyword">from</span> tflearn.layers.core <span class="keyword">import</span> input_data, dropout, fully_connected</span><br><span class="line"><span class="keyword">from</span> tflearn.layers.estimator <span class="keyword">import</span> regression</span><br><span class="line"><span class="keyword">import</span> tflearn.datasets.mnist <span class="keyword">as</span> mnist</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    x, y, test_x, test_y = mnist.load_data(one_hot=<span class="literal">True</span>)  <span class="comment"># 读取mnist数据集</span></span><br><span class="line">    x = x.reshape([<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line">    test_x = test_x.reshape([<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">    input_data = input_data(shape=[<span class="literal">None</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>], name=<span class="string">&#x27;input&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># nb_filter表示输出通道数，filter_size表示卷积核大小5x5</span></span><br><span class="line">    conv1 = conv_2d(input_data, nb_filter=<span class="number">32</span>, filter_size=<span class="number">5</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># kernel_size表示池化窗口大小2x2</span></span><br><span class="line">    pool1 = max_pool_2d(conv1, kernel_size=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    conv2 = conv_2d(pool1, nb_filter=<span class="number">64</span>, filter_size=<span class="number">5</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    pool2 = max_pool_2d(conv2, kernel_size=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    fc = fully_connected(pool2, n_units=<span class="number">1024</span>, activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">    fc = dropout(fc, <span class="number">0.8</span>)</span><br><span class="line"></span><br><span class="line">    output = fully_connected(fc, n_units=<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    network = regression(output, optimizer=<span class="string">&#x27;adam&#x27;</span>, learning_rate=<span class="number">0.01</span>, loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, name=<span class="string">&#x27;targets&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    model = tflearn.DNN(network, tensorboard_dir=<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(<span class="string">&#x27;tflearncnn.model.index&#x27;</span>) <span class="keyword">and</span> os.path.exists(<span class="string">&#x27;tflearncnn.model.meta&#x27;</span>):</span><br><span class="line">        model.load(<span class="string">&#x27;tflearncnn.model&#x27;</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        model.fit(&#123;<span class="string">&#x27;input&#x27;</span>: x&#125;, &#123;<span class="string">&#x27;targets&#x27;</span>: y&#125;, n_epoch=<span class="number">10</span>,</span><br><span class="line">                  validation_set=(&#123;<span class="string">&#x27;input&#x27;</span>: test_x&#125;, &#123;<span class="string">&#x27;targets&#x27;</span>: test_y&#125;),</span><br><span class="line">                  snapshot_step=<span class="number">500</span>, batch_size=<span class="number">128</span>, show_metric=<span class="literal">True</span>, run_id=<span class="string">&#x27;mnist&#x27;</span>)</span><br><span class="line">        model.save(<span class="string">&#x27;tflearncnn.model&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 做出预测</span></span><br><span class="line">    <span class="keyword">print</span> model.predict([test_x[<span class="number">1</span>]])</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning Project </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>男女性别预测</title>
      <link href="/2017/08/27/GenderPrediction/"/>
      <url>/2017/08/27/GenderPrediction/</url>
      
        <content type="html"><![CDATA[<p><strong>数据集说明：</strong>数据集为12组成人身高、体重、鞋码的组合数据，以及是男性还是女性。&lt;/br&gt;<br><strong>程序说明：</strong>采用决策树算法，根据男生女生生理特征（身高、体重、鞋号），由Python语言实现男生女生预测。&lt;/br&gt;<br><strong>算法理论请参照：</strong><a href="https://rocskyfly.github.io/2017/06/17/2.3%20%E5%86%B3%E7%AD%96%E6%A0%91">决策树算法</a>&lt;/br&gt;<br><strong>Ipynb演示文件：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/GenderPrediction/GenderPredict.ipynb">Ipynb文件</a>&lt;/br&gt;<br><strong>Python代码：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/GenderPrediction/GenderPrediction.py">Python代码</a>&lt;/br&gt;</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据说明：数据集为12组成人身高、体重、鞋码的组合数据，以及是男性还是女性&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># [height, weight, shoe size]</span></span><br><span class="line">X = [[<span class="number">181</span>, <span class="number">80</span>, <span class="number">44</span>], [<span class="number">177</span>, <span class="number">70</span>, <span class="number">43</span>], [<span class="number">160</span>, <span class="number">60</span>, <span class="number">38</span>], [<span class="number">154</span>, <span class="number">54</span>, <span class="number">37</span>],</span><br><span class="line">     [<span class="number">166</span>, <span class="number">65</span>, <span class="number">40</span>], [<span class="number">190</span>, <span class="number">90</span>, <span class="number">47</span>], [<span class="number">175</span>, <span class="number">64</span>, <span class="number">39</span>], [<span class="number">177</span>, <span class="number">70</span>, <span class="number">40</span>],</span><br><span class="line">     [<span class="number">159</span>, <span class="number">55</span>, <span class="number">38</span>], [<span class="number">171</span>, <span class="number">75</span>, <span class="number">42</span>], [<span class="number">181</span>, <span class="number">85</span>, <span class="number">43</span>], [<span class="number">148</span>, <span class="number">70</span>, <span class="number">42</span>]]</span><br><span class="line"></span><br><span class="line">y = [<span class="string">&#x27;male&#x27;</span>, <span class="string">&#x27;female&#x27;</span>, <span class="string">&#x27;female&#x27;</span>, <span class="string">&#x27;female&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;female&#x27;</span>, <span class="string">&#x27;female&#x27;</span>, <span class="string">&#x27;female&#x27;</span>, <span class="string">&#x27;female&#x27;</span>,</span><br><span class="line">         <span class="string">&#x27;male&#x27;</span>, <span class="string">&#x27;female&#x27;</span>, <span class="string">&#x27;male&#x27;</span>, <span class="string">&#x27;male&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;选择决策树算法，训练算法&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">&#x27;DecisionTree.pickle&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        clf = pickle.load(f)</span><br><span class="line"><span class="keyword">except</span> Exception, e:</span><br><span class="line">    <span class="comment"># 训练算法</span></span><br><span class="line">    clf = tree.DecisionTreeClassifier()</span><br><span class="line">    clf.fit(X, y)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 序列化算法</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">&#x27;DecisionTree.pickle&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">import</span> pickle</span><br><span class="line">        pickle.dump(clf, f)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># make a prediction.</span></span><br><span class="line">prediction = clf.predict([[<span class="number">190</span>, <span class="number">70</span>, <span class="number">43</span>], [<span class="number">156</span>, <span class="number">60</span>, <span class="number">36</span>]])</span><br><span class="line"><span class="keyword">print</span> prediction</span><br><span class="line">```    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="string">&quot;&quot;&quot;Visualization: graphviz export of the above tree trained on the entire dataset;</span></span><br><span class="line"><span class="string">the results are saved in an output file GenderClassifier.pdf&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Graphviz是图形绘制工具,可以很方便的用来绘制结构化的图形网络,支持多种格式输出</span></span><br><span class="line"><span class="keyword">import</span> graphviz </span><br><span class="line">dot_data = tree.export_graphviz(clf, out_file=<span class="literal">None</span>,</span><br><span class="line">                                feature_names=[<span class="string">&#x27;height&#x27;</span>, <span class="string">&#x27;weight&#x27;</span>, <span class="string">&#x27;shoe size&#x27;</span>],</span><br><span class="line">                                class_names=<span class="string">&#x27;gender&#x27;</span>,</span><br><span class="line">                                filled=<span class="literal">True</span>, rounded=<span class="literal">True</span>,</span><br><span class="line">                                special_characters=<span class="literal">True</span>)</span><br><span class="line">graph = graphviz.Source(dot_data)</span><br><span class="line">graph.render(<span class="string">&#x27;GenderClassifier&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning Project </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>良性恶性乳腺肿瘤预测</title>
      <link href="/2017/08/26/BreastCancerPrediction/"/>
      <url>/2017/08/26/BreastCancerPrediction/</url>
      
        <content type="html"><![CDATA[<p><strong>数据集说明：</strong>数据集来自威斯康星州医院的699条乳腺肿瘤数据，每条数据包含以下内容：</p><ol><li>Sample code number\ \ \ \ \ \ \ \ \ \ \ \ \ \id number</li><li>Clump Thickness\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 1 - 10</li><li>Uniformity of Cell Size\ \ \ \ \ \ \ \ \ \ \ 1 - 10</li><li>Uniformity of Cell Shape\ \ \ \ \ \ \ \ \ \  1 - 10</li><li>Marginal Adhesion\ \ \ \ \ \ \ \ \ \ \ \ \ \ 1 - 10</li><li>Single Epithelial Cell Size\ \ \ \ \ \ \ \ \ 1 - 10</li><li>Bare Nuclei\ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \  1 - 10</li><li>Bland Chromatin\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 1 - 10</li><li>Normal Nucleoli\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 1 - 10<ol><li>Mitoses \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  1 - 10</li><li>Class: \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (2 for benign, 4 for malignant)</li></ol></li></ol><a id="more"></a><p><strong>程序说明：</strong>采用KNN算法、支持向量机SVM，由Python语言实现良性恶性肿瘤预测。&lt;/br&gt;<br><strong>算法理论请参照：</strong><a href="https://rocskyfly.github.io/2017/06/17/2.2%20KNN%E7%AE%97%E6%B3%95/">KNN算法</a>、<a href="https://rocskyfly.github.io/2017/06/23/2.5%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">支持向量机SVM</a>&lt;/br&gt;<br><strong>Ipynb演示文件：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/BreastCancerPrediction/BreastCancerPrediction.ipynb">Ipynb文件</a>&lt;/br&gt;<br><strong>Python代码：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/BreastCancerPrediction/BreastCancerPrediction.py">Python代码</a>&lt;/br&gt;</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;获取并预处理原始数据集&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;breast-cancer-wisconsin.data.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#将所有列中为空的或未知的数据用-99999替代。</span></span><br><span class="line">df.replace(<span class="string">&#x27;?&#x27;</span>, <span class="number">-99999</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">df.fillna(<span class="number">-99999</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 去除id列，其与肿瘤是否为良性还是恶性无关，加入会严重影响分类的结果。</span></span><br><span class="line">df.drop([<span class="string">&#x27;id&#x27;</span>], <span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;将数据集划分为训练集及测试集合&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X = np.array(df.drop([<span class="string">&#x27;class&#x27;</span>], <span class="number">1</span>))</span><br><span class="line">y = np.array(df[<span class="string">&#x27;class&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;选择算法、训练算法并测试算法&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm, neighbors</span><br><span class="line">cls_dict=&#123;</span><br><span class="line">    <span class="string">&#x27;SVM-SVC&#x27;</span>:svm.SVC(),</span><br><span class="line">    <span class="string">&#x27;KNN&#x27;</span>:neighbors.KNeighborsClassifier()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练并测试算法：若算法需要调优，可手动删除model序列化文件。</span></span><br><span class="line"><span class="keyword">for</span> name, cls <span class="keyword">in</span> cls_dict.items():</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">import</span> pickle</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">&#x27;%s.pickle&#x27;</span> % name, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            cls = pickle.load(f)</span><br><span class="line">    <span class="keyword">except</span> Exception, e:</span><br><span class="line">        <span class="comment"># 训练算法</span></span><br><span class="line">        cls.fit(X_train, y_train)</span><br><span class="line">        <span class="keyword">print</span> e</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 序列化算法</span></span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">&#x27;%s.pickle&#x27;</span> % name, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            pickle.dump(cls, f)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试算法</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">&quot;%s Algorithm Accuracy: %s&quot;</span> % (name, cls.score(X_test, y_test))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 预测</span></span><br><span class="line">    samples = np.array([[<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>]])</span><br><span class="line">    samples = samples.reshape(len(samples), <span class="number">-1</span>)</span><br><span class="line">    prediction = cls.predict(samples)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">&quot;%s Algorithm prediction: %s\n&quot;</span> % (name, prediction)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning Project </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>股票价格预测</title>
      <link href="/2017/08/24/StockPricePrediction/"/>
      <url>/2017/08/24/StockPricePrediction/</url>
      
        <content type="html"><![CDATA[<p><strong>数据集说明：</strong>京东自2014年5月至2017年12月份公司股票交易数据共898条，通过这些历史数据建模，运用机器学习的方法，预测未来的股票价格。&lt;/br&gt;<br><strong>程序说明：</strong>采用线性回归、SVM线性回归、SVM多项式回归、SVM高斯回归，由Python语言实现垃圾邮件分类。&lt;/br&gt;<br><strong>算法理论请参照：</strong><a href="https://rocskyfly.github.io/2017/06/10/2.1%20%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/">线性回归算法</a>、<a href="https://rocskyfly.github.io/2017/06/23/2.5%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/">支持向量机SVM</a>&lt;/br&gt;<br><strong>Ipynb演示文件：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/StockPricePrediction/JDStockPricePrediction.ipynb">Ipynb文件</a>&lt;/br&gt;<br><strong>Python代码：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/StockPricePrediction/StockPricePrediction.py">Python代码</a>&lt;/br&gt;</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;原始数据处理，并进行特征工程&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取原始数据，清洗并处理数据：将所有列中为空的或未知的数据用-99999替代.</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;JDHistoricalQuotes.csv&#x27;</span>)</span><br><span class="line">df.replace(<span class="string">&#x27;?&#x27;</span>, <span class="number">-99999</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df.fillna(<span class="number">-99999</span>,inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将volume列转换为float类型；对日期进行升序排序；重新设置df的index为日期。</span></span><br><span class="line">df[<span class="string">&#x27;volume&#x27;</span>] = df[<span class="string">&#x27;volume&#x27;</span>] * <span class="number">1.0</span></span><br><span class="line">df[<span class="string">&#x27;date&#x27;</span>] = pd.to_datetime(df[<span class="string">&#x27;date&#x27;</span>])</span><br><span class="line">df.sort_values(by=<span class="string">&#x27;date&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">df.set_index(df[<span class="string">&#x27;date&#x27;</span>], inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取数据的关键特征。</span></span><br><span class="line">df[<span class="string">&#x27;hl_pct&#x27;</span>]=(df[<span class="string">&#x27;high&#x27;</span>]-df[<span class="string">&#x27;close&#x27;</span>])/df[<span class="string">&#x27;close&#x27;</span>] * <span class="number">100.0</span></span><br><span class="line">df[<span class="string">&#x27;pct_change&#x27;</span>] = (df[<span class="string">&#x27;close&#x27;</span>]-df[<span class="string">&#x27;open&#x27;</span>])/df[<span class="string">&#x27;open&#x27;</span>]*<span class="number">100.0</span></span><br><span class="line">df =df[[<span class="string">&#x27;close&#x27;</span>,<span class="string">&#x27;hl_pct&#x27;</span>,<span class="string">&#x27;pct_change&#x27;</span>, <span class="string">&#x27;volume&#x27;</span>]]</span><br><span class="line"></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 取数据集的最后1%的close作为需要预测的数据，余下的作为训练集和测试数据集</span></span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line">data_set_percent = <span class="number">0.01</span></span><br><span class="line">forecast_out = int(math.ceil(data_set_percent * len(df)))</span><br><span class="line">df[<span class="string">&#x27;label&#x27;</span>] = df[<span class="string">&#x27;close&#x27;</span>].shift(-forecast_out)</span><br><span class="line">df.tail(<span class="number">12</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取数据集的特征，归一化数据集的特征。</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">X = np.array(df.drop(<span class="string">&#x27;label&#x27;</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">X = preprocessing.scale(X)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将数据集划分为训练集、测试集以及预测数据集。</span></span><br><span class="line">X_validation = X[:-forecast_out]</span><br><span class="line">X_predict = X[-forecast_out:]</span><br><span class="line">df.dropna(inplace=<span class="literal">True</span>)</span><br><span class="line">y=np.array(df[<span class="string">&#x27;label&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X_validation, y, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;训练模型并评估模型，做出预测&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="comment"># 存放模型预测数据</span></span><br><span class="line">forecast_dict = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分别定义四种模型：线性回归、SVM线性回归、SVM多项式回归、SVM高斯回归</span></span><br><span class="line">cls_dict = &#123;</span><br><span class="line">    <span class="string">&#x27;LineRegression&#x27;</span>: LinearRegression(n_jobs=<span class="number">10</span>),  <span class="comment"># 设置并行数</span></span><br><span class="line">    <span class="string">&#x27;SvmLinearRegression&#x27;</span>: svm.SVR(kernel=<span class="string">&#x27;linear&#x27;</span>, C=<span class="number">1e3</span>),</span><br><span class="line">    <span class="string">&#x27;SvmPolyRegression&#x27;</span>: svm.SVR(kernel=<span class="string">&#x27;poly&#x27;</span>, C=<span class="number">8</span>, degree=<span class="number">3</span>),</span><br><span class="line">    <span class="string">&#x27;SvmRbfRegression&#x27;</span>: svm.SVR(kernel=<span class="string">&#x27;rbf&#x27;</span>, C=<span class="number">1e3</span>, gamma=<span class="number">0.1</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练并测试算法：并序列化model，若算法需要调优，可手动删除model序列化文件。</span></span><br><span class="line"><span class="keyword">for</span> name, cls <span class="keyword">in</span> cls_dict.items():</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">&#x27;%s.pickle&#x27;</span> % name, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            cls = pickle.load(f)</span><br><span class="line">    <span class="keyword">except</span> Exception, e:</span><br><span class="line">        <span class="comment"># 训练算法</span></span><br><span class="line">        cls.fit(X_train, y_train)</span><br><span class="line">        <span class="keyword">print</span> e</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 序列化算法</span></span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">&#x27;%s.pickle&#x27;</span> % name, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            pickle.dump(cls, f)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">print</span> <span class="string">&quot;%s Algorithm Accuracy: %s&quot;</span> % (name, cls.score(X_test, y_test))</span><br><span class="line"></span><br><span class="line">    forecast_dict.setdefault(name, (cls.predict(X_predict)))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;通过matplotlib图形化展示&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 获取所有可以使用的图形样式</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> style</span><br><span class="line">style.available</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取要预测时间点的起点：即dataframe最后一行数据中的index的值（类型为：class pandas._libs.tslib.Timestamp）</span></span><br><span class="line">last_date_object = df.iloc[<span class="number">-1</span>].name</span><br><span class="line">last_date_unix = last_date_object.value // <span class="number">10</span> ** <span class="number">9</span></span><br><span class="line">one_day_seconds = <span class="number">24</span> * <span class="number">60</span> * <span class="number">60</span></span><br><span class="line">next_date_unix = last_date_unix + one_day_seconds</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> key, predict <span class="keyword">in</span> forecast_dict.items():</span><br><span class="line">    df[<span class="string">&#x27;forecast-%s&#x27;</span> % key ] = np.nan</span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> predict:</span><br><span class="line">        <span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line">        next_date = datetime.fromtimestamp(next_date_unix)</span><br><span class="line">        next_date_unix += one_day_seconds</span><br><span class="line">        df.loc[next_date] = [np.nan <span class="keyword">for</span> _ <span class="keyword">in</span> range(len(df.columns)<span class="number">-1</span>)] + [value]</span><br><span class="line">    <span class="comment"># 回到需要预测时间的起点。</span></span><br><span class="line">    next_date_unix -= one_day_seconds * len(predict)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> key == <span class="string">&#x27;LineRegression&#x27;</span>:</span><br><span class="line">        color = <span class="string">&#x27;blue&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> key == <span class="string">&#x27;SvmPolyRegression&#x27;</span>:</span><br><span class="line">        color = <span class="string">&#x27;yellow&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> key == <span class="string">&#x27;SvmLinearRegression&#x27;</span>:</span><br><span class="line">        color = <span class="string">&#x27;green&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        color = <span class="string">&#x27;white&#x27;</span></span><br><span class="line">    </span><br><span class="line">    df[<span class="string">&#x27;forecast-%s&#x27;</span> % key].plot(label=key, color=color)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># legend 图例就是为了展示出每个数据对应的图像名称.</span></span><br><span class="line">plt.legend()</span><br><span class="line"><span class="comment"># plot(x, y)不指定x默认为df的index</span></span><br><span class="line">plt.plot(df[<span class="string">&#x27;close&#x27;</span>], color=<span class="string">&#x27;red&#x27;</span>, label=<span class="string">&#x27;Data&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Date&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Price&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Support Vector Regression&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning Project </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>垃圾邮件分类器</title>
      <link href="/2017/08/22/SpamEmailClassifier/"/>
      <url>/2017/08/22/SpamEmailClassifier/</url>
      
        <content type="html"><![CDATA[<p><strong>数据集说明：</strong>数据集包含5574条数据，每条数据包含邮件类型（是否为垃圾邮件）以及邮件的标题。&lt;/br&gt;<br><strong>程序说明：</strong>采用逻辑回归算法，由Python语言实现垃圾邮件分类。&lt;/br&gt;<br><strong>算法理论请参照：</strong><a href="https://rocskyfly.github.io/2017/06/10/2.1%20%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/">逻辑回归算法</a>、<a href="https://rocskyfly.github.io/2017/06/17/2.4%20%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/">贝叶斯分类器</a>、<a href="https://rocskyfly.github.io/2017/08/12/5.2%20%E5%A6%82%E4%BD%95%E8%AF%84%E4%BC%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/">偏斜类误差度量</a>&lt;/br&gt;<br><strong>Ipynb演示文件：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/SpamEmailClassifier/SpamEmailClassifier.ipynb">Ipynb文件</a>&lt;/br&gt;<br><strong>Python代码：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/SpamEmailClassifier/SpamEmailClassifier.py">Python代码</a>&lt;/br&gt;</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;Reading a text-based dataset by pandas&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 读取以制表符分隔值的文本文件，定义第一列列名为type，余下所有的列为一整体，列名为message。</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">sms = pd.read_table(<span class="string">&#x27;sms.tsv&#x27;</span>, header=<span class="literal">None</span>, names=[<span class="string">&#x27;type&#x27;</span>, <span class="string">&#x27;message&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看sms Dataframe的前五行内容。</span></span><br><span class="line">sms.head()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 统计type列中不同类型的总和</span></span><br><span class="line">sms[<span class="string">&#x27;type&#x27;</span>].value_counts()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将type列的内容进行数值转换，将结果存储在新的列label列中</span></span><br><span class="line">sms[<span class="string">&#x27;label&#x27;</span>] = sms[<span class="string">&#x27;type&#x27;</span>].map(&#123;<span class="string">&#x27;ham&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;spam&#x27;</span>: <span class="number">1</span>&#125;)</span><br><span class="line"></span><br><span class="line">sms.head()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分别获取原始数据集的特征和标签。</span></span><br><span class="line">X=sms[<span class="string">&#x27;message&#x27;</span>]</span><br><span class="line">y=sms[<span class="string">&#x27;label&#x27;</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机将数据集划分成成70%训练集，30%测试集。</span></span><br><span class="line"><span class="comment"># 设置random_state参数：get the same output the first time you make the split.</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">1</span>, test_size=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;Vectorizing our dataset：&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 初始化文本矢量类实例，将训练文本转化为稀疏矩阵</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">cls = CountVectorizer()</span><br><span class="line">X_train = cls.fit_transform(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># transform testing data (using fitted vocabulary) into a document-term matrix</span></span><br><span class="line">X_test = cls.transform(X_test)</span><br><span class="line">repr(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;Building and fit LogisticRegression model&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 采用逻辑回归和贝叶斯两种模型</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</span><br><span class="line"></span><br><span class="line">cls_dict = &#123;</span><br><span class="line">        <span class="string">&#x27;LogisticRegression&#x27;</span>: LogisticRegression(),</span><br><span class="line">        <span class="string">&#x27;NaiveBayes&#x27;</span>: MultinomialNB()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练并测试算法：若算法需要调优，可手动删除model序列化文件。</span></span><br><span class="line"><span class="keyword">for</span> name, cls <span class="keyword">in</span> cls_dict.items():</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">&#x27;%s.pickle&#x27;</span> % name, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            cls = pickle.load(f)</span><br><span class="line">    <span class="keyword">except</span> Exception, e:</span><br><span class="line">        <span class="comment"># 训练算法</span></span><br><span class="line">        cls.fit(X_train, y_train)</span><br><span class="line">        <span class="keyword">print</span> e</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 序列化算法</span></span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">&#x27;%s.pickle&#x27;</span> % name, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            pickle.dump(cls, f)</span><br><span class="line"></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Evaluating model&quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 获取混淆矩阵，及F_1 Score</span></span><br><span class="line">    <span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line">    <span class="keyword">print</span> <span class="string">&quot;%s Algorithm Accuracy: %s\nconfusion matrix:%s\nF1_score:%s\n&quot;</span> \</span><br><span class="line">          % (name, cls.score(X_test, y_test), metrics.confusion_matrix(y_test, cls.predict(X_test)),</span><br><span class="line">             metrics.f1_score(y_test, cls.predict(X_test)))</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning Project </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>鸢尾花分类器</title>
      <link href="/2017/08/20/IrisClassifier/"/>
      <url>/2017/08/20/IrisClassifier/</url>
      
        <content type="html"><![CDATA[<p><strong>数据集说明：</strong><br>Iris数据集也称鸢尾花卉数据集是常用的分类实验数据集，是一类多重变量分析的数据集。数据集包含150个数据集，分为3类，每类50个数据。每个数据包含4个属性：花萼长度，花萼宽度，花瓣长度，花瓣宽度。依据4个属性预测鸢尾花卉属于（Setosa，Versicolour，Virginica）三个种类中的哪一类。&lt;/br&gt;<br><strong>程序说明：</strong>采用三种不同的机器学习算法（sklearn KNN、Decision Tree、自实现的KNN）由Python语言实现鸢尾花卉的分类。&lt;/br&gt;<br><strong>算法理论请参照：</strong><a href="https://rocskyfly.github.io/2017/06/17/2.2%20KNN%E7%AE%97%E6%B3%95/">KNN算法</a>、<a href="https://rocskyfly.github.io/2017/06/17/2.3%20%E5%86%B3%E7%AD%96%E6%A0%91/">决策树算法</a>&lt;/br&gt;<br><strong>Ipynb演示文件：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/IrisClassifer/IrisClassifier.ipynb">Ipynb文件</a>&lt;/br&gt;<br><strong>Python代码：</strong><a href="https://github.com/rocskyfly/MachineLearning/blob/master/IrisClassifer/IrisClassifier.py">Python代码</a>&lt;/br&gt;</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#This data sets consists of 3 different types of irises’ (Setosa, Versicolour, and Virginica) </span></span><br><span class="line"><span class="comment">#petal and sepal length, stored in a 150x4 numpy.ndarray</span></span><br><span class="line"><span class="comment">#The rows being the samples and the columns being: Sepal Length, Sepal Width, Petal Length and Petal Width.</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">Y = iris.target</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取数据集大小</span></span><br><span class="line">len(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机将数据集划分成成70%训练集，30%测试集。</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=<span class="number">0.3</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#rocky实现的KNN算法</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> scipy.spatial <span class="keyword">import</span> distance</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.x_train = <span class="literal">None</span></span><br><span class="line">        self.y_train = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span>(<span class="params">self, x_train, y_train</span>):</span></span><br><span class="line">        self.x_train = x_train</span><br><span class="line">        self.y_train = y_train</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, x_test, k=<span class="number">3</span></span>):</span></span><br><span class="line">        predictions = []</span><br><span class="line">        <span class="keyword">for</span> element <span class="keyword">in</span> x_test:</span><br><span class="line">            <span class="comment"># 无投票，简单找到离训练集最近的点，返回其label最为预测x的label</span></span><br><span class="line">            label = self.closet(element)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 投票法</span></span><br><span class="line">            <span class="comment"># label = self.vote(element, k)</span></span><br><span class="line"></span><br><span class="line">            predictions.append(label)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> predictions</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span>(<span class="params">self, x_test, y_test</span>):</span></span><br><span class="line">        <span class="keyword">return</span> metrics.accuracy_score(y_test, self.predict(x_test))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">closet</span>(<span class="params">self, element</span>):</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">enc</span>(<span class="params">a, b</span>):</span></span><br><span class="line">            <span class="keyword">return</span> distance.euclidean(a, b)</span><br><span class="line"></span><br><span class="line">        best_dist = enc(element, self.x_train[<span class="number">0</span>])</span><br><span class="line">        best_index = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(<span class="number">1</span>, len(self.x_train)):</span><br><span class="line">            dist = enc(element, self.x_train[index])</span><br><span class="line">            <span class="keyword">if</span> dist &lt; best_dist:</span><br><span class="line">                best_dist = dist</span><br><span class="line">                best_index = index</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.y_train[best_index]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">vote</span>(<span class="params">self, element, k</span>):</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">enc</span>(<span class="params">a, b</span>):</span></span><br><span class="line">            <span class="keyword">return</span> distance.euclidean(a, b)</span><br><span class="line"></span><br><span class="line">        k_list = []</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(k):</span><br><span class="line">            best_dist = enc(element, self.x_train[index])</span><br><span class="line">            k_list.append([index, best_dist])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(k, len(self.x_train)):</span><br><span class="line">            dist = enc(element, self.x_train[index])</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(k):</span><br><span class="line">                <span class="keyword">if</span> dist &lt; k_list[i][<span class="number">1</span>]:</span><br><span class="line">                    k_list.pop(i)</span><br><span class="line">                    k_list.insert(i, [index, dist])</span><br><span class="line"></span><br><span class="line">        index_list = []</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> range(k):</span><br><span class="line">            index_list.append(k_list[index][<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="comment"># list with one element, it&#x27;s a tuple (index, times)</span></span><br><span class="line">        counter = Counter(index_list)</span><br><span class="line">        index = counter.most_common(<span class="number">1</span>)[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> self.y_train[index]</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 选择算法：分别选用KNeighborsClassifier、rocky实现的KNN、DecisionTreeClassifier算法</span></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">cls_dict = &#123;<span class="string">&#x27;Sklearn-KNN&#x27;</span>: KNeighborsClassifier(),</span><br><span class="line">            <span class="string">&#x27;Rocky-KNN&#x27;</span>: KNN(),</span><br><span class="line">            <span class="string">&#x27;DecisionTree&#x27;</span>: DecisionTreeClassifier(),</span><br><span class="line">            &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练并测试算法：并序列化，若算法需要调优，可手动删除序列化文件</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> name, cls <span class="keyword">in</span> cls_dict.items():</span><br><span class="line">    <span class="comment"># 训练算法：并序列化，若算法需要调优，可手动删除序列化文件</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">&#x27;%s.pickle&#x27;</span> % name, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            cls = pickle.load(f)</span><br><span class="line">    <span class="keyword">except</span> Exception, e:</span><br><span class="line">        <span class="comment"># 训练算法</span></span><br><span class="line">        cls.fit(X_train, Y_train)</span><br><span class="line">        <span class="keyword">print</span> e</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 序列化算法</span></span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">&#x27;%s.pickle&#x27;</span> % name, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            pickle.dump(cls, f)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试算法</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">&quot;%s Algorithm Accuracy: %s&quot;</span> % (name, cls.score(X_test, Y_test))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Machine Learning Project </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5.5 自动化机器学习</title>
      <link href="/2017/08/19/5.5%20%E8%87%AA%E5%8A%A8%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
      <url>/2017/08/19/5.5%20%E8%87%AA%E5%8A%A8%E5%8C%96%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>自动化机器学习起源于网格搜索概念，是一种流水线（也称管道）。它通过搜索方法、变换特征、混合参数值来获得最佳解决方案。TPOT是自动化机器学习的一个应用框架，提供了遗传算法这样的应用，可用来在某个配置中混合各个参数并达到最佳设置。常见的自动机器学习库：TPOT、Auto-Sklearn、Auto-Weka、Machine-JS、DataRobot。</p><a id="more"></a><h1 id="5-5-1-遗传算法基础"><a href="#5-5-1-遗传算法基础" class="headerlink" title="5.5.1 遗传算法基础"></a><font color=blue>5.5.1 遗传算法基础</font></h1><h2 id="遗传算法（Genetic-Algorithm）描述"><a href="#遗传算法（Genetic-Algorithm）描述" class="headerlink" title="遗传算法（Genetic Algorithm）描述:"></a><font color=DodgerBlue>遗传算法（Genetic Algorithm）描述:</font></h2><p>遵循适者生存、优胜劣汰的原则，是一类借鉴生物界自然选择和自然遗传机制的随机化搜索算法，属于启发式搜索算法一种。&lt;/br&gt;<br>模拟一个人工种群的进化过程，通过选择、交叉以及变异等机制，在每次迭代中都保留一组候选个体，重复此过程，种群经过若干代进化后，理想情况下其适应度达到近似最优的状态。</p><h2 id="遗传算法组成"><a href="#遗传算法组成" class="headerlink" title="遗传算法组成:"></a><font color=DodgerBlue>遗传算法组成:</font></h2><ol><li>基因：一个遗传因子。 </li><li>染色体：包含一组基因，个体是包含某一染色体。</li><li>种群：是染色体的集合，由个体组成。</li><li>适应度函数：个体适应度判断，适者生存。 </li><li>选择（Selection）：优胜劣汰，仅适应环境的个体可以繁衍后代。</li><li>交叉（Corssover）：新个体会遗传父母双方各一部分的基因。</li><li>变异（Mutation）：子代的基因在继承父母的基因的基础上会有一定的概率发生基因变异。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.13%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E7%BB%84%E6%88%90.png?raw=true" alt="遗传算法组成"></li></ol><h2 id="应用"><a href="#应用" class="headerlink" title="应用:"></a><font color=DodgerBlue>应用:</font></h2><ul><li>机器学习：特征选取、算法选择、混合参数调优等。 </li><li>交通与船运路线：遗传算法已被很多贸易公司用来让运输更省时、经济。</li><li>工程设计：遗传算法在这里可以进行优化并给出一个即快又经济的模型。</li></ul><h2 id="特点"><a href="#特点" class="headerlink" title="特点:"></a><font color=DodgerBlue>特点:</font></h2><ul><li>是启发式群体搜索，不是盲目穷举， 易于并行化处理。</li><li>适应度函数不受连续、可微等条件的约束，适用范围很广。</li><li>容易实现，一旦有了一个遗传算法的程序，如果想解决一个新的问题，只需针对新的问题重新进行基因编码就行；若编码方法也相同，那只需要改变一下适应度函数就可以了。</li></ul><font color=orangered>缺点：全局搜索能力不强，很容易陷入局部最优解跳不出来。</font><h1 id="5-5-2-遗传算法步骤"><a href="#5-5-2-遗传算法步骤" class="headerlink" title="5.5.2 遗传算法步骤"></a><font color=blue>5.5.2 遗传算法步骤</font></h1><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.15%E9%81%97%E4%BC%A0%E7%AE%97%E6%B3%95%E6%AD%A5%E9%AA%A4.png?raw=true" alt="遗传算法步骤"><br>以背包问题为例：假若要去野游 ，但是只能背一个限重 30 公斤的背包。现在有不同的必需物品，它们每一个都有自己的生存点数。因此目标是在有限的背包重量下，最大化生存点数。</p><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.14%E8%83%8C%E5%8C%85%E7%89%A9%E5%93%81%E5%8F%82%E6%95%B0.png?raw=true" alt="背包物品参数"></p><h2 id="初始化：产生初始种群"><a href="#初始化：产生初始种群" class="headerlink" title="初始化：产生初始种群"></a><font color=DodgerBlue>初始化：产生初始种群</font></h2><p>染色体长度选择：取决于具体的问题，即编码方法。&lt;/br&gt;<br>种群大小选择：规模大小取决于编码的方法，即编码串的大小。比较大的种群规模并不能优化遗传算法结果，推荐使用20-30。&lt;/br&gt;<br>种群中的每个个体包含一染色体，每条染色体上有6个基因，分别对应不同的物品，1 代表该物品装入袋中，0 代表该物品不装。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.16%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%E5%9F%BA%E5%9B%A0%E4%B8%8E%E7%A7%8D%E7%BE%A4.png?raw=true" alt="背包问题基因与种群"></p><h2 id="个体适应度判断：定义适应度函数，区分个体优劣"><a href="#个体适应度判断：定义适应度函数，区分个体优劣" class="headerlink" title="个体适应度判断：定义适应度函数，区分个体优劣"></a><font color=DodgerBlue>个体适应度判断：定义适应度函数，区分个体优劣</font></h2><p>当染色体包含更多生存分数时，也就意味着它的适应性更强。如染色体 \(A1\) 重量是29，生存分数为28；染色体A2重量是16，生存分数为23， \(A1\) 适应性强于 \(A2\)。</p><h2 id="选择父体母体：定义选择函数，从总体中选择可以进行繁殖的染色体，产生下一代"><a href="#选择父体母体：定义选择函数，从总体中选择可以进行繁殖的染色体，产生下一代" class="headerlink" title="选择父体母体：定义选择函数，从总体中选择可以进行繁殖的染色体，产生下一代"></a><font color=DodgerBlue>选择父体母体：定义选择函数，从总体中选择可以进行繁殖的染色体，产生下一代</font></h2><p>一般使用轮盘赌选择法：将一个轮盘分割成\(m \)个部分， \(m \) 代表总体中染色体的个数。每条染色体在轮盘上占有的区域面积将根据适应度分数成比例表达出来。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.18%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%E8%BD%AE%E7%9B%98%E9%80%89%E6%8B%A9.png?raw=true" alt="背包问题轮盘选择"><br>这个轮盘开始旋转，将被图中固定的指针指到的那片区域选为第一个亲本。然后对于第二个亲本进行同样的操作。有时候也会在途中标注两个固定指针，可以在一轮中就获得两个亲本。将这种方法成为随机普遍选择法。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.17%E8%83%8C%E5%8C%85%E9%97%AE%E9%A2%98%E8%BD%AE%E7%9B%98%E9%80%89%E6%8B%A9%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B.png?raw=true" alt="背包问题轮盘选择计算结果"></p><font color=orangered>若为精英操作只选最优。</font><h2 id="交叉运算：定义交叉函数，繁殖后代"><a href="#交叉运算：定义交叉函数，繁殖后代" class="headerlink" title="交叉运算：定义交叉函数，繁殖后代"></a><font color=DodgerBlue>交叉运算：定义交叉函数，繁殖后代</font></h2><p>将已经选择出了可以产生后代的亲本染色体，进行交叉（繁殖）即染色体 1 和 4。&lt;/br&gt;<br>单点交叉：是交叉最基本的形式，随机选择一个交叉点，然后将交叉点前后的染色体部分进行染色体间的交叉对调，于是就产生了新的后代。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.19%E5%8D%95%E7%82%B9%E4%BA%A4%E5%8F%89.png?raw=true" alt="单点交叉"><br>多点交叉：设置两个交叉点，产生新的后代。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.20%E5%A4%9A%E7%82%B9%E4%BA%A4%E5%8F%89.png?raw=true" alt="多点交叉"></p><font color=orangered>交叉率一般来说应该比较大，推荐使用80％-95％。</font><h2 id="变异运算：定义变异函数，保证生物的多样性"><a href="#变异运算：定义变异函数，保证生物的多样性" class="headerlink" title="变异运算：定义变异函数，保证生物的多样性"></a><font color=DodgerBlue>变异运算：定义变异函数，保证生物的多样性</font></h2><p>它可以被定义为染色体上发生的随机变化，正是因为变异，种群中才会存在多样性，变异完成之后，就得到了新为个体，进化也就完成了<br> <img src= "/img/loading.gif" data-lazy-src="/img/ai/5.21%E5%8F%98%E5%BC%82.png?raw=true" alt="变异"><br>内变异：所谓内变异就是在自己内部发生变异，是一种比较有效的手段。&lt;/br&gt;<br>外变异：外变异是引入创新，突破传统的质的飞跃, 也是启发算法中所谓的全域搜索。在当前基因中引入外部基因（如：当前集合的补集）。</p><font color=orangered>变异率：变异率一般来说应该比较小，一般使用0.5％-1％最好。</font><h2 id="个体适应度检验：替代种群中适应度较低的个体"><a href="#个体适应度检验：替代种群中适应度较低的个体" class="headerlink" title="个体适应度检验：替代种群中适应度较低的个体"></a><font color=DodgerBlue>个体适应度检验：替代种群中适应度较低的个体</font></h2><p>用适应度函数对这些新的后代进行验证，如果函数判定它们适应度足够，那么就会用它们从总体中替代掉那些适应度不够的染色体，继续以上过程。</p><h2 id="定义终止条件：退出迭代，获取最优状态。"><a href="#定义终止条件：退出迭代，获取最优状态。" class="headerlink" title="定义终止条件：退出迭代，获取最优状态。"></a><font color=DodgerBlue>定义终止条件：退出迭代，获取最优状态。</font></h2><ul><li>事先定义好进化的次数。</li><li>当适应度函数已经达到了预先定义的值。</li><li>当连续N代子代种群的最优个体适应度都小于等于父代最优个性的适应度。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 5. 机器学习系统设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5.4 大型机器学习系统应用技术</title>
      <link href="/2017/08/15/5.4%20%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
      <url>/2017/08/15/5.4%20%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="5-4-1-机器学习系统上限分析"><a href="#5-4-1-机器学习系统上限分析" class="headerlink" title="5.4.1 机器学习系统上限分析"></a><font color=blue>5.4.1 机器学习系统上限分析</font></h1><p>在大型机器学习的应用中，通常需要通过几个步骤才能进行最终的预测，一般先通过上限分析知道哪一部分最值得花时间和精力去改善，上限分析是从管道开始的部分，逐级手工提供 100%正确的输出结果，然后看应用的整体效果提升了多少，找出瓶颈。</p><a id="more"></a><h1 id="5-2-2-大规模数据机器学习"><a href="#5-2-2-大规模数据机器学习" class="headerlink" title=" 5.2.2 大规模数据机器学习"></a><font color=blue> 5.2.2 大规模数据机器学习</font></h1><h2 id="通过学习曲线诊断，确定合适大小的训练集"><a href="#通过学习曲线诊断，确定合适大小的训练集" class="headerlink" title="通过学习曲线诊断，确定合适大小的训练集"></a><font color=DodgerBlue>通过学习曲线诊断，确定合适大小的训练集</font></h2><p>假若应对一个有 100 万条记录的训练集，以线性回归模型为例，每一次梯度下降迭代，都需要计算训练集的误差的平方和，计算代价非常大。&lt;/br&gt;<br>通过绘制学习曲线判定大规模的训练集是否必要，也许只用 1000记录也能获得较好的效果。</p><h2 id="采用随机梯度下降法，加快训练速度"><a href="#采用随机梯度下降法，加快训练速度" class="headerlink" title=" 采用随机梯度下降法，加快训练速度 "></a><font color=DodgerBlue> 采用随机梯度下降法，加快训练速度 </font></h2><p>Stochastic Gradient Descent—随机梯度下降法</p><h3 id="适用场景："><a href="#适用场景：" class="headerlink" title="适用场景："></a>适用场景：</h3><p>若一定需要一个大规模的训练集，可以尝试使用随机梯度下降法来代替批量梯度下降法。定义单一训练实例的代价函数为：</p><script type="math/tex; mode=display">cost(θ,(x^{(i}),y^{(i)}))=\frac {1}{2} (h_θ (x^{(i)})-y^{(i)})^2</script><h3 id="算法描述："><a href="#算法描述：" class="headerlink" title="算法描述："></a>算法描述：</h3><p>首先将训练集数据打乱，然后运行随机梯度下降算法，在每一次计算训练实例之后便更新参数 \(θ\)，最后根据数据集的大小一般循环执行1-10次。</p><script type="math/tex; mode=display">Repeat\ \ \{   for\ \ i:=1:m\{    θ_j≔θ_j-α(h_θ (x^{(i)} )-y^{(i)}) x_j^{(i)}\\  For every j=0,…,n  \}\}</script><h3 id="算法特点："><a href="#算法特点：" class="headerlink" title="算法特点："></a>算法特点：</h3><ul><li>优点：梯度下降速度快。不需要首先将所有的训练集求和，在每一次计算之后便更新参数\(θ\)。</li><li>缺点：可能无法找到最小值点。不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法找到个最小值的那一点，而是在最小值点附近徘徊。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.9%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E8%BF%87%E7%A8%8B%E5%9B%BE.png?raw=true" alt="随机梯度下降过程图"></li></ul><h3 id="随机梯度下降收敛："><a href="#随机梯度下降收敛：" class="headerlink" title="随机梯度下降收敛："></a>随机梯度下降收敛：</h3><ul><li>绘制 \(cost(θ,(x^{(i}) , y^{(i)}))\) 曲线判断算法是否收敛：每次对最后 1000 个或多个样本，计算出它们的代价函数平均值；观察曲线，保证正常运转和收敛，也可用它来调整学习速率 \(η\) 的大小。</li><li>学习率 \(η\) 随着迭代次数的增加而减小：随着不断地靠近全局最小值，通过减小学习率，迫使算法收敛而非在最小值附近徘徊，但对 \(η\) 进行调整所耗费的计算通常不值得。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.10%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%94%B6%E6%95%9B.png?raw=true" alt="随机梯度下降收敛"></li></ul><h2 id="采用小批量梯度下降方法，加快训练速度"><a href="#采用小批量梯度下降方法，加快训练速度" class="headerlink" title="采用小批量梯度下降方法，加快训练速度"></a><font color=DodgerBlue>采用小批量梯度下降方法，加快训练速度</font></h2><p>小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算常数 \(b (bϵ[2,100])\)次训练实例，便更新一次参数 \(θ\).</p><script type="math/tex; mode=display">Repeat \{       for\ i:=1:m \{        θ_j≔θ_j-α \frac {1}{b} ∑_{k=i}^{i+b-1} (h_θ (x^{(k)})-y^{(k)})  x_j^{(k)} \\            For every j=0,…,n           i+=10;      \}\}</script><p>可以用向量化的方式来循环b个训练实例，如果用的线性代数函数库比较好，能够支持平行处理。梯度下降的速度有时比随机梯度下降还要快。</p><h2 id="采用在线学习法"><a href="#采用在线学习法" class="headerlink" title="采用在线学习法"></a><font color=DodgerBlue>采用在线学习法</font></h2><p>与随机梯度下降算法有些类似，对单一的实例进行学习，而非对一个提前定义的训练集进行循环。一旦对一个数据的学习完成了，便可以丢弃该数据，不需要再存储它了。<br>算法可以很好的适应用户的倾向性，可以针对用户的当前行为不断地更新模型以适应该用户。<br>若有一个变化的用户群，又或者在尝试预测的事情，在缓慢变化，就像用户的品味在缓慢变化，可以慢慢地调试所学习到的假设， 将其调节更新到最新的用户行为。</p><h2 id="采用分布式并行计算方法"><a href="#采用分布式并行计算方法" class="headerlink" title="采用分布式并行计算方法"></a><font color=DodgerBlue>采用分布式并行计算方法</font></h2><p>用批量梯度下降算法来求解大规模数据集的最优解，需要对整个训练集进行循环，计算偏导数和代价，再求和，计算代价非常大。可通过在集群或多核CPU上应用Map-reduce，实现并行计算。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.11%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E9%9B%86%E5%88%92%E5%88%86%E7%A4%BA%E4%BE%8B.png?raw=true" alt="分布式数据集划分示例"></p><h3 id="Map-reduce概念："><a href="#Map-reduce概念：" class="headerlink" title="Map-reduce概念："></a>Map-reduce概念：</h3><ul><li>Map函数将数据分割并映射成不同的区块，分配给计算机机群处理达到分布式运算的效果。</li><li>Reduce 函数将结果汇整，从而输出开发者需要的结果。</li></ul><h3 id="大规模数据集中Map-reduce应用："><a href="#大规模数据集中Map-reduce应用：" class="headerlink" title="大规模数据集中Map-reduce应用："></a>大规模数据集中Map-reduce应用：</h3><ul><li>将数据集分配给多台计算机，让每一台计算机处理一个数据集子集，然后将结果汇总再求和。 </li><li>很多高级的线性代数函数库已经能够利用多核 CPU  的多个核心来并行地处理矩阵运算。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.11%E9%9B%86%E7%BE%A4.png?raw=true" alt="集群"><br><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.12%E5%A4%9A%E6%A0%B8.png?raw=true" alt="多核"></p>]]></content>
      
      
      <categories>
          
          <category> 5. 机器学习系统设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5.3 如何改进机器学习算法?</title>
      <link href="/2017/08/13/5.3%20%E5%A6%82%E4%BD%95%E6%94%B9%E8%BF%9B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
      <url>/2017/08/13/5.3%20%E5%A6%82%E4%BD%95%E6%94%B9%E8%BF%9B%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="5-3-1-机器学习算法一般改进方法"><a href="#5-3-1-机器学习算法一般改进方法" class="headerlink" title="5.3.1 机器学习算法一般改进方法"></a><font color=blue>5.3.1 机器学习算法一般改进方法</font></h1><h2 id="获得更多的训练实例"><a href="#获得更多的训练实例" class="headerlink" title=" 获得更多的训练实例"></a><font color=DodgerBlue> 获得更多的训练实例</font></h2><p>可解决高方差（未来）。</p><a id="more"></a><h2 id="改变特征数量"><a href="#改变特征数量" class="headerlink" title=" 改变特征数量"></a><font color=DodgerBlue> 改变特征数量</font></h2><p>减少特征数量可解决高方差（未来），增加特征数量可解决高偏差（现在）。</p><h2 id="改变多项式次数"><a href="#改变多项式次数" class="headerlink" title=" 改变多项式次数"></a><font color=DodgerBlue> 改变多项式次数</font></h2><p>减少多项式次数可解决高方差（未来），增加多项式次数可解决高偏差（现在）。</p><h2 id="改变正则化参数"><a href="#改变正则化参数" class="headerlink" title=" 改变正则化参数"></a><font color=DodgerBlue> 改变正则化参数</font></h2><h3 id="作用："><a href="#作用：" class="headerlink" title="作用："></a>作用：</h3><p>减小正则化参数可解决高偏差（现在），增加正则化参数可解决高方差（未来）。</p><h3 id="基本思想："><a href="#基本思想：" class="headerlink" title="基本思想："></a>基本思想：</h3><p>基于模型复杂性对其进行惩罚，偏好那些相对简单的能更好的泛化的模型，可以解决过度拟合问题。通过增加一个额外的项到代价函数上，这个项叫做正则化项，消除或降低过拟合的影响，缺点是惩罚会造成欠拟合很难校准。</p><h3 id="正则化分类："><a href="#正则化分类：" class="headerlink" title="正则化分类："></a>正则化分类：</h3><ol><li>L1 正则化：\(C=C_0+λ/n ∑_w |w|\) ，在原始代价函数 \(C_0\) 上加上一个权重绝对值的和。倾向于聚集权重在相对少量的高重要度连接上，而其他权重就会被驱使向 0 接近。 </li><li>L2正则化：\(C=C_0+λ/n ∑_w w^2\)，在原始代价函数 \(C_0\) 上加上一个权重平方和。可以看做是一种寻找小的权重和最小化原始的代价函数之间的折中，\(λ\)  越小就偏向于最小化原始代价函数，反之倾向于小的权重。</li></ol><font color=orangered>λ=0,0.01,0.02,0.04,0.08,…,10</font><h1 id="5-3-2-神经网络改进方法"><a href="#5-3-2-神经网络改进方法" class="headerlink" title="5.3.2 神经网络改进方法"></a><font color=blue>5.3.2 神经网络改进方法</font></h1><p>神经网络优化涉及很多的神经网络超参数，分类如下：</p><ol><li>影响分类正确率：神经网络层数 \( L\)、隐藏层中神经元个数 \( j\)、输出编码方式、正则化参数 \(λ\)。</li><li>影响代价函数曲线下降速度，同时有时也会影响分类正确率：权重初始化、代价函数的选择、神经元激活函数的种类。</li><li>影响模型分类正确率和训练用总体时间：学习的回合数Epoch、小批量数据 mini-batch 的大小、参加训练模型数据的规模。</li><li>影响学习速度，主要体现为代价函数曲线的下降速度：学习率 \(η\)</li></ol><h2 id="5-3-2-1-神经网络改进基本方法"><a href="#5-3-2-1-神经网络改进基本方法" class="headerlink" title="5.3.2.1 神经网络改进基本方法"></a><font color=DodgerBlue>5.3.2.1 神经网络改进基本方法</font></h2><h3 id="建立神经网络的首要原则："><a href="#建立神经网络的首要原则：" class="headerlink" title="建立神经网络的首要原则："></a>建立神经网络的首要原则：</h3><ul><li>对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，针对不同隐藏层的神经网络训练神经网络，然后选择交叉验证集代价最小的神经网络。</li><li>通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。<ul><li>使用较小的神经网络，特征参数较少，容易导致高偏差和欠拟合，但计算代价较小。</li><li>使用较大的神经网络，特征参数较多，容易导致高方差和过拟合，可以通过正则化手段来调整而更加适应数据，虽然计算代价比较大。</li><li>检测过度拟合的明显方法是跟踪测试数据集合上的准确率随训练变化情况，或绘制学习曲线。若测试数据上的准确率不再提升，那么就停止训练。</li></ul></li></ul><h3 id="选择不同的神经元类型："><a href="#选择不同的神经元类型：" class="headerlink" title="选择不同的神经元类型："></a>选择不同的神经元类型：</h3><p> 在神经网络中S型函数激励的神经元较常见，除此之外还有tanh神经元、修正线性神经元等。</p><h3 id="选择不同的代价函数："><a href="#选择不同的代价函数：" class="headerlink" title="选择不同的代价函数："></a>选择不同的代价函数：</h3><p><strong>代价函数必须具备的特性：</strong></p><ul><li>非负性：求和中的所有独立的项都是正数。</li><li>趋于零：若对于所有的训练输入实际的输出接近目标值，代价函数值接近于零。</li></ul><p><strong>二次代价函数：</strong></p><script type="math/tex; mode=display">C(w; b)=\frac {1}{2m}  ∑_x ∥y(x)- a∥2</script><ul><li>神经元是通过改变权重和偏置，并以一个代价函数的偏导数（ \(\frac {∂C}{∂w}\) 和 \(\frac {∂C}{∂b}\) ）决定学习速度。二次代价函数会让学习变得很缓慢，学习缓慢实际上就是这些偏导数很小，当神经元的输出接近1的时候，S曲线变得相当平，\(σ’ (z)\) 就很小了，即 \(\frac {∂C}{∂w}\) 和 \(\frac {∂C}{∂b}\) 会非常小。</li><li>当在神经元犯了明显的错误时，学习速度比学习快接近真实值的时候更加缓慢。<script type="math/tex; mode=display">\begin {cases}C=\frac {(y-a)^2}{2} ⇒^{a=σ(z)}  \frac {(y-σ(z))^2}{2} \\⇒^{z=wx+b} \frac {(y-σ(wx+b))^2}{2}\\\frac {∂C}{∂w}=(σ(z)-y)σ'(z)x ⇒^{x=1,y=0} σ(z) σ'(z)\\ \frac {∂C}{∂b}=(σ(z)-y) σ'(z) ⇒^{x=1,y=0} σ(z) σ'(z)\\\end {cases}</script></li></ul><p><strong>交叉熵代价函数：</strong></p><script type="math/tex; mode=display">C(w; b) =- \frac {1}{n}  ∑_x [y lna + (1-y) ln(1-a)]</script><ul><li>学习速度更快，因为偏导数 \(\frac {∂C}{∂w}\) 和 \(\frac {∂C}{∂b}\) 消除了 \(σ’ (z)\) 的影响； </li><li>\(\frac {∂C}{∂w}\) 和 \(\frac {∂C}{∂b}\) 输出中有误差的控制，误差越大，意味着更快的学习速度。</li><li>在神经元犯了明显错误的时候，学习速度变得更快，且并不依赖于如何设置学习速率。</li><li>若在输出神经元是 S 型神经元时，交叉熵一般都是更好的选择。<script type="math/tex; mode=display">\begin {cases}C = - \frac {1}{n}  ∑_x [y lna + (1-y) ln(1-a)] \\ ⇒^{a=σ(z)}- \frac {1}{n} ∑_x [ylnσ(z)  + (1-y)ln(1-σ(z))] \\σ(z)=\frac {1}{1+e^{-z}} →σ'(z)=σ(z)(1-σ(z))\\ \frac {∂C}{∂w_j}=- \frac {1}{n} ∑_x (\frac {y}{σ(z)} -\frac {1-y}{(1-σ(z)} \frac {σ(z)}{∂w_j} \\⇒^{z=wx+b)} - \frac {1}{n} ∑_x (\frac {y}{σ(z)} -\frac {1-y}{(1-σ(z)} σ'(z) x_j \\⇒^{σ'(z)=σ(z)(1-σ(z))} - \frac {1}{n} ∑_x \frac {σ'(z)x_j}{σ(z)(1-σ(z))} (σ(z)-y) \\=\frac {1}{n}  ∑_x x_j (σ(z)-y)\\\frac {∂C}{∂b_j}=\frac {1}{n} ∑_x (σ(z)-y) \end {cases}</script></li><li>交叉熵推广到很多神经元的多层神经网络：假设  \(y=y_1,y_2,…\) 是输出神经元上的目标值，而 \(a_1^L, a_2^L ,…\) 是实际输出值，那么定义交叉熵： <script type="math/tex">C = - \frac {1}{n} ∑_x ∑_j [y_j lna_j^L  + (1-y_j )ln(1-a_j^L )]</script></li></ul><h3 id="权重初始化优化："><a href="#权重初始化优化：" class="headerlink" title="权重初始化优化："></a>权重初始化优化：</h3><ul><li><p>模型假设：若一神经元网络有1000个输入，根据独立高斯随机变量来选择权重和偏置（被归一化为均值为 0，标准差 1），初始化了第一个隐藏层的权重。<br>若训练集的输入 \(x\) 有一半的输入为 1，另一半为0。某个隐藏神经元的带权输入 \(z=∑_j w_j x_j +b\) ，其中 500 个项消去了，\(z\) 是遍历总共 501 个归一化的高斯随机变量的和，包含 500 个权重项和额外的 1 个偏置项。因此 \(z\)  本身是一个均值为 0标准差为 \(\sqrt {501}  ≈ 22.4\) 的高斯分布。\(z\) 是一个非常宽的高斯分布，\(|z|\) 会变得非常的大，即 \( z ≫ 1\) 或者 \(z ≪ -1\)<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.7%E7%8B%AC%E7%AB%8B%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96.png?raw=true" alt="独立高斯分布权重初始化"></p></li><li><p>问题分析：会出现隐藏神经元饱和现象</p><ul><li>隐藏神经元的输出 \(σ(z) \) 会接近 1 或者 0。也就表示隐藏神经元会饱和。</li><li>在权重中进⾏微小的调整仅仅会给隐藏神经元的激活值带来极其微弱的改变。而这种微弱的改变也会影响网络中剩下的神经元，然后会带来相应的代价函数的改变。这些权重在进⾏梯度下降算法时会学习得⾮常缓慢。</li><li>通过选择交叉熵代价函数可以解决在输出神经元上的错误值上饱和导致学习的下降，但对于隐藏神经元的饱和却一点作⽤都没有。</li></ul></li><li>标准权重初始化：假设有 \(n<em>{in} \) 个输入权重的神经元，使用均值为 0 标准差为 \(\frac {1}{\sqrt {n</em>{in}}}\) 的高斯随机分布初始化这些权重。使用这种方法，会向下挤压高斯分布（有更尖锐的峰值），让神经元更不可能饱和，不大可能遇到学习速度下降的问题。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.8%E6%A0%87%E5%87%86%E6%9D%83%E9%87%8D%E5%88%9D%E5%A7%8B%E5%8C%96.png?raw=true" alt="标准权重初始化"></li></ul><h2 id="5-3-2-2-神经网络的超参数选择步骤"><a href="#5-3-2-2-神经网络的超参数选择步骤" class="headerlink" title=" 5.3.2.2 神经网络的超参数选择步骤"></a><font color=DodgerBlue> 5.3.2.2 神经网络的超参数选择步骤</font></h2><h3 id="首先用宽泛策略先大致搭建一个简单网络："><a href="#首先用宽泛策略先大致搭建一个简单网络：" class="headerlink" title="首先用宽泛策略先大致搭建一个简单网络："></a>首先用宽泛策略先大致搭建一个简单网络：</h3><ul><li>确定神经元类型、代价函数、输出层的编码方式、输出层模式（是否采用softmax）、确定神经网络中隐层的数目以及每一个隐层中神经元的个数。</li><li>使用简化的训练数据，验证简化的网络性能，逐渐增加神经元的个数和网络的层数。</li><li>确定小批量数据的大小, 小批量数据大小的选择独立于网络整体架构外的参数，使用某些可以接受的值作为其他参数的选择，然后进行不同小批量数据大小的尝试。<ul><li>小批量数据太小浪费矩阵库的快速计算能力，太大不能够足够频繁地更新权重。常见大小为bϵ[2,100]</li><li>获得验证准确率的值随时间（非回合）变化的图，选择那个得到最快性能的提升的小批量数据大小。得到了小批量数据大小后就可以对其他的超参数进行优化。</li></ul></li></ul><h3 id="其次调整学习率-η-："><a href="#其次调整学习率-η-：" class="headerlink" title="其次调整学习率 \(η\)："></a>其次调整学习率 \(η\)：</h3><ul><li>首先选择在训练数据上的代价函数立即开始下降而非震荡或增加时的 \(η\) 为阈值的大致估计。 </li><li>若代价函数在训练的前面若干回合开始下降，则逐步增加 \(η\) 的量级，直到找到一个的值使得在开始若干回合代价就开始震荡或者增加。</li><li>相反若代价函数曲线开始震荡或者增加，那就尝试减小量级直到找到代价在开始回合就下降的设定，取阈值的一半就确定了学习速率 。</li></ul><h3 id="再次确定学习的回合数Epoch："><a href="#再次确定学习的回合数Epoch：" class="headerlink" title="再次确定学习的回合数Epoch："></a>再次确定学习的回合数Epoch：</h3><ul><li>提前停止表示在每个回合的最后，都要计算验证集上的分类准确率，当准确率不再提升，就终止它也就确定了迭代次数。另外提前停止也能够避免过度拟合。</li><li>分类准确率在整体趋势下降的时候仍旧会抖动或者震荡。若在准确度刚开始下降的时候就停止，那么肯定会错过更好的选择。一般在分类准确率在一段时间内不再提升的时候终止。建议在更加深入地理解网络训练的方式时，仅仅在初始阶段使用 10 回合不提升规则，然后逐步地选择更久的回合，比如20回合不提升就终止，30回合不提升就终止，以此类推。</li></ul><h3 id="最后确定正则化参数λ-："><a href="#最后确定正则化参数λ-：" class="headerlink" title="最后确定正则化参数λ ："></a>最后确定正则化参数λ ：</h3><ul><li>使用确定出来的 \(η\)，用验证数据来选择好的 \(λ \) 。尝试从 \(λ=1 \) 开始，然后根据验证集上的性能按照因子 10   增加或减少其值。一旦已经找到一个好的量级，可以改进 \(λ \) 的值。 </li><li>确定 \(λ \) 后返回再重新优化 \(η\)。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 5. 机器学习系统设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5.2 如何评估机器学习算法?</title>
      <link href="/2017/08/12/5.2%20%E5%A6%82%E4%BD%95%E8%AF%84%E4%BC%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
      <url>/2017/08/12/5.2%20%E5%A6%82%E4%BD%95%E8%AF%84%E4%BC%B0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>一般先通过交叉验证的方法，获取算法的测试数据，然后运用一系列的评估指标来量化算法优劣，如：学习曲线诊断、偏差方差诊断、偏斜误差度量等。</p><a id="more"></a><h1 id="5-2-1-机器算法评估基础"><a href="#5-2-1-机器算法评估基础" class="headerlink" title="5.2.1 机器算法评估基础 "></a><font color=blue>5.2.1 机器算法评估基础 </font></h1><h2 id="测试集划分方法："><a href="#测试集划分方法：" class="headerlink" title=" 测试集划分方法： "></a><font color=DodgerBlue> 测试集划分方法： </font></h2><h3 id="常规方法："><a href="#常规方法：" class="headerlink" title="常规方法："></a>常规方法：</h3><ol><li>将数据集随机分成训练集（70%）和测试集（30%）。</li><li>算法通过训练集训练后，得到使代价函数 \(J(θ)\) 最小的 \(θ\)参数，获得模型。</li><li>对测试集运用该模型，计算代价函数误差指标 \(J<em>{test} (θ)\) 或误分类比率指标 \(Test</em>{error}\)。</li><li>根据计算出的指标值来评估机器学习算法的优劣。</li></ol><font color=orangered>常规方法弊可能可以很好的拟合数据集，但是可能不能很好的推广到一般情况。</font><h3 id="交叉验证集方法："><a href="#交叉验证集方法：" class="headerlink" title="交叉验证集方法："></a>交叉验证集方法：</h3><ol><li>将数据集随机分成训练集（60%）、交叉验证集（20%）和测试集（20%）。</li><li>算法通过训练集训练后，使代价函数 \(J_{train} (θ)\) 最小的 \(θ\)参数，获得模型。</li><li>对交叉验证集运用该模型，计算得出交叉验证误差 \(J_{cv} (θ)\)，选取代价函数值最小的模型。</li><li>用选出的模型对测试集计算得出推广误差，即代价函数的 \(J_{test} (θ)\)。</li></ol><h2 id="数据泛化与过度拟合："><a href="#数据泛化与过度拟合：" class="headerlink" title=" 数据泛化与过度拟合： "></a><font color=DodgerBlue> 数据泛化与过度拟合： </font></h2><ol><li>数据泛化：模型对于新的数据或新的场景的预测能力或推广能力。</li><li>过度拟合：模型能够很好的拟合已有的数据，但对于新的数据出现很难泛化的现象。</li></ol><font color=orangered>一般当数据泛化表现不理想时，多半是因为出现了欠拟合或过拟合。</font><h1 id="5-2-2-学习曲线诊断"><a href="#5-2-2-学习曲线诊断" class="headerlink" title="5.2.2 学习曲线诊断 "></a><font color=blue>5.2.2 学习曲线诊断 </font></h1><p>分别以训练集误差\(J<em>{train} (θ)\)和交叉验证集误差\(J</em>{cv} (θ)\)为纵坐标，以训练集的数量m为横坐标，绘制成一张图表。&lt;/br&gt;<br>可获得训练集m大小的最优选择，以及是否存在欠拟合或过拟合的情况。欠拟合代表的是偏差（过去）比较大；过拟合代表的是方差（未来）比较大。</p><h2 id="曲线类型："><a href="#曲线类型：" class="headerlink" title="曲线类型："></a><font color=DodgerBlue>曲线类型：</font></h2><p><strong>高偏差学习曲线：增加训练集数量基本上对算法的改进没有帮助</strong></p><ul><li>训练集误差曲线上升比较陡峭，验证集误差下降的比较缓慢。</li><li>当训练样本数量达到或超过了特定的数值，训练误差和验证误差就趋于接近且不变。</li><li>且\(J<em>{train} (θ)\)和\(J</em>{cv} (θ)\)的误差都处于较高的误差值上（高偏差的主要判断依据）。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.2%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF%E8%AF%8A%E6%96%AD%E9%AB%98%E5%81%8F%E5%B7%AE.png?raw=true" alt="学习曲线诊断高偏差"> </li></ul><p><strong>高方差学习曲线：增加训练集数量对算法的改进很有帮助。</strong></p><ul><li>训练误差相对较小，且随样本数量增加的涨幅比较平缓。</li><li>训练误差\(J<em>{train} (θ)\)很小，验证误差\(J</em>{cv} (θ)\)很大。 </li><li>随着训练集样本数量增加，训练误差和验证误差这两条学习曲线正在相互靠近。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.2%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF%E8%AF%8A%E6%96%AD%E9%AB%98%E6%96%B9%E5%B7%AE.png?raw=true" alt="学习曲线诊断高方差"></li></ul><h2 id="应用："><a href="#应用：" class="headerlink" title="应用："></a><font color=DodgerBlue>应用：</font></h2><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.3%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF%E9%AB%98%E5%81%8F%E5%B7%AE.png?raw=true" alt="学习曲线高偏差"><br><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.4%E5%AD%A6%E4%B9%A0%E6%9B%B2%E7%BA%BF%E9%AB%98%E6%96%B9%E5%B7%AE.png?raw=true" alt="学习曲线高方差"></p><h1 id="5-2-3-偏差和方差诊断"><a href="#5-2-3-偏差和方差诊断" class="headerlink" title=" 5.2.3 偏差和方差诊断"></a><font color=blue> 5.2.3 偏差和方差诊断</font></h1><p>分别以训练集代价函数误差\(J<em>{train} (θ)\)和交叉验证集的代价函数误差\(J</em>{cv} (θ)\)为纵坐标，以算法超级参数的不同取值为横坐标，绘制一张图表。获得是否存在欠拟合或过拟合的情况，解决算法超级参数调优问题。</p><h2 id="多项式模型选择："><a href="#多项式模型选择：" class="headerlink" title="多项式模型选择："></a><font color=DodgerBlue>多项式模型选择：</font></h2><ul><li>欠拟合：训练集误差\(J<em>{train} (θ)\)数值比较大，且与交叉验证集误差\(J</em>{cv} (θ)\)相近<script type="math/tex">J_{cv} (θ)≈J_{train} (θ)</script></li><li>过拟合：训练集误差\(J<em>{train} (θ)\)数值比较小，且交叉验证集误差\(J</em>{cv} (θ)\)远大于训练集误差\(J<em>{train} (θ)\) $$J</em>{cv} (θ)≫J_{train} (θ)$$</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.5%E5%A4%9A%E9%A1%B9%E5%BC%8F%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE%E8%AF%8A%E6%96%AD.png?raw=true" alt="多项式偏差方差诊断"></p><h2 id="正则化-λ-参数选择："><a href="#正则化-λ-参数选择：" class="headerlink" title="正则化\(λ\)参数选择："></a><font color=DodgerBlue>正则化\(λ\)参数选择：</font></h2><ul><li>过拟合：当\(λ\)较小时，训练集误差\(J<em>{train} (θ)\)较小，交叉验证集误差\(J</em>{cv} (θ)\)较大。</li><li>欠拟合：随着λ 的增大，训练集误差\(J<em>{train} (θ)\)不断增加，而交叉验证集误差\(J</em>{cv} (θ)\)则是先减小后增加。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.6%E6%AD%A3%E5%88%99%E5%8F%82%E6%95%B0%E5%81%8F%E5%B7%AE%E6%96%B9%E5%B7%AE%E8%AF%8A%E6%96%AD.png?raw=true" alt="正则参数偏差方差诊断"></p><h1 id="5-2-4-偏斜类的误差度量"><a href="#5-2-4-偏斜类的误差度量" class="headerlink" title="5.2.4 偏斜类的误差度量"></a><font color=blue>5.2.4 偏斜类的误差度量</font></h1><p>偏斜类指的是训练集中有非常多的同一种类的实例，只有非常少的或没有其它类的实例。在误差分析时，通过误分类比率来度量误差、评判算法效果是没有意义的。以下以人群患癌为例，若实际患癌的比例为0.5%，通过交叉验证集进行误差分析，将算法准确率提高至99.2%或99.5%是无意义的。</p><h2 id="混淆矩阵："><a href="#混淆矩阵：" class="headerlink" title="混淆矩阵："></a><font color=DodgerBlue>混淆矩阵：</font></h2><p>根据算法预测值和实际值之间的不同组合得到四种类型：</p><script type="math/tex; mode=display">\left [\begin {matrix}TP&FP\\FN&TN\end {matrix}\right ]</script><ul><li>正确肯定：预测为真，实际为真（True Positive—TP）。</li><li>正确否定：预测为假，实际为假（True Negative—TN）。</li><li>错误肯定：预测为真，实际为假（False Positive—FP）。</li><li>错误否定：预测为假，实际为真（False Negative—FN）。</li></ul><h2 id="偏斜类误差度量："><a href="#偏斜类误差度量：" class="headerlink" title="偏斜类误差度量："></a><font color=DodgerBlue>偏斜类误差度量：</font></h2><ul><li>查准率：正确肯定在肯定中的占比，越高越好<script type="math/tex">Precision=\frac {True Positives}{no.of predict positive}=\frac {TP}{TP+FP}</script>（在所有预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比。）</li><li>查全率：正确肯定在实际值为真中的占比，越高越好<script type="math/tex">Recall=\frac {True Positives}{no.of actual positive}=\frac {TP}{TP+FN}</script>（在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比（以上预测病人肿瘤为良性的非机器学习算法，其查全率是0。）</li></ul><h2 id="查准率和召回率的权衡："><a href="#查准率和召回率的权衡：" class="headerlink" title="查准率和召回率的权衡："></a><font color=DodgerBlue>查准率和召回率的权衡：</font></h2><ul><li>若算法有较高的查准率和较高的查全率，则认为这个算法是一个好的算法。在实际应用中通常需要保证查准率和召回率的相对平衡：若只在非常确信的情况下预测为真，即希望更高的查准率，可使用比0.5 更大的阀值，如0.7，0.9；若希望提高查全率，可使用比0.5 更小的阀值，如0.3。</li><li>查准率和召回率的权衡：选择最高的阀值<script type="math/tex">F_1  Score=2 \frac {Precision*Recall}{Precision+Recall}</script></li></ul>]]></content>
      
      
      <categories>
          
          <category> 5. 机器学习系统设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>5.1 机器学习系统设计最佳实践</title>
      <link href="/2017/08/09/5.1%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"/>
      <url>/2017/08/09/5.1%20%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[<p><img src= "/img/loading.gif" data-lazy-src="/img/ai/5.1%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5.png?raw=true" alt="机器学习系统最佳实践"></p><a id="more"></a><h1 id="定义问题"><a href="#定义问题" class="headerlink" title=" 定义问题 "></a><font color=blue> 定义问题 </font></h1><p>进行需求分析，定义问题。不同类型的问题有各自适用的机器学习模型，如：图像分析采用CNN模型、推荐相关问题有专门的推荐算法、安全相关问题有异常检测模型等。所以脱离具体问题去讨论算法或模型是没有意义的。</p><h1 id="收集数据"><a href="#收集数据" class="headerlink" title="收集数据 "></a><font color=blue>收集数据 </font></h1><p><strong>收集数据的重要性：</strong>若数据集足够的大且包含了足够多的信息，在机器学习领域中普遍的共识是“取得成功的人不是拥有最好算法的人，而是拥有最多数据的人”。</p><p><strong>数据集获取方式：</strong></p><ol><li>众包外包：从第三方获取被标记的数据集。</li><li>手动收集数据并标记数据：需要考虑手动收集标记需要的时间。</li><li>人工数据合成：在已有的数据上加上噪声合成新的数据，从而获得更多的数据。</li></ol><h1 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程 "></a><font color=blue>特征工程 </font></h1><p>将原始数据转变为特征，通过这些特征可以很好的描述这些数据，并且利用这些特征建立模型，能让模型在未知的数据上的表现出最优的性能。&lt;/br&gt;</p><p><strong>主要工作：</strong></p><ol><li>从原始数据中清洗出特征数据和标注数据。</li><li>对清洗出的特征和标注数据进行进一步处理，最终生成的数据集供训练和测试模型使用。如：异常点去除、样本采集、特征降维，特征归一化、特征组合等过程。</li></ol><p><strong>一般步骤：</strong></p><ol><li>提取特征：运用“头脑风暴”法，不考虑特征的重要性，从原始数据中提取尽可能多的特征。</li><li>设计特征：根据具体的问题，可以采用自动特征提取、手工构造特征，或两者混合。</li><li>选择特征：使用不同的特征重要性评分和特征选择方法进行特征选择。</li><li>评估模型：使用选择的特征进行建模，同时使用已知的数据来评估模型的精度。</li></ol><p><strong>特征选择原则：</strong></p><ol><li>特征能提供有用信息：若某一特征概率均匀分布即为无用。</li><li>特征之间需独立：获取非冗余信息。</li><li>特征需简单：缩短对特征学习的时间。</li></ol><h1 id="选择算法"><a href="#选择算法" class="headerlink" title=" 选择算法 "></a><font color=blue> 选择算法 </font></h1><ol><li>整体原则：头脑风暴列出尽可能多的算法，从中选取最简单、最适合的算法，快速实现的算法，建立模型。</li><li>若想要预测目标变量的值，则可以选择监督学习算法：需要进一步确定目标变量类型，若目标变量是离散型，则可以选择分类器算法；若目标变量是连续型的数值，则需要选择回归算法。</li><li>若不是预测目标变量的值，可以选择无监督学习算法：进一步分析，若需要将数据是离散的，则使用聚类算法；若需要估计数据与每个分组的相似程度，则需要使用密度估计算法。</li></ol><h1 id="训练算法"><a href="#训练算法" class="headerlink" title="训练算法"></a><font color=blue>训练算法</font></h1><p>将训练数据集输入到算法，完成算法训练，获得训练模型。</p><h1 id="测试算法"><a href="#测试算法" class="headerlink" title=" 测试算法 "></a><font color=blue> 测试算法 </font></h1><p>将测试数据集输入模型，评估模型。对于监督学习则评估模型预测目标变量值的准确率，对于非监督学习则利用其他的评测手段来检验算法的成功率。</p><h1 id="系统调优"><a href="#系统调优" class="headerlink" title="系统调优"></a><font color=blue>系统调优</font></h1><p>根据验证的结果，运用多种工具，评估算法，如：绘制学习曲线，进行误差分析等。根据最终分析的结果，改进或更换特征或算法。</p>]]></content>
      
      
      <categories>
          
          <category> 5. 机器学习系统设计 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4.7 循环神经网络</title>
      <link href="/2017/08/06/4.7%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2017/08/06/4.7%20%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="4-7-1-循环神经网络基本模型"><a href="#4-7-1-循环神经网络基本模型" class="headerlink" title="4.7.1 循环神经网络基本模型"></a><font color=blue>4.7.1 循环神经网络基本模型</font></h1><h2 id="4-7-1-1-单向循环神经网络"><a href="#4-7-1-1-单向循环神经网络" class="headerlink" title=" 4.7.1.1 单向循环神经网络"></a><font color=DodgerBlue> 4.7.1.1 单向循环神经网络</font></h2><h3 id="模型描述："><a href="#模型描述：" class="headerlink" title="模型描述："></a>模型描述：</h3><ul><li>输入：\(X_t\)向量表示\(t\)时刻输入层的值。</li><li>隐藏层：\(A_t\)向量表示\(t\)时刻隐藏层的值，隐藏层的节点数与向量\(A_t\)的维度相同。</li><li>输入层到隐藏层权重矩阵：\(U\)向量表示输入层到隐藏层的权重矩阵。</li><li>隐藏层到输出层的权重矩阵：\(V\)向量表示隐藏层到输出层的权重矩阵。</li></ul><a id="more"></a><ul><li>上次输出权重矩阵：隐藏层的值\(A<em>t\)不仅仅取决于当前这次的输入\(X_t\)，还取决于上一次隐藏层的值\(A</em>{t-1}\)。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。 <script type="math/tex; mode=display">A_t=f(U*X_t+W*A_{t-1})\ \ \ \ \ \ \ （f为激活函数）</script></li><li>输出层：\(o\)向量表示输出层的值。<script type="math/tex">h_t=g(V*A_t )\ \ \ \ \  \ \ \  （ g为激活函数）</script></li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.23%E5%8D%95%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.png?raw=true" alt="单向循环神经网络模型"></p><h3 id="特点："><a href="#特点：" class="headerlink" title="特点："></a>特点：</h3><ol><li>循环神经网络神经元的输出在下一个时间戳直接作用到自身，理论上神经元的输出可以受前面任意多个时刻输入的影响。而在前馈神经⽹络中，信息总是向前传播，从不反向回馈。</li><li>理论上循环神经网络可以处理任意长度的输入。</li><li>循环神经网络拥有记忆功能（中间状态也被保存下来），解决了无法对时间序列上的变化进行建模问题，利用历史的信息，可以一起用于预测输出序列，解决一些重要复杂的问题。</li><li>循环神经网络是一个在时间上传递的神经网络，随着时间深度的加长，也会出现“梯度消失”或“梯度爆炸”现象，即RNN 会丧失学习到连接很远的信息的能力。</li></ol><h2 id="4-7-1-2-双向循环神经网络"><a href="#4-7-1-2-双向循环神经网络" class="headerlink" title=" 4.7.1.2 双向循环神经网络"></a><font color=DodgerBlue> 4.7.1.2 双向循环神经网络</font></h2><ul><li>正向计算和反向计算不共享权重。</li><li>在序列信号分析中，同时利用历史和未来的信息，来解决某些复杂的问题。</li></ul><script type="math/tex; mode=display">A_t=f(U*X_t+W*A_{t-1})  \\       A_t'=f(U'*X_t+W'*A_{t-1}')    \\y_t=g(V*A_t+V'*A_t')</script><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.24%E5%8F%8C%E5%90%91%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E5%9B%BE.png?raw=true" alt="双向循环神经网络模型"></p><h2 id="4-7-1-3-BPTT算法（沿时间反向传播）"><a href="#4-7-1-3-BPTT算法（沿时间反向传播）" class="headerlink" title=" 4.7.1.3 BPTT算法（沿时间反向传播）"></a><font color=DodgerBlue> 4.7.1.3 BPTT算法（沿时间反向传播）</font></h2><p><strong>沿时间反向传播算法—Backpropgattion Through Time</strong></p><ol><li>前向计算每个神经元的输出值。</li><li>反向计算每个神经元的误差项值，它是代价函数\(C\)对神经元j的加权输入的偏导数</li><li>计算每个权重的梯度。</li><li>最后再用随机梯度下降算法更新权重。</li></ol><h1 id="4-7-2-长短时间记忆循环神经网络"><a href="#4-7-2-长短时间记忆循环神经网络" class="headerlink" title="4.7.2 长短时间记忆循环神经网络"></a><font color=blue>4.7.2 长短时间记忆循环神经网络</font></h1><p>长短时间记忆循环神经网络可以解决在时间上梯度消失的问题，通过门的开关实现记住长期信息的功能，从而可以学习长期依赖信息。广泛应用于语音识别、图片描述、自然语言处理等领域中。</p><h2 id="4-7-2-1-LSTM概述"><a href="#4-7-2-1-LSTM概述" class="headerlink" title=" 4.7.2.1 LSTM概述"></a><font color=DodgerBlue> 4.7.2.1 LSTM概述</font></h2><ol><li>信息被存放在循环网络正常信息流之外的门控单元中，通过门控制单元存储、写入或读取信息。通过门的开关判定存储哪些信息，以及何时允许读取、写入或清除信息。</li><li>门依据接收到的信号并用自有的权重集对信息进行筛选，根据其强度和导入内容决定是否允许信息通过。这些权重会通过循环网络的学习过程进行调整。</li><li>门是一种让信息选择式通过的方法，包含一个 sigmoid 神经网络层和一个 pointwise 乘法操作，Sigmoid 层输出 0 到 1 之间的数值，描述每个部分有多少量可以通过。</li></ol><h2 id="4-7-2-2-LSTM模型"><a href="#4-7-2-2-LSTM模型" class="headerlink" title=" 4.7.2.2 LSTM模型"></a><font color=DodgerBlue> 4.7.2.2 LSTM模型</font></h2><h3 id="基本模型："><a href="#基本模型：" class="headerlink" title="基本模型："></a>基本模型：</h3><ul><li>在RNN的隐藏层增加一个新的状态\(C\)，让它来保存长期的状态，称为单元状态(cell state)。</li><li>输入：当前时刻网络的输入值\(X<em>t\)、上一时刻LSTM的输出值\(h</em>{t-1}\)、以及上一时刻的单元状态\(C_{t-1}\)。</li><li>输出：当前时刻LSTM输出值\(h_t\)、和当前时刻的单元状态\(C_t\)。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.25LSTM%E6%A8%A1%E5%9E%8B.png?raw=true" alt="LSTM模型"></p><h3 id="长期单元状态-C-的控制："><a href="#长期单元状态-C-的控制：" class="headerlink" title="长期单元状态\(C\)的控制："></a>长期单元状态\(C\)的控制：</h3><ul><li>开关1负责控制继续保存长期单元状态\(C_{t-1}\)。</li><li>开关2负责控制把即时状态\(C_t’\)输入到长期单元状态\(C_t\)。</li><li>开关3负责控制是否把长期单元状态\(C_t\)作为当前的输出\(h_t\)。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.26%E9%95%BF%E6%9C%9F%E5%8D%95%E5%85%83%E7%8A%B6%E6%80%81%E6%8E%A7%E5%88%B6.png?raw=true" alt="长期单元状态控制"></p><h3 id="LSTM模型参数：共有八组："><a href="#LSTM模型参数：共有八组：" class="headerlink" title="LSTM模型参数：共有八组："></a>LSTM模型参数：共有八组：</h3><ol><li>正向传播：遗忘门的权重矩阵\(w_f\)和偏置项\(b_f\)；输入门的权重矩阵\(w_i\)和偏置项\(b_i\)；输出门的权重矩阵\(w_o\)和偏置项\(b_o\)；以及计算单元状态的权重矩阵\(w_C\)和偏置项\(b_C\)。</li><li>反向传播：遗忘门的权重矩阵\(w_f’\)和偏置项\(b_f’\)；输入门的权重矩阵\(w_i’\)和偏置项\(b_i’\)；输出门的权重矩阵\(w_o’\)和偏置项\(b_o’\)；以及计算单元状态的权重矩阵\(w_C’\)和偏置项\(b_C’\)。</li></ol><h2 id="4-7-2-3-LSTM工作原理："><a href="#4-7-2-3-LSTM工作原理：" class="headerlink" title=" 4.7.2.3 LSTM工作原理："></a><font color=DodgerBlue> 4.7.2.3 LSTM工作原理：</font></h2><ol><li>第一步：通过一个忘记门决定从上一个长期单元状态中丢弃什么信息。该门会读取 \(h<em>{t-1}\)和\(x_t\)，输出一个在 0 到 1之间的数值，给长期单元状态 \(C</em>{t-1}\)，1 表示“完全保留”，0 表示“完全舍弃”。<script type="math/tex">f_t=σ(w_f [h_{t-1},x_t ]+ b_f)</script></li><li>第二步：确定什么样的新信息被存放在当前的即刻长期单元状态中 sigmoid 层称 “输入门层”    决定将要更新什么值：\(i<em>t=σ(w_i [h</em>{t-1},x<em>t]+ b_i)\) tanh    层创建一个新的候选值向量\(C_t’\)，会被加入到状态中：$$C_t’=tanh(w_C [h</em>{t-1},x_t ]+ b_C)$$</li><li>第三步：更新长期单元状态信息。将\(C<em>{t-1}\)更新为 \(C_t\)，把旧状态\(C</em>{t-1}\)与 \(f<em>t \) 相乘，丢弃掉确定需要丢弃的信息，接着加上\(i_t<em>C_t’\)这就是新的候选值，决定更新每个状态的程度进行变化。   $$C_t=f_t</em>C</em>{t-1}+i_t*C_t’$$</li><li>最后一步：基于长期单元状态信息确定输出值。首先运行 sigmoid    层确定单元状态的哪个部分将输出出去，接着把单元状态通过 tanh进行处理（得到一在-1到1之间的值）并将它和 sigmoid    门的输出相乘，最终仅仅会输出确定要输出的那部分。<script type="math/tex; mode=display">o_t=σ(w_o [h_{t-1},x_t ]+ b_o) \\h_t=o_t*tanh(C_t)</script></li></ol><font color=OrangeRed> 存在的问题：只能记住10-20个时间状态。</font> ]]></content>
      
      
      <categories>
          
          <category> 4. 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4.6 卷积神经网络</title>
      <link href="/2017/08/05/4.6%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2017/08/05/4.6%20%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<p>卷积神经网络特别适用于图像识别。特点如下：&lt;/br&gt;<br>A.    对于卷积神经网络来说，并不是所有上下层神经元都能直接相连，而是通过“卷积核”作为中介。同一个卷积核在所有图像是共享的，图像通过卷积操作后仍然保留原先的位置关系。&lt;/br&gt;<br>B.    若没有卷积操作，图像学习的参数量是灾难级的。&lt;/br&gt;<br>C.    利用语音语谱结构中的局部信息，卷积神经网络照样能应用在语音识别中。</p><font color=OrangeRed> 1. 在普通的全连接网络或卷积神经网络中，每层神经元的信号只能向下一层传播，样本的处理在各个时刻独立，因此又被成为前向神经网络(Feed-forward Neural Networks)。</br>2. 可以以傅里叶变换和滤波器的概念去理解卷积的作用。</font><a id="more"></a><h1 id="4-6-1-卷积神经网络基本模型及基本概念"><a href="#4-6-1-卷积神经网络基本模型及基本概念" class="headerlink" title=" 4.6.1 卷积神经网络基本模型及基本概念"></a><font color=blue> 4.6.1 卷积神经网络基本模型及基本概念</font></h1><p>以MNIST数字图像分类输入28×28像素为例，基本模型如下：输入层有28×28个输入神经元，这些神经元用于对 MNIST 图像的像素强度进行编码；卷积层采用一个5×5局部感受野和 20 个特征映射，其结果是一个20×24×24隐藏特征神经元层；池化层采用2×2最大值池化，遍及20个特征映射，结果是一个20×12×12隐藏特征神经元层；全连接隐藏层连接最大值池化层的每一个神经元；输出层也是一个全连接层。</p><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.16%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%80%E8%88%AC%E7%BB%93%E6%9E%84.png?raw=true" alt="卷积神经网络一般结构"><br><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.17MNIST%E5%9B%BE%E7%89%87%E7%BB%93%E6%9E%84.png?raw=true" alt="MNIST图片结构"></p><h2 id="局部感受野"><a href="#局部感受野" class="headerlink" title=" 局部感受野: "></a><font color=DodgerBlue> 局部感受野: </font></h2><ul><li><p>将输入像素连接到一个隐藏神经元层，不是把每个输入像素连接到每个隐藏神经元，而是把输入图像进行小的局部区域的连接。</p><ul><li><p>将第一个隐藏层中的每个神经元连接到一个输入神经元的一个小区域。如\(5×5\)的区域（25像素）。这个输入图像的区域被称为隐藏神经元的局部感受野。每个连接学习一个权重，隐藏神经元同时也学习一个总的偏置。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.185x5%E5%B1%80%E9%83%A8%E6%84%9F%E5%8F%97%E9%87%8E.png?raw=true" alt="5x5局部感受野"></p></li><li><p>然后在整个输入图像上交叉移动局部感受野。对于每个局部感受野，在第一个隐藏层中有一个不同的隐藏神经元。从左上⻆开始一个局部感受野，然后往右一个像素移动局部感受野，连接到第二个隐藏神经元。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.194.185x5%E5%B1%80%E9%83%A8%E6%84%9F%E5%8F%97%E9%87%8E%E7%A7%BB%E5%8A%A8.png?raw=true" alt="5x5局部感受野移动"></p></li></ul></li><li>如此重复构建起第一个隐藏层。若有一个\(28×28\)的输入图像，\(5×5\)的局部感受野，那么隐藏层中就会有\(24×24\)个神经元。</li><li>局部感受野每次移动可以使用不同的跨距：可以往右（或下）移动 2 个像素的局部感受野，这种情况下使用了 2 个跨距，在这里大部分时候会固定使用 1 的跨距。</li></ul><h2 id="共享权重和偏置"><a href="#共享权重和偏置" class="headerlink" title=" 共享权重和偏置: "></a><font color=DodgerBlue> 共享权重和偏置: </font></h2><ul><li>对\(24×24\)隐藏神经元中的每一个使用相同的权重和偏置，对第 \( j\)，\(k\) 个隐藏神经元，输出为：   <script type="math/tex">σ(b+∑_{l=0}^4 ∑_{m=0}^4 w_{l,m}a_{j+l,k+m})</script><br>\(σ \)是神经元的激活函数可以是 S 型函数，\(b \)是偏置的共享值，\(w<em>{l,m}\)是一个共享权重的5×5数组，\(a</em>{x,y}\)来表示位置为\( x,y\)的输入激活值。</li><li>第一个隐藏层的所有神经元检测完全相同的特征，只是在输入图像的不同位置。在图像中应用相同的特征检测器是非常有用的，能很好地适应图像的平移不变性。</li><li>把从输入层到隐藏层的映射称为一个特征映射，把定义特征映射的权重称为共享权重，把以这种方式定义特征映射的偏置称为共享偏置。共享权重和偏置经常被称为一个卷积核或者滤波器。</li><li>为了完成图像识别需要超过一个的特征映射，一个完整的卷积层由几个不同的特征映射组成，假设有 3 个特征映射,  每个特征映射定义为一个\(5×5\)共享权重和单个共享偏置的集合。其结果是网络能够检测 3 种不同的特征，每个特征都在整个图像中可检测。   在实践中卷积网络可能使用很多的特征映射。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.20%E7%89%B9%E5%BE%81%E6%98%A0%E5%B0%84.png?raw=true" alt="特征映射"></li><li>共享权重和偏置的优点：它大大减少了参与的卷积网络的参数。<ul><li>对于每个特征映射需要\(5×5\)个共享权重，加上一个共享偏置，每个特征映射需要 26 个参数。 </li><li>若有 20 个特征映射，那么总共有\(20×26\)个参数来定义卷积层。作为对比，假设有一个全连接的第一层，具有\(28×28\)个输入神经元，和一个相对适中的 30   个隐藏神经元，总共有\(784×30\)个权重，加上额外30 个偏置，共有 23550 个参数。</li></ul></li></ul><h2 id="池化层"><a href="#池化层" class="headerlink" title=" 池化层: "></a><font color=DodgerBlue> 池化层: </font></h2><ul><li>池化层通常紧接着在卷积层之后使用，从卷积层输出的每一个特征映射，并且为它们准备一个凝缩的特征映射，用来是简化从卷积层输出的信息。</li><li>常用的池化程序：最大值混合（取阵列中激活值的最大值）、L2 混合（取阵列中激活值的平方和的平方根）。在实践中，两种技术都被广泛应用。</li><li>池化层的每个单元可能概括了前一层的一个\(2×2\)的区域，在最大值混合中，一个混合单元简单地输出其\(2×2\)输入区域的最大激活值，卷积层有\(24×24\)个神经元输出，混合后得到\(12×12\)个神经元。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.21%E6%B7%B7%E5%90%88%E5%B1%82.png?raw=true" alt="混合层"></p><h1 id="4-6-2-卷积神经网络基本参数"><a href="#4-6-2-卷积神经网络基本参数" class="headerlink" title=" 4.6.2 卷积神经网络基本参数 "></a><font color=blue> 4.6.2 卷积神经网络基本参数 </font></h1><h2 id="图像输入参数input："><a href="#图像输入参数input：" class="headerlink" title=" 图像输入参数input： "></a><font color=DodgerBlue> 图像输入参数input： </font></h2><p>需要做卷积的输入图像，是一张量shape为[batch, in_height, in_width, in_channels]，具体含义是[训练时一个batch的图片数量, 图片高度, 图片宽度, 图像通道数]，注意这是一个4维的Tensor，要求类型为float32和float64其中之一。</p><h2 id="图像边界填充参数padding："><a href="#图像边界填充参数padding：" class="headerlink" title=" 图像边界填充参数padding： "></a><font color=DodgerBlue> 图像边界填充参数padding： </font></h2><ol><li>相同填充：为了保证卷积前后图像的像素不变，在卷积过后，超出图像边框的部分使用补零操作，使得输入输出的图像尺寸相同。在TensorFlow的padding参数为“SAME”。</li><li>有效填充：采用缩小像素值的办法，在TensorFlow的padding参数值为“VALID”。</li></ol><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.22%E5%8D%B7%E7%A7%AF%E5%A1%AB%E5%85%85%E6%96%B9%E5%BC%8F.png?raw=true" alt="填充方式"></p><h2 id="卷积核参数filter："><a href="#卷积核参数filter：" class="headerlink" title=" 卷积核参数filter： "></a><font color=DodgerBlue> 卷积核参数filter： </font></h2><p>相当于CNN中的卷积核，是一张量shape为[filter_height, filter_width, in_channels, out_channels]，具体含义是[卷积核高度，卷积核宽度，图像输入通道数，图像输出通道数]，要求类型与参数input相同,filter的通道数要求与input的in_channels一致，有一个地方需要注意，第三维in_channels，就是参数input的第四维。</p><h2 id="步长参数strides：卷积时在图像每一维的步长，是一维向量长度大于等于4"><a href="#步长参数strides：卷积时在图像每一维的步长，是一维向量长度大于等于4" class="headerlink" title=" 步长参数strides：卷积时在图像每一维的步长，是一维向量长度大于等于4  "></a><font color=DodgerBlue> 步长参数strides：卷积时在图像每一维的步长，是一维向量长度大于等于4  </font></h2><ol><li>第一个元素值一般为1：表示在输入batch维度上的移动步长，表示不跳过任何一个样本。</li><li>最后一个元素一般为 1：表示在输入通道维度上的移动步长，表示不跳过任何一个输入的通道。</li><li>中间元素表示在真实图像维度上的移动步长，若为二维图像则分别是纵轴和横轴步长。</li></ol><h2 id="GPU加速参数use-cudnn-on-gpu："><a href="#GPU加速参数use-cudnn-on-gpu：" class="headerlink" title=" GPU加速参数use_cudnn_on_gpu： "></a><font color=DodgerBlue> GPU加速参数use_cudnn_on_gpu： </font></h2><p>bool类型，是否使用cudnn加速，默认为true。 </p><h2 id="池化核参数："><a href="#池化核参数：" class="headerlink" title=" 池化核参数： "></a><font color=DodgerBlue> 池化核参数： </font></h2><ol><li>ksize：池化的核大小。可以用kernel_h和kernel_w分别设定。</li><li>padding: 和卷积层的pad的一样，设定图像边界处理方式。</li><li>stride: 池化的步长，可以用stride_h和stride_w来设置。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 4. 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4.5 神经网络梯度消失问题</title>
      <link href="/2017/08/03/4.5%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98/"/>
      <url>/2017/08/03/4.5%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="4-5-1-梯度消失现象"><a href="#4-5-1-梯度消失现象" class="headerlink" title=" 4.5.1 梯度消失现象"></a><font color=blue> 4.5.1 梯度消失现象</font></h1><p>在理论上的深度网络比浅层网络更加强大，但在实际中会发现深度神经网络并不能比浅层网络性能好太多，不同层的学习速度差异很大，根本的原因是的学习的速度下降了。&lt;/br&gt;<br>在某些深度神经网络中，在隐藏层BP的时候梯度倾向于变小。这意味着在前面的隐藏层中的神经元学习速度要慢于后面的隐藏层。这个现象也被称作是消失的梯度问题。</p><a id="more"></a><h1 id="4-5-2-梯度消失原因分析"><a href="#4-5-2-梯度消失原因分析" class="headerlink" title=" 4.5.2 梯度消失原因分析"></a><font color=blue> 4.5.2 梯度消失原因分析</font></h1><p>假设一神经网络，有三层隐藏层且每一层都只有一个单一的神经元：\(w<em>1,w_2,…\)是权重，\(b_1,b_2,…\)是偏置，\(C \)则是某个代价函数，从第 \(j\)个神经元的输出\(a_j=σ(z_j)\)，其中\(σ\)是通常的 S   型激活函数，\(z_j=w_j∙a</em>{j-1}+b_j\)是神经元的带权输入，代价函数\(C \) 是网络输出\(a_4\)的函数。</p><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.14%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E6%A8%A1%E5%9E%8B.png?raw=true" alt="梯度消失模型"></p><h2 id="计算第一个隐藏神经元梯度-∂C-∂b-1-："><a href="#计算第一个隐藏神经元梯度-∂C-∂b-1-：" class="headerlink" title=" 计算第一个隐藏神经元梯度 \(∂C/∂b_1\)： "></a><font color=DodgerBlue> 计算第一个隐藏神经元梯度 \(∂C/∂b_1\)： </font></h2><p>假设对偏置 \(b_1\) 进行了微小的调整\(\Delta b_1\)，会导致网络中剩下的元素一系列的变化。首先会对第一个隐藏元输出产生意个\(\Delta a_1\)的变化，这样就会导致第二个神经元的带权输入产生 \(\Delta z_2\)的变化，从第二个神经元输出随之发生 \(\Delta a_2\)的变化，以此类推，最终会对代价函数产生 \(\Delta C\)的变化： \(∂C/∂b_1≈\frac {\Delta C}{\Delta b_1} \)</p><script type="math/tex; mode=display">\begin {cases}a_1=σ(z_1 )=σ(w_1a_0+b_1)\\ → \Delta a_1=\frac {∂σ(w_1a_0+b_1)}{∂b_1}\Delta b_1= σ'(z_1 )\Delta b_1\\z_2=w_2a_1+b_2 \\→ \Delta z_2=\frac {∂σ(z_2)}{∂a_1} \Delta a_1= w_2 \Delta a_1=w_2 σ'(z_1 ) \Delta b_1\\...\\...\\\Delta C=σ'(z_1)w_2 σ'(z_2)w_3 σ'(z_3)w_4 σ'(z_4) \frac {∂C}{∂a_4} \Delta b_1 \\→ \frac {∂C}{∂b_1}=σ'(z_1)w_2 σ'(z_2)w_3 σ'(z_3)w_4 σ'(z_4)\frac {∂C}{∂a_4}\\\end {cases}</script><h2 id="梯度消失问题："><a href="#梯度消失问题：" class="headerlink" title=" 梯度消失问题："></a><font color=DodgerBlue> 梯度消失问题：</font></h2><ul><li>S函数导数在 \(σ′(0) = 1/4\) 时达到最高。若使用标准方法来初始化网络中的权重，那么会使用一个均值为 0 标准差为 1 的高斯分布。因此所有的权重通常会满足 \(|w_j | &lt; 1\)，则\(w_j×σ’(z_j)&lt; 1/4\)</li><li>当进行了所有这些项的乘积时，最终结果肯定会指数级下降，项越多，乘积的下降的越快，即梯度消失。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.15S%E5%9E%8B%E5%87%BD%E6%95%B0%E5%AF%BC%E6%95%B0.png?raw=true" alt="S型函数导数"></li></ul><h2 id="梯度激增问题："><a href="#梯度激增问题：" class="headerlink" title=" 梯度激增问题："></a><font color=DodgerBlue> 梯度激增问题：</font></h2><p>若项\(w_j×σ’(z_j)\)变得很大（超过 1），这时候梯度会在 BP 的时候发生指数级地增长.</p><ul><li>首先将网络的权重设置得很大，比如 \(w1 = w2 = w3 = w4 = 100\)。</li><li>然后选择偏置使得 \(σ’(z_j)\) 项不会太小：若保证每个神经元的带权输入是 \(z_j=0, σ’(z_j )=1/4\)。</li></ul><h1 id="4-5-3-梯度不稳定问题解决思路"><a href="#4-5-3-梯度不稳定问题解决思路" class="headerlink" title=" 4.5.3 梯度不稳定问题解决思路"></a><font color=blue> 4.5.3 梯度不稳定问题解决思路</font></h1><h2 id="本质原因："><a href="#本质原因：" class="headerlink" title=" 本质原因："></a><font color=DodgerBlue> 本质原因：</font></h2><ul><li>在前面的层上的梯度是来自后面的层上项的乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景。</li><li>若使用标准的基于梯度的学习算法，在网络中的不同层会出现按照不同学习速度学习的情况。sigmoid 网络中前面的层的梯度会以指数级地速度消失。</li></ul><h2 id="解决办法："><a href="#解决办法：" class="headerlink" title=" 解决办法："></a><font color=DodgerBlue> 解决办法：</font></h2><p>唯一让所有层都接近相同的学习速度的方式是所有这些项的乘积都能得到一种平衡。若没有某种机制或者更加本质的保证来达成平衡，那网络就很容易不稳定了。</p>]]></content>
      
      
      <categories>
          
          <category> 4. 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4.4 深入反向传播算法</title>
      <link href="/2017/08/01/4.4%20%E6%B7%B1%E5%85%A5%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/"/>
      <url>/2017/08/01/4.4%20%E6%B7%B1%E5%85%A5%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>反向传播算法是计算梯度的快速算法。反向传播算法是一个对代价函数\( C \) 关于任何权重\( w\) 或偏置\( b\)的偏导数的表达式，通过这个表达式，在改变权重和偏置时，表明代价函数变化的快慢。</p><a id="more"></a><h1 id="4-4-1-反向传播模型与假设"><a href="#4-4-1-反向传播模型与假设" class="headerlink" title="4.4.1 反向传播模型与假设"></a><font color=blue>4.4.1 反向传播模型与假设</font></h1><h2 id="主要思想："><a href="#主要思想：" class="headerlink" title=" 主要思想："></a><font color=DodgerBlue> 主要思想：</font></h2><ol><li>将训练集数据输入到DNN的输入层，经过隐藏层，最后达到输出层并输出结果，这是DNN的前向传播过程。</li><li>由于DNN的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层。</li><li>仅仅用一次前向传播，加上一次后向传播，就可以同时计算所有的偏导数\(\frac {∂C}{∂w_{jk}^l}\)、\(\frac {∂C}{∂b_j^l}\)。</li><li>根据代价函数的误差，调整各种参数的值，不断迭代上述过程，直至收敛。</li></ol><font color=OrangeRed>反向传播的计算代价大概是前向传播的两倍，比起直接计算导数，显然有着更大的优势。</font><h2 id="模型描述："><a href="#模型描述：" class="headerlink" title=" 模型描述："></a><font color=DodgerBlue> 模型描述：</font></h2><ul><li>参数 \(w_{jk}^l\) 表示从 \((l-1)^{th}\) 层的 \(k^{th}\) 个神经元到 \(l^{th}\) 层的 \(j^{th}\) 个神经元的连接上的权重。</li><li>\(b_j^l\)   表示在\(l^{th}\) 层第 \(j^{th}\) 个神经元的偏置。</li><li>\(z<em>j^l\) 表示 \(l^{th}\)层第 \(j \)  个神经元激活函数的带权输入，\(z^l\) 表示 \(l^{th}\) 层神经元激活函数的带权输入 $$z_j^l=∑_k w</em>{jk}^l  a_k^{l-1}+b_j^l\→z^l=w^l a^{l-1}+b^l$$</li><li>\(a<em>j^l  \) 表示 \(l^{th}\)  层第 \(j^{th}\) 个神经元的激活值，\(a^l\) 表示 \(l^{th}\) 层神经元的激活值 $$a_j^l=σ(∑_k w</em>{jk}^l  a_k^{l-1}+b_j^l )\→a^l=σ(w^l a^{l-1}+b^l )$$</li><li>\(l^{th}\) 层的激活值 \(a^l\) 与 \(l^{th}\) 层神经元的带权输入 \(z^l\) 之间关系： <script type="math/tex">a_j^l=σ(z_j^l )→a^l=σ(z^l )</script> </li><li>\(δ_j^l\) 表示 \(l^{th}\) 层的第 \(j^{th}\) 个神经元上的误差度量，\(δ_j^l\) 表示 \(l^{th}\) 层神经元的误差度量：<script type="math/tex">δ_j^l=\frac {∂C}{∂z_j^l}→δ_l=\frac {∂C}{∂z_l}</script></li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.12%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0.png?raw=true" alt="模型参数"></p><h2 id="代价函数的两个假设："><a href="#代价函数的两个假设：" class="headerlink" title=" 代价函数的两个假设："></a><font color=DodgerBlue> 代价函数的两个假设：</font></h2><ol><li>假设一：代价函数可以被写成一个在每个训练样本\(x\)上的代价函数 \(C_x\)的均值：\(C=\frac {1}{m} ∑_x C_x \)。反向传播实际上是计算每一个独立\(\frac {∂C_x}{∂w}\)和(\(\frac {∂C_x}{∂b}\)后，通过在所有训练样本上进行平均化获得\(\frac {∂C}{∂w}\)和\(\frac {∂C}{∂b}\)。</li><li>假设二：代价函数可以写成神经网络输出\(a^L\)的函数。</li></ol><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.13%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E5%81%87%E8%AE%BE.png?raw=true" alt="代价函数假设"></p><font color=OrangeRed>二次代价函数满足对于每一个独立的训练样本\\(x\\)：\\(C=\frac {1}{2} ||y-a^L ||^2=\frac {1}{2} ∑_j ||y_j-a_j^L ||^2 \\) ，将 \\(C\\) 看成仅有输出激活值\\(a^L\\)的函数，而\\( y\\)仅仅是帮助定义函数的参数而已。</font><h1 id="4-4-2-反向传播的四个基本方程"><a href="#4-4-2-反向传播的四个基本方程" class="headerlink" title="4.4.2 反向传播的四个基本方程"></a><font color=blue>4.4.2 反向传播的四个基本方程</font></h1><p>反向传播最终极的含义其实就是计算偏导数 \(\frac {∂C}{∂w<em>{jk}^l}\) 和 \(\frac {∂C}{∂b_j^l}\) ：首先引入一个中间量 \(δ_j^l\) 称为在 \(l^{th}\) 层第 \(j^{th}\) 个神经元上的误差，反向传播将给出计算误差 \(δ_j^l\) 的流程，然后将其关联到计算 \(\frac {∂C}{∂w</em>{jk}^l}\) 和 \(\frac {∂C}{∂b_j^l}\) 上。</p><h2 id="计算输出层误差方程："><a href="#计算输出层误差方程：" class="headerlink" title=" 计算输出层误差方程："></a><font color=DodgerBlue> 计算输出层误差方程：</font></h2><h3 id="方程："><a href="#方程：" class="headerlink" title="方程："></a>方程：</h3><script type="math/tex; mode=display">δ_j^L=\frac {∂C}{∂a_j^L} σ' (z_j^L )     →   δ^L=∇_a C⊙σ'(z^L)</script><h3 id="证明："><a href="#证明：" class="headerlink" title="证明："></a>证明：</h3><script type="math/tex; mode=display">δ_j^L=\frac {∂C}{∂z_j^L} ⇒^{链式法则} δ_j^L=\frac {∂C}{∂a_j^L} \frac {∂a_j^L}{∂z_j^L} \\⇒^{a_j^L=σ(z_j^L )}  \frac {∂C}{∂a_j^L} \frac {∂σ(z_j^L )}{∂z_j^L } ⇒^{求导数} \frac {∂C}{∂a_j^L}  σ'(z_j^L )</script><h3 id="说明："><a href="#说明：" class="headerlink" title="说明："></a>说明：</h3><ul><li>\(\frac {∂C}{∂a_j^L}\) 表示代价函数随着 \(j^{th}\) 输出激活值变化而变化的速度，\(σ’(z_j^L)\) 表示在 \(z_j^L \) 处激活函数 \(σ\) 变化速度。</li><li>向量 \(∇_a C\) 其元素是偏导数 \(\frac {∂C}{∂a_j^L} \)，在使用二次代价函数时： <script type="math/tex">\frac {∂C}{∂a_j^L} =(a_j-y_j)→δ^L=(a^L-y)⊙σ'(z^L)</script></li></ul><h2 id="由后一层的误反向计算前一层的误差："><a href="#由后一层的误反向计算前一层的误差：" class="headerlink" title=" 由后一层的误反向计算前一层的误差："></a><font color=DodgerBlue> 由后一层的误反向计算前一层的误差：</font></h2><h3 id="方程：-1"><a href="#方程：-1" class="headerlink" title="方程："></a>方程：</h3><script type="math/tex; mode=display">∑ w_{kj}^{l+1}δ_{k}^{l+1}σ'(z_j^L)→δ^l=((w^{l+1})^T δ^{l+1})⊙σ'(z^l)</script><h3 id="证明：-1"><a href="#证明：-1" class="headerlink" title="证明："></a>证明：</h3><script type="math/tex; mode=display">δ_j^l=\frac {∂C}{∂z_j^l} ⇒{链式法则} δ_j^l=∑_k \frac {∂C}{∂z_k^{l+1}}  \frac {∂z_k^{l+1}}{∂z_j^l} \\ ⇒^{δ_j^{l+1}=\frac {∂C}{∂z_j^{l+1}}} ∑_k \frac {∂z_j^{l+1}}{∂z_j^l} δ_k^{l+1} \\ ⇒^{链式法则} ∑_k \frac {∂z_j^{l+1}}{∂a_j^l} \frac {∂a_j^l}{∂z_j^l} δ_k^{l+1} \\ ⇒^{\frac {∂a_j^l}{∂z_j^l}=∂\frac {σ(z_j^l)}{∂z_j^l}} ∑_k \frac {∂z_j^{l+1}}{∂a_j^l} δ_k^{l+1} σ'(z_j^l) \\ ⇒^{\frac {∂z_j^{l+1}}{∂a_j^l}=\frac {∂(w_{kj}^{l+1} a_j^l+b_j^l)}{∂a_j^l }} ∑_k w_{kj}^{l+1} δ_k^{l+1} σ'(z_j^l)</script><h3 id="说明：-1"><a href="#说明：-1" class="headerlink" title="说明："></a>说明：</h3><ul><li>假设知道 \((l+1)^{th}\) 层的误差 \(δ^{l+1}\) ，应用转置的权重矩阵 \((w^{l+1}）^T\) ，可以凭直觉地把它看作是在沿着网络反向移动误差，给了度量在 \(l^{th}\) 层输出的误差方法，然后进行 Hadamard乘积运算。这会让误差通过 \(l^{th}\) 层的激活函数反向传递回来并给出在第 \(l^{th}\) 层的带权输入的误差 \(δ\)</li><li>通过组合以上两个公式，可以计算任何层的误差 \(δ^l\) 。首先使用第一个公式计算 \(δ^l\) ，然后应用 第二个公式来计算 \(δ^{l-1}\) ，然后再次计算 \(δ^{l-2}\) ，如此一步一步地反向传播完整个网络。</li></ul><h2 id="代价函数关于网络中任意偏置的改变率："><a href="#代价函数关于网络中任意偏置的改变率：" class="headerlink" title=" 代价函数关于网络中任意偏置的改变率："></a><font color=DodgerBlue> 代价函数关于网络中任意偏置的改变率：</font></h2><h3 id="方程：-2"><a href="#方程：-2" class="headerlink" title="方程："></a>方程：</h3><script type="math/tex; mode=display">\frac {∂C}{∂b_j^l} =δ_j^l→\frac {∂C}{∂b}=δ\ \ \ \ \  (δ 和偏置 b 都是针对同一个神经元)</script><h3 id="证明：-2"><a href="#证明：-2" class="headerlink" title="证明："></a>证明：</h3><script type="math/tex; mode=display">\frac {∂C}{∂b_j^l}⇒^{链式法则} \frac {∂C}{∂z_j^l} \frac {∂z_j^l}{∂b_j^l} \\⇒^{δ_j^l=\frac {∂C}{∂z_j^l}} δ_j^l  \frac {∂z_j^l}{∂b_j^l} \\⇒^{z_j^l=w_jk^l a_k^{l-1} +b_j^l} δ_j^l \frac {∂(w_{kj}^l a_k^{l-1} +b_j^l)}{∂b_j^l}=δ_j^l</script><h2 id="代价函数关于任何一个权重的改变率："><a href="#代价函数关于任何一个权重的改变率：" class="headerlink" title=" 代价函数关于任何一个权重的改变率："></a><font color=DodgerBlue> 代价函数关于任何一个权重的改变率：</font></h2><h3 id="方程：-3"><a href="#方程：-3" class="headerlink" title="方程："></a>方程：</h3><script type="math/tex; mode=display">\frac {∂C}{∂w_{jk}^l}= a_k^{l-1} δ_j^l→\frac {∂C}{∂w}=a_{in} δ_{out}</script><h3 id="证明：-3"><a href="#证明：-3" class="headerlink" title="证明："></a>证明：</h3><script type="math/tex; mode=display">\frac {∂C}{∂w_{jk}^l} ⇒^{链式法则} \frac {∂C}{∂z_j^l} \frac {∂z_j^l}{∂w_{jk}^l} ⇒^{δ_j^l=\frac {∂C}{∂z_j^l}} δ_j^l  \frac {∂z_j^l}{∂w_jk^l} \\⇒{z_j^l=w_jk^l a_k^{l-1} + b_j^l}  \frac {∂(w_{jk}^l a_k^{l-1} +b_j^l)}{∂w_{jk}^l}= a_k^{l-1} δ_j^l</script><h1 id="4-4-3-反向传播算法应用步骤"><a href="#4-4-3-反向传播算法应用步骤" class="headerlink" title="4.4.3 反向传播算法应用步骤"></a><font color=blue>4.4.3 反向传播算法应用步骤</font></h1><h2 id="随机梯度下降应用步骤："><a href="#随机梯度下降应用步骤：" class="headerlink" title=" 随机梯度下降应用步骤："></a><font color=DodgerBlue> 随机梯度下降应用步骤：</font></h2><ol><li>通过前向传播，计算并保存每层神经元的带权输入值\(z^1\)和激活值 \(a^1\) : <script type="math/tex">z^l=w^l a^{l-1}+b^l \\ a^l=σ(z^l )</script>  </li><li>计算输出层误差 \(δ^L\) ： 计算向量 <script type="math/tex">δ^L=∇_a C⊙σ'(z^L)</script></li><li>通过反向传播，计算并保存每层的误差：<script type="math/tex">δ^l=((w^{l+1})^T δ^{l+1})⊙σ'(z^l)\ \ \ \ \ l=L-1,L-2,…,2</script> </li><li>由每层的激活值 \(a^1\)和每层的误差\(δ^l\)，计算并保存每层的代价函数的梯度：<script type="math/tex">\frac {∂C}{∂w_{jk}^l}=a_k^{l-1} δ_j^l\\ \frac {∂C}{∂b_j^l} =δ^l</script></li><li>通过梯度下降算法，更新每层神经元的偏置和权重。</li></ol><h2 id="小批量数据应用步骤："><a href="#小批量数据应用步骤：" class="headerlink" title=" 小批量数据应用步骤："></a><font color=DodgerBlue> 小批量数据应用步骤：</font></h2><ol><li>通过前向传播，计算并保存每层神经元的带权输入值\(z^1\)和激活值 \(a^1\) : <script type="math/tex">z^{x,l}=w^l a^{x,l-1}b^l\\ a^l=σ(z^{x,l})</script></li><li>计算输出层误差 \(δ^{x,L}\) ： 计算向量 <script type="math/tex">δ^{x,L}=∇_a C_x⊙σ'(z^{x,l})</script></li><li>通过反向传播，计算并保存每层的误差\(δ^{x,l}\): <script type="math/tex">δ^{x,l}=((w^{l+1})^T δ^{x,l+1})⊙σ'(z^{x,l}) \ \ \ \ \ l=L-1,L-2,…,2</script></li><li>由每层的激活值 \(a^1\)和每层的误差\(δ^l\)，计算并保存每层的代价函数的梯度,通过梯度下降算法，更新每层神经元的偏置和权重：： <script type="math/tex">w^l→w^l=w_k-\frac {η}{m} ∑_x δ^{x,l} (a_k^{x,l-1})^T \\ b^l→b^l=b^l-\frac {η}{m} ∑_x δ^{x,l}</script></li><li>在实践中实现随机梯度下降，还需要一个产生训练样本的小批量数据的循环，进行多次迭代。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 4. 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4.3 深入梯度下降算法</title>
      <link href="/2017/07/30/4.3%20%E6%B7%B1%E5%85%A5%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/"/>
      <url>/2017/07/30/4.3%20%E6%B7%B1%E5%85%A5%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="4-3-1-神经网络代价函数"><a href="#4-3-1-神经网络代价函数" class="headerlink" title=" 4.3.1 神经网络代价函数"></a><font color=blue> 4.3.1 神经网络代价函数</font></h1><h2 id="代价函数-C-w-b-："><a href="#代价函数-C-w-b-：" class="headerlink" title=" 代价函数 \(C(w; b)\)： "></a><font color=DodgerBlue> 代价函数 \(C(w; b)\)： </font></h2><p>设计算法找到权重\(w\)和偏置\(b\)，使得网络的输出 \(y(x) \) 最好的拟合所有的训练输入 \(x\)的输出\(a\)，为了量化这个目标，需要定义一个代价函数\(C(w; b)\))，常见的二次代价函数为:</p><script type="math/tex; mode=display">C(w; b)=\frac {1}{2m}  ∑_x ∥y(x)- a∥ ^2</script><p>训练神经网络的目的就是要找到能最小话得代价函数\(C(w,b) ≈ 0\)的权重和偏置。</p><a id="more"></a><h2 id="求解-C-w-b-最小值的方法："><a href="#求解-C-w-b-最小值的方法：" class="headerlink" title=" 求解\(C(w; b)\) 最小值的方法： "></a><font color=DodgerBlue> 求解\(C(w; b)\) 最小值的方法： </font></h2><ol><li>微积分求解极值：若\(C\)是一个只有少数几个变量的函数，可以通过微积分寻找\(C\)的极值点。但神经网络有很多很多权重和偏置的变量，通过微积分来计算最小值是不可行的。</li><li>梯度下降的算法近似求全局最小值\(min\ C(w; b)\)：在机器学习中通常采用的求极值的方法。</li></ol><h1 id="4-3-2-神经网络梯度下降算法"><a href="#4-3-2-神经网络梯度下降算法" class="headerlink" title="4.3.2 神经网络梯度下降算法"></a><font color=blue>4.3.2 神经网络梯度下降算法</font></h1><h2 id="算法推演："><a href="#算法推演：" class="headerlink" title=" 算法推演： "></a><font color=DodgerBlue> 算法推演： </font></h2><p>假设\(C(v) \)函数只有两个变量\(v_1 \) 和\(v_2\)：</p><ol><li>当在 \(v_1 \) 和\(v_2\)方向分别移动一个很小的量，即\(\Delta v_1\)和 \(\Delta v_2\)时：<script type="math/tex">\Delta C ≈  \frac {∂C}{∂v_1} \Delta v_1+  \frac {∂C}{∂v_2} \Delta v_2</script></li><li>定义\(\Delta v\)为\(v\)变化的向量：\(\Delta v=(\Delta v_1,\Delta v_2)^T\)，用\(\Delta C \)来表示\(C\)梯度向量（偏导数的向量）：\(∇C=(∂C/∂v_1 , ∂C/∂v_2)^T\)  ，则:\(\Delta C ≈ ∇C · \Delta v\)</li><li>寻找\(\Delta v_1 \)和 \(\Delta v_2\)的一种组合\(\Delta v\)使得\(\Delta C\)为负，让\(C\)减小。假设选取\(\Delta v= -η∇C\)（\(η \) 是一个很小的正数，称为学习速率）：<script type="math/tex">\\Delta C ≈ -η∇C·∇C = -η∥∇C∥^2</script>由于\(∥∇C∥^2≥ 0\),这保证了\(\Delta C ≤ 0\)，按照此规则去改变\(v\)，那么\(C\)会一直减小，不会增加。 <script type="math/tex">v→v'=v - η \Delta C</script></li><li>然后反复持续用此规则计算下一次移动，\(C\)将持续减小，直至到获得一个全局的最小值。</li></ol><h2 id="算法特点："><a href="#算法特点：" class="headerlink" title=" 算法特点： "></a><font color=DodgerBlue> 算法特点： </font></h2><ol><li>工作方式是重复计算梯度 \(∇C\)，然后沿着梯度相反的⽅向移动，即沿着山谷“滚落”。</li><li>通过在 \(C \)下降最快的方向上微小的变化，逐步获得\(C \)的最小值。</li></ol><h2 id="算法应用："><a href="#算法应用：" class="headerlink" title=" 算法应用： "></a><font color=DodgerBlue> 算法应用： </font></h2><p>寻找能使得方程 \(C(w; b)=\frac {1}{2m}  ∑<em>x∥y(x)- a∥^2 \)的代价取得最小值的权重 \(w_k \) 和偏置 \(b_l\)，梯度向量\(∇C=(∂C/∂w_k, ∂C/∂b_l)^T\)用这些分量来写梯度下降的更新规则：$$w_k→w</em>{k}{‘}=w<em>k-η \frac {∂C}{∂w_k}\ b_l→b</em>{l}{‘}=b_l-η \frac {∂C}{∂b_l}$$</p><h2 id="随机梯度下降算法："><a href="#随机梯度下降算法：" class="headerlink" title=" 随机梯度下降算法： "></a><font color=DodgerBlue> 随机梯度下降算法： </font></h2><p>通过随机选取小量训练输入样本来计算均方误差，可以快速得到一个对于实际梯度 \(∇C\)的很好的估算，这有助于加速梯度下降，进而加速学习过程。 随机梯度下降通过随机选取小量的\(m\) 个训练输入来工作，将它们称为一个小批量数据： </p><script type="math/tex; mode=display">w_k→w_{k}{'}=w_k-\frac {η}{m} ∑_j \frac {∂C_{x_j}}{∂w_k} \\ b_l→b_{l}{'}=b_l-\frac {η}{m} ∑_j\frac {∂C_{x_j}}{∂b_l}</script>]]></content>
      
      
      <categories>
          
          <category> 4. 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4.2 深入神经元激活函数</title>
      <link href="/2017/07/29/4.2%20%E6%B7%B1%E5%85%A5%E7%A5%9E%E7%BB%8F%E5%85%83%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
      <url>/2017/07/29/4.2%20%E6%B7%B1%E5%85%A5%E7%A5%9E%E7%BB%8F%E5%85%83%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>单层感知器一般采用的是符号激活函数（阶跃函数）作为神经元激励。随着深度学习发展，后续出现了S型神经元、Tanh型神经元、ReLU型神经元等。</p><a id="more"></a><h1 id="4-2-1-阶跃激活函数型神经元"><a href="#4-2-1-阶跃激活函数型神经元" class="headerlink" title=" 4.2.1 阶跃激活函数型神经元 "></a><font color=blue> 4.2.1 阶跃激活函数型神经元 </font></h1><h2 id="模型描述："><a href="#模型描述：" class="headerlink" title=" 模型描述： "></a><font color=DodgerBlue> 模型描述： </font></h2><ul><li>输入：多个输入\(x_1,x_2…,x_m\)，输入值为任意实数。</li><li>权重：\(w_1,w_2,…,w_m\)每个输入有与之对应的权重，表示相应输入对于输出重要性的实数。</li><li>阈值：也称为偏置，是神经元的一个参数。可以把偏置看作一种表示让神经元输出1有多容易的估算，若偏置很大输出 1 则容易的。若偏置是一个非常小的负数，输出 1 则很困难。<br>\(w_0×1=b\)</li><li>输出：0者1。由分配权重后的总和\(∑_j {w_j x_j}\)，分配权重后的总和小于或大于一些阈值决定。<script type="math/tex; mode=display">output =\begin {cases}  0\ \ \ \ \ \ \ if \ \ ∑_j {w_j x_j} ≤ threshold \\  1\ \ \ \ \ \ \ if \ \ ∑_j {w_j x_j} > threshold\end {cases}</script>若\(wx=∑_j {w_j x_j},b=-threshold\),则：<script type="math/tex; mode=display">output =\begin {cases}  0\ \ \ \ \ \ \ if  \ \ wx+b ≤ 0 \\  1\ \ \ \ \ \ \ if  \ \ wx+b > 0 \end {cases}</script></li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.5%E9%98%B6%E8%B7%83%E5%87%BD%E6%95%B0.png?raw=true" alt="阶跃激活函数"><br><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.4%E9%98%B6%E8%B7%83%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%A5%9E%E7%BB%8F%E5%85%83.png?raw=true" alt="阶跃激活函数型神经元"></p><h2 id="工作原理："><a href="#工作原理：" class="headerlink" title=" 工作原理： "></a><font color=DodgerBlue> 工作原理： </font></h2><p>神经网络根据外部的刺激和结果，自动调整神经元的权重和偏置，对权重或偏置的微小改动能够引起输出的微小变化，反复改动权重和偏置以产生更好的输出，让⽹络能够表现得像想要的那样。这时神经⽹络就在学习，借此能学会解决某些问题。</p><h2 id="缺陷："><a href="#缺陷：" class="headerlink" title=" 缺陷： "></a><font color=DodgerBlue> 缺陷： </font></h2><ul><li>可以实现逻辑功能，由于与非门是通用运算，多个与非门可以构建出任何运算，故可实现任何逻辑功能，但对于计算稍微复杂的函数其计算力显得无能为力。</li><li>在网络中一个权重或偏置的微小改动，有时会引起神经元的输出完全翻转，如 0 变到 1。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.7%E6%9D%83%E9%87%8D%E6%94%B9%E5%8F%98%E5%BD%B1%E5%93%8D%E8%BE%93%E5%87%BA.png?raw=true" alt="权重微小改变"></p><h1 id="4-2-2-S型神经元"><a href="#4-2-2-S型神经元" class="headerlink" title=" 4.2.2 S型神经元"></a><font color=blue> 4.2.2 S型神经元</font></h1><h2 id="模型描述：-1"><a href="#模型描述：-1" class="headerlink" title=" 模型描述： "></a><font color=DodgerBlue> 模型描述： </font></h2><ul><li>输入：多个输入\(x_1,x_2…,x_m\)，输入值为 0 何 1 之间的任意值，不仅仅是 0 或 1。</li><li>权重：\(w_1,w_2,…,w_m\) 为每个输入与之对应的权重，表示相应输入对于输出重要性的实数。</li><li>阈值：偏置\(b\)。</li><li>输出值：\(σ(wx + b)\)是介于0和1之间的任意值，\(σ\)为 S 型函数：<ul><li>当\(z = w x + b \) 是一个很大的正数，\(σ(z) ≈ 1\)，S 型神经元近似一个符号型神经元。</li><li>当\(z = w x + b \)是一个很大的负数，\(σ(z) ≈ 0\)，S 型神经元近似一个符号型神经元。</li><li>在\(z = w x + b \)取中间值时，S 型神经元的行为和符号型神经元有比较大的偏离。<script type="math/tex; mode=display">σ(z)=\frac {1}{1+e^{-z}}  → output=\frac {1}{1+exp(-∑_j {w_j x_j}-b) }</script></li></ul></li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.9S%E5%87%BD%E6%95%B0.png?raw=true" alt="S函数"><br><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.8S%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E7%A5%9E%E7%BB%8F%E5%85%83.png?raw=true" alt="S型神经元"></p><h2 id="符号型神经元和-S-型神经元比较："><a href="#符号型神经元和-S-型神经元比较：" class="headerlink" title=" 符号型神经元和 S 型神经元比较： "></a><font color=DodgerBlue> 符号型神经元和 S 型神经元比较： </font></h2><ul><li>符号型神经元输出只能是0或1，而S 型神经元可以输出 0 和 1 之间的任何实数。</li><li>符号型神经元对于一个权重或偏置的微小改动可能会引起输出的完全翻转，这样的翻转可能会引起网络行为的完全改变；S型神经元是阶跃型的平滑版，\(σ\) 的平滑意味着权重和偏置的微小变化\(\Delta w_j\)和\(\Delta b\)，会从神经元产生一个更加微小的输出变化\(\Delta output\)</li></ul><h2 id="缺陷：-1"><a href="#缺陷：-1" class="headerlink" title=" 缺陷： "></a><font color=DodgerBlue> 缺陷： </font></h2><p>σ′ 降低了梯度，减缓了学习。</p><h1 id="4-2-3-Tanh激励函数型神经元"><a href="#4-2-3-Tanh激励函数型神经元" class="headerlink" title=" 4.2.3 Tanh激励函数型神经元 "></a><font color=blue> 4.2.3 Tanh激励函数型神经元 </font></h1><h2 id="模型描述：-2"><a href="#模型描述：-2" class="headerlink" title=" 模型描述： "></a><font color=DodgerBlue> 模型描述： </font></h2><ul><li>输入: \(x\)，权重向量为\(w\)，偏置为\(b\) </li><li>输出：<script type="math/tex; mode=display">z=w∙x+b\\tanh(z)=\frac {e^z-e^{-z}}{e^z+e^{-z}}\\σ(z)=\frac {1 + tanh \frac {z}{2}}{2}</script></li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.10Tanh%E5%87%BD%E6%95%B0.png?raw=true" alt="tanh函数"></p><h2 id="Tanh型神经元与S型神经元比较："><a href="#Tanh型神经元与S型神经元比较：" class="headerlink" title=" Tanh型神经元与S型神经元比较： "></a><font color=DodgerBlue> Tanh型神经元与S型神经元比较： </font></h2><p>Tanh 型神经元的输出的值域是 (-1,1) 而非S型神经元的(0,1)，若构建Tanh 型神经元，需要正规化最终的输出。</p><h2 id="缺陷：-2"><a href="#缺陷：-2" class="headerlink" title=" 缺陷： "></a><font color=DodgerBlue> 缺陷： </font></h2><p>\(σ′\) 降低了梯度，减缓了学习。</p><h1 id="4-2-4-ReLU修正线性激励函数型神经元"><a href="#4-2-4-ReLU修正线性激励函数型神经元" class="headerlink" title=" 4.2.4 ReLU修正线性激励函数型神经元 "></a><font color=blue> 4.2.4 ReLU修正线性激励函数型神经元 </font></h1><h2 id="模型描述：-3"><a href="#模型描述：-3" class="headerlink" title=" 模型描述： "></a><font color=DodgerBlue> 模型描述： </font></h2><p>输入为\(x\)，权重向量为\(w\)，偏置为\(b\)的 ReLU 神经元的输出为：</p><script type="math/tex; mode=display">max(0,w∙x + b)</script><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.11%E4%BF%AE%E6%AD%A3%E7%BA%BF%E6%80%A7%E5%87%BD%E6%95%B0.png?raw=true" alt="修正线性函数"></p><h2 id="特点："><a href="#特点：" class="headerlink" title=" 特点： "></a><font color=DodgerBlue> 特点： </font></h2><ul><li>ReLU 能够用来计算任何函数，可以使用反向传播算法和随机梯度下降进行训练。</li><li>提高ReLU 的带权输入并不会导致其饱和，所以就不存在学习速度下降。</li><li>另外当带权输入是负数的时候，梯度就消失了，神经元就完全停止学习。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 4. 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>4.1 深度学习概述</title>
      <link href="/2017/07/23/4.1%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/"/>
      <url>/2017/07/23/4.1%20%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<p>深度学习是早期人工神经网络算法的衍生，是深度神经网络的统称。&lt;/br&gt;<br><strong>Deep Neural Network（DNN）—深度神经网络 &lt;/br&gt;</strong><br><strong>Convolutional Neural Network（CNN）—卷积神经网络 &lt;/br&gt;</strong><br><strong>Recurrent Neural Network（RNN）—循环神经网络  &lt;/br&gt;</strong><br><strong>Long Short Term Memory （LSTM）—长短记忆时间循环神经网络 &lt;/br&gt;</strong></p><a id="more"></a><h1 id="神经网络的发展阶段："><a href="#神经网络的发展阶段：" class="headerlink" title=" 神经网络的发展阶段："></a><font color=blue> 神经网络的发展阶段：</font></h1><p>按照隐藏层的层数可分为：感知器（无隐藏层）、多层感知器（包含一隐藏层）、深度神经网络（包含两个及两个以上的隐藏层）。深度神经网络又分化为卷积神经网络、循环神经网络。</p><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.1%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%88%86%E7%B1%BB.png?raw=true" alt="深度神经网络分类"></p><h1 id="深度神经网络一般模型及特点："><a href="#深度神经网络一般模型及特点：" class="headerlink" title=" 深度神经网络一般模型及特点："></a><font color=blue> 深度神经网络一般模型及特点：</font></h1><p><strong>描述：</strong>包含两个及两个以上的隐藏层的神经网络称为深度神经网络，其中“深度”没有固定定义。&lt;/br&gt;<br><strong>局限性：</strong>DNN在理论上可以拟合任一函数，但对于非连续的且有突然极陡跳跃的函数，一般来说无法使用DNN进行近似。&lt;/br&gt;<br><strong>优势：</strong>DNN表达力更强，更节约资源。虽然仅有一个隐藏层的神经网络就能拟合任一函数，但需要很多的神经元。而深层网络用少得多的神经元就能拟合同样的函数，使用一个深而窄的网络，往往更节约资源。&lt;/br&gt;<br><strong>劣势：</strong></p><ul><li>全连接深度神经网络（下层神经元和所有上层神经元都形成连接）参数数量膨胀问题。</li><li>随着神经网络层数的加深，“梯度消失”现象更加严重，利用有限数据训练的深层网络，性能还不如较浅层网络。</li></ul><font color=OrangeRed>A. S型神经元对于幅度为1的信号，在BP反向传播梯度时，每传递一层，梯度衰减为原来的0.25。层数一多，梯度指数衰减后低层基本上接受不到有效的训练信号。</br>B. 为了克服梯度消失，ReLU、maxout等激励函数函数代替了 Sigmoid。</br>C. 避免了梯度弥散问题，出现了高速公路网络和深度残差学习，残差学习网络层数已达到152层。</font><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.2%E6%B7%B1%E5%BA%A6%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.png?raw=true" alt="深度神经网络模型"></p><h1 id="深度学习直观理解：以人脸识别为例"><a href="#深度学习直观理解：以人脸识别为例" class="headerlink" title=" 深度学习直观理解：以人脸识别为例 "></a><font color=blue> 深度学习直观理解：以人脸识别为例 </font></h1><ol><li>人脸识别可以分解为若干问题，如：左上角有眼睛吗？中间有一个鼻子吗？</li><li>每个问题可以继续被分解为若干子问题，最终可以分解到回答那些只包含若干个像素点的简单问题。</li><li>神经网络分层模拟这些问题，通过多个网络层传递得越来越远，最终那些只包含若干个像素点的问题可以被那些与图像中原始像素点相连的单个神经元所回答。</li></ol><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/4.3%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB.png?raw=true" alt="人脸识别"></p>]]></content>
      
      
      <categories>
          
          <category> 4. 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.4 推荐系统</title>
      <link href="/2017/07/16/3.4%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
      <url>/2017/07/16/3.4%20%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
      
        <content type="html"><![CDATA[<p>推荐系统是机器学习中的一个重要的应用。对机器学习来说特征选择是很重要的，所选择的特征将对学习算法的性能有很大的影响，推荐系统可以自动学习一套好的特征。具体应用：</p><ol><li>降低信息过载，提高站点的点击率/转化率。</li><li>加深对用户的了解，为用户提供定制化服务。</li><li>帮助用户找到想要的商品，可以让小众的用户找到自己感兴趣的内容（长尾理论）。</li></ol><p>推荐系统采用的算法有：基于内容的算法、协同过滤算法、基于模型的算法（用机器学习来预测）、混合算法（现实应用中一般通过给不同算法的结果加权重来综合结果）。</p><a id="more"></a><h1 id="3-4-1-基于内容的推荐系统"><a href="#3-4-1-基于内容的推荐系统" class="headerlink" title="3.4.1 基于内容的推荐系统"></a><font color=blue>3.4.1 基于内容的推荐系统</font></h1><h2 id="基于内容的推荐系统的概念："><a href="#基于内容的推荐系统的概念：" class="headerlink" title=" 基于内容的推荐系统的概念："></a><font color=DodgerBlue> 基于内容的推荐系统的概念：</font></h2><p><strong>基于内容的推荐系统：</strong>根据用户对物品的评价历史，以及物品之间的相似度进行推荐。</p><p><strong>算法主要步骤：</strong></p><ol><li>物品表征建模：从物品中抽取出一些特征来表示该物品。</li><li>用户喜好特征建模：从用户对某些物品的历史评价中，学习用户的偏好模型。</li><li>推荐物品：通过匹配某一用户的偏好及候选物的特征，为此用户推荐一组相关性最大的物品。</li></ol><h2 id="基于内容的电影推荐系统："><a href="#基于内容的电影推荐系统：" class="headerlink" title=" 基于内容的电影推荐系统："></a><font color=DodgerBlue> 基于内容的电影推荐系统：</font></h2><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/3.4.1%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90.png?raw=true" alt="电影推荐"></p><p><strong>模型：</strong>用户数量\(n_u\)、电影数量\(n_m\)、若用户\(j \)给电影i打过分则\(r(i,j)=1\)、用户\(j \)给电影\(i\)的评分\(y^{(i , j)}\)、用户\(j \)评过分的电影总数\(m^{(j)}\)<br>用户\(j\)的参数向量:<script type="math/tex">θ^{(j)} ∈ R^{n+1}</script>电影\(i\)的特征向量:<script type="math/tex">x^{(i)} ∈ R^{n+1}  ;x_0^{(i)}=1\\</script>用户\(j \)对\(i\)电影评分预测为<script type="math/tex">(θ^{(j)})^T x^{(i)}</script>基于内容的推荐系统的算法如下：</p><p><strong>代价函数：</strong>&lt;/br&gt;<br>用户\(j\)代价函数：<script type="math/tex">min_{θ^{(j)}}\frac {1}{2m^{(j)}} ∑_{i:r(i,j)=1} ((θ^{(j)} )^T x^{(i) }-y^{(i,j)})^2 +\frac {λ}{2m^{(j)}}∑_{k=1}^n (θ_k^{(j)})^2</script><br>所有用户的代价函数：<script type="math/tex">min_{θ^{(1)} ,…, θ^{(n_u)}} \frac {1}{2} ∑_{j=1}^{n_u}∑_{i:r(i,j)=1} ((θ^{(j)})^T x^{(i)}-y^{(i,j)})^2+\frac {λ}{2} ∑_{j=1}^{n_u} ∑_{k=1}^n (θ_k^{(j)})^2</script></p><p><strong>梯度下降算法：</strong>&lt;/br&gt;</p><script type="math/tex; mode=display">θ_k^{(j)}≔θ_k^{(j)}-α∑_{i:r(i,j)=1)} ((θ^{(j)})^T x^{(i)}-y^{(i,j)}) x_k^{(i)}\ \ \ \ \ for k=0</script><script type="math/tex; mode=display">θ_k^{(j)}≔θ_k^{(j)}-α(∑_{i:r(i,j)=1)} ((θ^{(j)})^T x^{(i)}-y^{(i,j)}) x_k^{(i)}+λθ_k^{(j)})\ \ \ \ \ for k=0</script><h1 id="3-4-3-协同过滤算法"><a href="#3-4-3-协同过滤算法" class="headerlink" title="3.4.3 协同过滤算法"></a><font color=blue>3.4.3 协同过滤算法</font></h1><h2 id="协同过滤的概念："><a href="#协同过滤的概念：" class="headerlink" title=" 协同过滤的概念："></a><font color=DodgerBlue> 协同过滤的概念：</font></h2><p><strong>基于用户的协同算法：</strong>分析各个用户对物品的评价（通过浏览记录、购买记录等）-&gt;依据用户对物品的评价计算得出所有用户之间的相似度-&gt;选出与当前用户最相似的N个用户-&gt;将这N个用户评价最高并且当前用户又没有浏览过的物品推荐给当前用户。&lt;/br&gt;<br><strong>基于物品的协同算法：</strong>分析各个用户对物品的浏览记录-&gt;依据浏览记录分析得出所有物品之间的相似度-&gt;对于当前用户评价高的物品，找出相似度最高的N个物品-&gt;将这N个物品推荐给用户。</p><h2 id="基于协同过滤的电影推荐系统："><a href="#基于协同过滤的电影推荐系统：" class="headerlink" title=" 基于协同过滤的电影推荐系统："></a><font color=DodgerBlue> 基于协同过滤的电影推荐系统：</font></h2><p><strong>自学习电影和用户的特征：</strong>&lt;/br&gt;</p><ul><li>若掌握了每一部电影的可用特征，使用这些特征可训练出每一个用户的参数：\(θ^{(1)} ,…, θ^{(n_u)}\)<script type="math/tex; mode=display">min_{θ^{(1)} ,…, θ^{(n_u)}} \frac {1}{2} ∑_{j=1}^{n_u} ∑_{i:r(i,j)=1} ((θ^{(j)})^T x^{(i)}-y^{(i,j)})^2+ \frac {λ}{2}∑_{j=1}^{n_u} ∑_{k=1}^n (θ_k^{(j)})^2</script></li><li>若拥有用户的参数，就可以学习得出所有电影的特征：\(x^{(1)} ,…, x^{(n_m)}\)<script type="math/tex; mode=display">min_{x^{(1)},…,x^{(n_m)}} \frac {1}{2}∑_{i=1}^{n_m} ∑_{j:r(i,j)=1} ((θ^{(j) })^T x^{(i)}-y^{(i,j)})^2 +  \frac {λ}{2} ∑_{i=1}^{n_m} ∑_{k=1}^n (x_k^{(j)})^2</script></li><li>协同过滤在既没有用户的参数，也没有电影的特征，可以同时学习用户参数以及电影特征：<br>\(θ→x→θ→⋯\)</li><li>代价函数：<script type="math/tex; mode=display">J(x^{(1)},…,x^{(n_m)},θ^{(1)},…,θ^{(n_u)} )\\= \frac {1}{2} ∑_{(i, j):r(i,j)=1} ((θ^{(j) })^T x^{(i)}-y^{(i,j)})^2 +\\  \frac {λ}{2} ∑_{i=1}^{n_m} ∑_{k=1}+ \frac {λ}{2}∑_{j=1}^{n_u} ∑_{k=1}^n (θ_k^{(j)})^2</script></li><li>梯度下降：<script type="math/tex; mode=display">min_{x^{(1)},…,x^{(n_m)},θ^{(1)},…,θ^{(n_u)}}J(x^{(1)},…,x^{(n_m)},θ^{(1)},…,θ^{(n_u)} )</script><script type="math/tex; mode=display">x_k^{(i)}≔x_k^{(i)}-α(∑_{i:r(i,j)=1)}((θ^{(j) })^T x^{(i)}-y^{(i,j) }) x_k^{(i)}+λx_k^{(i)}) \ \ \ \ \ xϵR^n</script><script type="math/tex; mode=display">θ_k^{(j)}≔x_k^{(i)}-α(∑_{i:r(i,j)=1)}((θ^{(j) })^T x^{(i)}-y^{(i,j) }) x_k^{(i)}+λθ_k^{(j)}) \ \ \ \ \ θϵR^n</script></li></ul><h2 id="推荐系统应用注意点："><a href="#推荐系统应用注意点：" class="headerlink" title=" 推荐系统应用注意点："></a><font color=DodgerBlue> 推荐系统应用注意点：</font></h2><p><strong>如何推荐：</strong></p><script type="math/tex; mode=display">small ||x^{(i)}-x^{(j)}||</script><p><strong>均值归一化：</strong></p><ul><li>计算每部电影的评分平均值\(μ_i\)。</li><li>将每一个用户对某一部电影的评分减去对该电影的评分平均值。</li><li>用新的数据集训练算法。</li><li>预测出来的评分，需将平均值重新加回去：<script type="math/tex">(θ^{(j)})^T x^{(i)}+μ_i</script></li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/3.4.2%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90%E5%9D%87%E5%80%BC%E5%BD%92%E4%B8%80%E5%8C%96.png?raw=true" alt="电影推荐均值归一化"></p>]]></content>
      
      
      <categories>
          
          <category> 3. 非监督学习算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.3 异常检测</title>
      <link href="/2017/07/15/3.3%20%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/"/>
      <url>/2017/07/15/3.3%20%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B/</url>
      
        <content type="html"><![CDATA[<p>异常检测主要用于非监督学习问题，但它又类似于一些监督学习问题，但又无法根据结果变量y 的值来确定数据是否真的是异常的。</p><a id="more"></a><h1 id="3-3-1-异常检测与高斯分布"><a href="#3-3-1-异常检测与高斯分布" class="headerlink" title="3.3.1 异常检测与高斯分布"></a><font color=blue>3.3.1 异常检测与高斯分布</font></h1><h2 id="异常检测基本概念："><a href="#异常检测基本概念：" class="headerlink" title=" 异常检测基本概念："></a><font color=DodgerBlue> 异常检测基本概念：</font></h2><p><strong>描述：</strong>给定假设数据都为正常的数据集\(\{x^{(1)}, x^{(2)} ,…, x^{(m)}\} x∈R^n\)，判断测试数据\(x<em>{test}\)属于正常数据的可能性\(p(x)\)，即判断\(x</em>{test}\)是不是异常的：</p><script type="math/tex; mode=display">\begin {cases}p(x)<ε\ \ \ \ anormal\\p(x)≥ε\ \ \ \ normal\end {cases}</script><p><strong>异常检测与监督学习：</strong>对于正样本的数量很少甚至为零的问题，通常使用的是异常检测算法。</p><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/3.3.1%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E4%B8%8E%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.png?raw=true" alt="异常检测与监督学习"></p><h2 id="高斯分布（正态分布）："><a href="#高斯分布（正态分布）：" class="headerlink" title=" 高斯分布（正态分布）："></a><font color=DodgerBlue> 高斯分布（正态分布）：</font></h2><p><strong>基本概念：</strong>变量\(x\)符合高斯分布\(x~Ν(μ,σ^2)\)则其概率密度函数为:<script type="math/tex">p(x;μ,σ^2 )=\frac 1 {\sqrt{2π} σ} exp(- \frac {(x-μ)^2}{2σ^2})</script></p><ul><li>\(μ\)为变量\(x\)的均值：<script type="math/tex">μ= \frac {1}{m} ∑_{i=1}^m x^{(i)}</script></li><li>\(σ\)为变量\(x\)的方差：<script type="math/tex">σ^2=\frac {1}{m} ∑_{i=1}^m (x^{(i)}-μ)^2</script></li><li>统计学中对于方差通常只除以\(m-1\)。在实际使用中，到底是选择使用\(m\)还是\(m-1\)其实区别很小，只要有一个还算大的训练集，在机器学习领域更习惯使用\(m\) 。</li></ul><p><strong>高斯分布异常检测算法：</strong></p><ul><li>给定数据训练集：\(\{x^{(1)}, x^{(2)} ,…, x^{(m)}\} x∈R^n\)</li><li>计算每个特征的均值和方差：<script type="math/tex">μ_j=\frac {1}{m} ∑_{i=1}^m x_j^{(i)}\\ σ_j^2=\frac {1}{m} ∑_{i=1}^m (x_j^{(i)}-μ_j)^2</script></li><li>给定新的实例x，根据独立估计，计算:<script type="math/tex">p(x)=∏_{j=1}^n p(x;μ_j,σ_j^2 ) =∏_{j=1}^n \frac {1}{\sqrt {2π} σ_j} exp(- \frac {(x-μ_j)^2}{2σ_j^2})</script><br>\(当p(x)&lt;ℇ时，实例x为异常\)</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/3.3.2%E5%BC%82%E5%B8%B8%E6%A3%80%E6%B5%8B%E7%A4%BA%E4%BE%8B.png?raw=true" alt="异常检测示例"></p><h1 id="3-3-2-异常检测系统评价"><a href="#3-3-2-异常检测系统评价" class="headerlink" title="3.3.2 异常检测系统评价"></a><font color=blue>3.3.2 异常检测系统评价</font></h1><p>当在开发一个异常检测系统时，从带标记（异常或正常）的数据着手，从其中选择一部分正常数据用于构建训练集，然后用剩下的正常数据和异常数据混合的数据构成交叉检验集和测试集。</p><h2 id="评价方法："><a href="#评价方法：" class="headerlink" title=" 评价方法："></a><font color=DodgerBlue> 评价方法：</font></h2><ol><li>将正常的数据随机分成训练集（60%）、交叉验证集（20%）和测试集（20%），将异常的数据随机分成交叉验证集（50%）和测试集（50%）</li><li>根据训练集，估计特征的平均值和方差并构建\(p(x)\)函数。</li><li>对交叉检验集，尝试使用不同的 \(ε \)值作为阀值，并预测数据是否异常，根据F\(F_1  Score\)查准率与查全率的比例来选择ε。</li><li>选出 \(ε \)后，针对测试集进行预测，计算异常检验系统的\(F_1  Score\)或查准率与查全率之比。</li></ol><h2 id="注意点："><a href="#注意点：" class="headerlink" title=" 注意点："></a><font color=DodgerBlue> 注意点：</font></h2><p><strong>特征处理：</strong>对于异常检测算法，所选择的特征要符合高斯分布，如果某个特征的分布不是高斯分布，异常检测算法也能够工作，但是最好还是将数据转换成高斯分布，如使用对数函数：\(x = log(x+c) ,其中c 为非负常数\)其中c 为非负常数；或者\(x=x^c , c 为0-1 之间的一个分数\) 等方法。</p><p><strong>误差分析：</strong>一个常见的问题是一些异常的数据可能也会有较高的 \(p(x)\)值，因而被算法认为是正常的。这种情况下通过误差分析那些被算法错误预测为正常的数据，观察能否找出一些问题。如：需要增加一些新的特征，增加这些新特征后获得的新算法能够帮助更好地进行异常检测。</p><h1 id="3-3-3-多元高斯分布与异常检测"><a href="#3-3-3-多元高斯分布与异常检测" class="headerlink" title="3.3.3 多元高斯分布与异常检测"></a><font color=blue>3.3.3 多元高斯分布与异常检测</font></h1><h2 id="多元高斯分布："><a href="#多元高斯分布：" class="headerlink" title=" 多元高斯分布："></a><font color=DodgerBlue> 多元高斯分布：</font></h2><p>变量\(x\)符合多元高斯分布，则其概率密度函数为:<script type="math/tex">p(x)=\frac {1}{(2π)^{n/2} |Σ|^{1/2}} exp(-\frac {1}{2} (x-μ)^T Σ^{-1} (x-μ))</script></p><p>\(μ\)为变量\(x\)的均值：<script type="math/tex">μ=\frac {1}{m} ∑_{i=1}^m x^{(i)}</script></p><p>\(Σ\)为变量\(x\)的协方差矩阵：<script type="math/tex">Σ=\frac {1}{m} ∑_{i=1}^n (x^{(i)}-μ)(x^{(i)}-μ)^T\ \ \  Σ∈R^{n×n}</script></p><h2 id="多元高斯分布异常检测算法："><a href="#多元高斯分布异常检测算法：" class="headerlink" title=" 多元高斯分布异常检测算法："></a><font color=DodgerBlue> 多元高斯分布异常检测算法：</font></h2><ol><li>给定数据集：\(\{x^{(1)}, x^{(2)} ,…, x^{(m)}\} x∈R^n\) </li><li>计算所有特征的平均值及协方差矩阵Σ：<script type="math/tex; mode=display">μ=\frac {1}{m} ∑_{i=1}^m x^{(i)} \\Σ=\frac {1}{m} ∑_{i=1}^n (x^{(i)}-μ)(x^{(i)}-μ)^T\ \ \  Σ∈R^{n×n}</script></li><li>给定新的实例x计算：<script type="math/tex">p(x)=\frac {1}{(2π)^{n/2} |Σ|^{1/2}} exp(-\frac {1}{2} (x-μ)^T Σ^{-1} (x-μ))</script>当\(p(x)&lt;ℇ\)时，实例\(x\)为异常.</li></ol><h2 id="多元高斯分布模型与原高斯分布模型的关系："><a href="#多元高斯分布模型与原高斯分布模型的关系：" class="headerlink" title=" 多元高斯分布模型与原高斯分布模型的关系："></a><font color=DodgerBlue> 多元高斯分布模型与原高斯分布模型的关系：</font></h2><p>原高斯分布模型被广泛使用着，若特征之间在某种程度上存在相互关联的情况，可以通过构造新新特征的方法来捕捉这些相关性；若训练集不是太大，并且没有太多的特征，可以使用多元高斯分布模型。</p><p>当\(Σ\)为以下对角矩阵时，多元高斯分布，退化成原高斯分布。</p><script type="math/tex; mode=display">Σ=\left [\begin {matrix}δ_1^2&0&0\\…&…&…\\0&0&δ_n^2\end {matrix}\right ]</script><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/3.3.3%E6%BA%90%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E6%A8%A1%E5%9E%8B%E4%B8%8E%E5%A4%9A%E5%85%83%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E6%A8%A1%E5%9E%8B.png?raw=true" alt="原高斯分布模型与多元高斯分布模型"></p>]]></content>
      
      
      <categories>
          
          <category> 3. 非监督学习算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.2 降维算法类</title>
      <link href="/2017/07/07/3.2%20%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E7%B1%BB/"/>
      <url>/2017/07/07/3.2%20%E9%99%8D%E7%BB%B4%E7%AE%97%E6%B3%95%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<p>降维指的是降低数据的维度，降维的用途很广泛：</p><ol><li>用来压缩数据，使用较少的计算机内存或磁盘空间，同时也可以加快学习算法。</li><li>除去数据的冗余（如：特征中分别有用厘米表示和用英寸表示，两者有重复）。</li><li>数据可视化，将三维以上的高维数据降维至二维或三维。</li></ol><a id="more"></a><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/3.2.1%E4%BA%8C%E7%BB%B4%E9%99%8D%E7%BB%B4.png?raw=true" alt="二维降维"><br><img src= "/img/loading.gif" data-lazy-src="/img/ai/3.2.2%E4%B8%89%E7%BB%B4%E9%99%8D%E7%BB%B4.png?raw=true" alt="三维降维"></p><h1 id="3-2-1-主成分分析PCA"><a href="#3-2-1-主成分分析PCA" class="headerlink" title="3.2.1 主成分分析PCA "></a><font color=blue>3.2.1 主成分分析PCA </font></h1><p>主成分分析（PCA）是最常见的降维算法。PCA特点：</p><ol><li>对数据进行降维处理后，保证数据的特性损失最小。</li><li>对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去。</li><li>不需要人为设定参数或根据任何经验对计算进行干预，最后的结果只与数据本身相关。</li></ol><h2 id="算法基本思想："><a href="#算法基本思想：" class="headerlink" title=" 算法基本思想： "></a><font color=DodgerBlue> 算法基本思想： </font></h2><p>找到一个方向向量，当把所有的数据都投射到该向量上时，希望投射平均均方误差能尽可能地小。<br>方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。<br>将\(n\)维降至\(k\)维，目标是找到向量\(μ_1,μ_2,…,μ_K\)使得总的投射误差最小。</p><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/3.2.3%E9%99%8D%E7%BB%B4%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3.png?raw=true" alt="降维基本思想"></p><h2 id="算法步骤："><a href="#算法步骤：" class="headerlink" title=" 算法步骤： "></a><font color=DodgerBlue> 算法步骤： </font></h2><p>假设有数据集:\(\{x^{(1)} , x^{(2)} ,…, x^{(m)}\}\)</p><ol><li>均值归一化：<script type="math/tex">μ_j=1/m ∑_{i=1}^m x_j^{(i)} \\ x_j^{(i)}=  (x_j-μ_j)/δ^2</script></li><li>计算协方差矩阵Σ：<script type="math/tex">Sigma=1/m ∑_{i=1}^n(x^{(i)})(x^{(i)})^T</script></li><li>计算协方差矩阵Σ 的特征向量：\([U,S,V]=svd(Sigma)\)<script type="math/tex; mode=display">U= \left[\begin{matrix} u_{n×1}^{(1)} & u_{n×1}^{(2)}&u_{n×1}^{(k)}&...&u_{n×1}^{(n)}\end{matrix}\right]_{n×n}</script></li><li>获取\(U_{reduce}\)，从\(U\)中选取前\(K \)个向量：<script type="math/tex; mode=display">U_{reduce}= \left[\begin{matrix} u_{n×1}^{(1)} & u_{n×1}^{(2)}&u_{n×1}^{(k)}\end{matrix}\right]_{n×k}</script></li><li>获取新的K维新特征向量\(Z^{(i)}\)：<script type="math/tex; mode=display">Z^{(i)}=U_{reduce}^T x^{(i)}</script></li></ol><h2 id="重建的压缩表示："><a href="#重建的压缩表示：" class="headerlink" title=" 重建的压缩表示： "></a><font color=DodgerBlue> 重建的压缩表示： </font></h2><script type="math/tex; mode=display">x^{(i)}≈x_{approx}^{(i)}=U_{reduce} Z^{(i)}</script><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/3.2.4%E9%87%8D%E5%BB%BA%E5%8E%8B%E7%BC%A9.png?raw=true" alt="重建"></p><h2 id="如何选择主成分的数量K："><a href="#如何选择主成分的数量K：" class="headerlink" title=" 如何选择主成分的数量K： "></a><font color=DodgerBlue> 如何选择主成分的数量K： </font></h2><p>通过\(S<em>{n×n}\)对角阵计算平均均方差与训练集方差的比例<br>训练集方差：$$1/m ∑</em>{i=1}^n ||x^{(i)}||^2 <script type="math/tex">平均均方差：</script>1/m ∑<em>{i=1}^n ||x^{(i)}-x</em>{approx}^{(i)} ||^2 <script type="math/tex">降维误差：</script>(1/m ∑<em>{i=1}^n||x^{(i)}-x</em>{approx}^{(i)} ||^2 )/(1/m ∑<em>{i=1}^n ||x^{(i)} ||^2 )\ =1-∑</em>{i=1}^k S<em>{ii}/∑</em>{i=1}^m S<em>{ii}≤ 1\% &lt;=&gt; ∑</em>{i=1}^k S<em>{ii}/∑</em>{i=1}^m S_{ii}≥99\%$$意味着原本数据的偏差有99%都保留下来。</p><font color=OrangeRed>PCA只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征，只在有必要的时候才考虑采用主要成分分析。 </font>]]></content>
      
      
      <categories>
          
          <category> 3. 非监督学习算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>3.1 聚类算法类</title>
      <link href="/2017/07/07/3.1%20%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E7%B1%BB/"/>
      <url>/2017/07/07/3.1%20%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<p>非监督学习是要让计算机学习无标签数据，需要将一系列无标签的训练数据，输入到某个算法中，找到这个数据的内在规律、结构或模式。<br>聚类算法（clustering）实现将一系列无标签的训练数据，分成不同的点集（称为簇）。典型的聚类算法有K-均值算法。典型的应用有：</p><a id="more"></a><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/3.1.2%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95%E5%BA%94%E7%94%A8.png?raw=true" alt="聚类算法应用"></p><h1 id="3-1-1-K-Means（K-均值算法）"><a href="#3-1-1-K-Means（K-均值算法）" class="headerlink" title=" 3.1.1 K-Means（K-均值算法）"></a><font color=blue> 3.1.1 K-Means（K-均值算法）</font></h1><p>\(K\)-均值算法接受一个未标记的数据集，然后将数据聚类成多个类，即使在没有非常明显区分的组群的情况下也可以。</p><h2 id="算法基本思想："><a href="#算法基本思想：" class="headerlink" title=" 算法基本思想："></a><font color=DodgerBlue> 算法基本思想：</font></h2><ol><li>首先随机选择\(K\)个点，称为聚类中心。</li><li>其次对于数据集中的每一个数据，按照距离 \(K\) 个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点组成一个组。</li><li>然后计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。<br>再次重复以上两个步骤直至中心点不再变化，至此数据集聚类成\(K\)个组。</li></ol><h2 id="算法描述："><a href="#算法描述：" class="headerlink" title=" 算法描述："></a><font color=DodgerBlue> 算法描述：</font></h2><p><strong>模型参数：</strong>聚类个数: \(K\)；训练集 \( \{x^{(1)} , x^{(2)} ,…, x^{(m)}\}\ \ \ \ \ \  (x^{(i)} ∈ R^n , x_0=1) \); 随机选择\(K\)个聚类中心点：\(μ_1,μ_2,…,μ_K\ \ \ \ \ \ \ (μ_k∈R^n)\)</p><p><strong>代价函数：</strong>最小化所有的数据点与其所关联的聚类中心点之间的距离之和。</p><script type="math/tex; mode=display">J(c^{(1)},c^{(2)},…,c^{(m)},μ_1,μ_2,…,μ_K )=1/m ∑_{i=1}^m ||x^{(i)}-μ_{c^{(i)}} ||^2</script><script type="math/tex; mode=display">min_{c^{(1)},c^{(2)},…,c^{(m)},μ_1,μ_2,…,μ_K} J(c^{(1)},c^{(2)},…,c^{(m)},μ_1,μ_2,…,μ_K )</script><pre>Repeat {    for i = 1 to m        c(i):= index (from 1 to  K) of cluster centroid closest to x(i)    for k= 1 to K        μ_k:= average (mean) of points assigned to cluster K}</pre><h2 id="算法应用注意点："><a href="#算法应用注意点：" class="headerlink" title=" 算法应用注意点："></a><font color=DodgerBlue> 算法应用注意点：</font></h2><p><strong>聚类中心随机初始化：</strong>随机选择\(K\)个训练实例(\(K&lt; m \))，即聚类中心点的个数要小于所有训练集实例的数量，然后令 \(K\) 个聚类中心分别与这\(K\)个训练实例相等。初始化存在可能会停留在一个局部最小值处的问题：</p><ul><li>当\(K\)较小的时候（2-10）：通常需要多次运行 \(K\)-均值算法，每一次都重新进行随机初始化，最后再比较多次运行 \(K\)-均值的结果，选择代价函数最小的结果。</li><li>当\(K\)较大，可能不会有明显地改善，一般不需要做多次的随机初始化。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/3.1.3K%E9%80%89%E6%8B%A9.png?raw=true" alt="K值选择"></p><p><strong>选择聚类数K：</strong></p><ul><li>先思考运用 \(K\)-均值算法聚类的动机是什么，然后选择能最好服务于该目标的聚类数。</li><li>“肘部法则”：从\(K=1\)开始，改变K值，运行\(K\)均值聚类方法，计算畸变函数 \(J\)，绘制\(J\)曲线。畸变值在刚开始是会迅速下降，在 3 的时候达到一个肘点；在此之后，畸变值就下降的非常慢， \(K=3\)之后就下降得很慢，那么就选\(K\)等于 3；应用“肘部法则”可以有效地选择聚类个数。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/3.1.4%E8%82%98%E9%83%A8%E6%B3%95%E5%88%99.png?raw=true" alt="肘部法则"></p><h2 id="K-means与KNN比较："><a href="#K-means与KNN比较：" class="headerlink" title=" K-means与KNN比较："></a><font color=DodgerBlue> K-means与KNN比较：</font></h2><p><strong>相同点：</strong>都是计算最近邻，一般都用欧氏距离。&lt;/br&gt;<br><strong>不同点：</strong></p><ol><li>KNN是分类、输入样本带标签的、监督学习、没有训练过程的算法</li><li>K-means是聚类、输入样本不带标签的、非监督学习、有训练过程的算法</li><li>K的含义不同。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 3. 非监督学习算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.6 神经网络算法</title>
      <link href="/2017/07/02/2.6%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95/"/>
      <url>/2017/07/02/2.6%20%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>人类大脑可以处理非常多的不同事情，但不需要用实现成千上万个不同的程序，只需要一个单一的学习算法，然后通过神经网络自学掌握如何处理这些不同类型的数据。神经网络算法是模拟大脑学习的一种很古老的且计算量偏大的算法。</p><a id="more"></a><h1 id="2-6-1-神经网络算法基础"><a href="#2-6-1-神经网络算法基础" class="headerlink" title="2.6.1 神经网络算法基础"></a><font color=blue>2.6.1 神经网络算法基础</font></h1><h2 id="2-6-1-1-大脑中的神经网络"><a href="#2-6-1-1-大脑中的神经网络" class="headerlink" title="2.6.1.1 大脑中的神经网络"></a><font color=DodgerBlue>2.6.1.1 大脑中的神经网络</font></h2><p><strong>大脑神经元模型：树突接受输入信息，输入信息经由神经核根据自身的模型进行处理，由轴突将处理后的信息向其他神经元传递。</strong></p><ul><li>树突（Dendrite）：信息输入单元，一个神经元可包含多个输入。</li><li>神经核（Nucleus）：信息处理单元。</li><li>轴突（Axon）：信息输出单元，一个神经元有且只有一个输出。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.6.1%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png?raw=true" alt="大脑神经元"></p><p><strong>大脑神经网络模型：由大量神经元相互链接并通过电脉冲来交流的一个网络。</strong></p><h2 id="2-6-1-2-神经网络模型"><a href="#2-6-1-2-神经网络模型" class="headerlink" title="2.6.1.2 神经网络模型"></a><font color=DodgerBlue>2.6.1.2 神经网络模型</font></h2><h3 id="神经元模型："><a href="#神经元模型：" class="headerlink" title="神经元模型："></a>神经元模型：</h3><script type="math/tex; mode=display">h_θ (x)= g(θ_0^{(1)} x_0+θ_1^{(1)} x_1+θ_2^{(1)} x_2+θ_3^{(1)} x_3) →</script><script type="math/tex; mode=display">h_θ (x)= g(θ_1^{(1)} x_1+θ_2^{(1)} x_2+θ_3^{(1)} x_3 + b)</script><ul><li>\(x_0\)代表偏置量，固定值为1。</li><li>\( g(z)= \frac {1}{1+e^{-z}} \) 为激励函数。</li><li>参数\(θ\)为权重。</li><li>\(θ_0^{(1)}\) \(x_0\)为偏置，一般记为\(b\).</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.6.2%E7%A5%9E%E7%BB%8F%E5%85%83%E6%A8%A1%E5%9E%8B.png?raw=true" alt="神经元模型"></p><h3 id="神经网络一般模型："><a href="#神经网络一般模型：" class="headerlink" title="神经网络一般模型："></a>神经网络一般模型：</h3><p><strong>模型描述：</strong>训练集 \(\{(x^{(1)} , y^{(1)}) , (x^{(2)} , y^{(2)}) ,…, (x^{(m)} , y^{(m)})\}\) 有\(m\)个训练样本；神经网络总层数\(L=4\)；每层网络的神经元个数 \(S_1=3;S_2=5;S_3=5;S_4=4\)；输出层神经元个数个数\(S_L=4\)；输出单元的个数\(K=4\)；</p><ul><li>输入层负责原始数据输入：\(x_1,x_2,x_3\)</li><li>隐藏层负责将数据进行处理，然后呈递到下一层, \(a_i^{(j)}\) 代表第\(j\)层第\(i\)个激活单元输出值。 如：第2层的第1、2、3激活单元的输出值为 \(a_1^{(2)} , a_2^{(2)} , a_3^{(2)}\) </li><li>输出层负责最终输出\(h_θ (x)\)。</li><li>神经网络中除输入层外每个神经元都包含有一个偏置\(b\)。</li><li>\( θ^{(j)} \)代表从第\(j\)层映射到第\(j+1\)层时的权重矩阵。若神经网络在\(j\)层有\(m\)个单元，\(j+1\)层有\(n\)个单元，则\( θ^{(j)} \)的维度为\( n(m+1) \) 。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.6.3%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%80%E8%88%AC%E6%A8%A1%E5%9E%8B.png?raw=true" alt="神经网络一般模型"></p><p><strong>模型表示：</strong>\(x\)代表输入层矩阵，\(z\)代表中间层激活单元的带权输入值矩阵，\(a\)代表中间层激活单元输出值矩阵，\(θ\)代表每层的权重矩阵.</p><script type="math/tex; mode=display">\begin {cases}a_1^{(2)}=g(θ_{11}^{(1)} x_1+θ_{12}^{(1)} x_2+θ_{13}^{(1)} x_3 + b_1^{(2)})\\a_2^{(2)}=g(θ_{21}^{(1)} x_1+θ_{22}^{(1)} x_2+θ_{23}^{(1)} x_3 + b_2^{(2)})\\a_3^{(2)}=g(θ_{31}^{(1)} x_1+θ_{32}^{(1)} x_2+θ_{33}^{(1)} x_3 + b_3^{(2)})\\a_4^{(2)}=g(θ_{41}^{(1)} x_1+θ_{42}^{(1)} x_2+θ_{43}^{(1)} x_3 + b_4^{(2)})\\a_5^{(2)}=g(θ_{51}^{(1)} x_1+θ_{52}^{(1)} x_2+θ_{53}^{(1)} x_3 + b_5^{(2)})\\\end {cases}</script><script type="math/tex; mode=display">z^{(2)}= θ^{(1)} x+b^{(2)}</script><script type="math/tex; mode=display">a^{(2)}=g(z^{(2)})</script><script type="math/tex; mode=display">z^{(3)}= θ^{(2)} a^{(2)}+b^{(3)}</script><script type="math/tex; mode=display">h_θ (x)=g(z^{(3)})</script><h3 id="神经网络分类："><a href="#神经网络分类：" class="headerlink" title="神经网络分类："></a>神经网络分类：</h3><ol><li>二类分类神经网络：\(S_L=1,yϵ\{0,1\}\)&lt;/br&gt;</li><li><p>多类分类神经网络（\(K\)类分类：神经网络输出层有多个输出)</p><ul><li>描述：<script type="math/tex">S_L=K k≥3,y ϵ R^K\ \ R^K为K维只有一个元素为1的向量组</script><script type="math/tex; mode=display">y ϵ ( \left[\begin{matrix}  1  \\    0  \\    0  \\    0  \\\end{matrix}\right],\left[\begin{matrix}  0  \\    1  \\    0  \\    0  \\\end{matrix}\right],\left[\begin{matrix}  0  \\    0  \\    1  \\    0  \\\end{matrix}\right],\left[\begin{matrix}  0  \\    0  \\    0  \\    1  \\\end{matrix}\right])</script></li><li>示例：训练一个神经网络算法来识别路人、汽车、摩托车和卡车，在输出层应该有四个值，输出层四个神经元分别用来表示4 类。</li></ul></li></ol><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.6.10%E5%9B%9B%E5%88%86%E7%B1%BB%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B.png?raw=true" alt="四分类神经网络模型"></p><h2 id="2-6-1-3-神经网络与逻辑回归"><a href="#2-6-1-3-神经网络与逻辑回归" class="headerlink" title="2.6.1.3 神经网络与逻辑回归"></a><font color=DodgerBlue>2.6.1.3 神经网络与逻辑回归</font></h2><p>可以将逻辑回归看做是仅含有一个神经元的单层的神经网络。也可理解为神经网络来源于逻辑回归，神经网络只不过把逻辑回归中的输出重新变成了中间层的输入。</p><h2 id="2-6-1-4-神经网络与逻辑函数构造"><a href="#2-6-1-4-神经网络与逻辑函数构造" class="headerlink" title="2.6.1.4 神经网络与逻辑函数构造"></a><font color=DodgerBlue>2.6.1.4 神经网络与逻辑函数构造</font></h2><p>神经网络可以构造出基本的逻辑运算，通过逻辑运算的叠加，又可以构造出越来越复杂的函数。</p><p><strong>逻辑与：</strong>\(x<em>1,x_2∈{0,1} ; \  \  \  y</em>=  x<em>1\ AND \ x_2\)<br>    权重矩阵：\(θ=(-30,20,20)\)则：\(h</em>θ (x)=g(-30+20x_1+20x_2)\)，神经网络模型及真值表：<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.6.5%E9%80%BB%E8%BE%91%E4%B8%8E.png?raw=true" alt="逻辑与"></p><p><strong>逻辑或：</strong>\(x<em>1,x_2∈{0,1} ; \  \  \  y=x_1 \  OR \ x_2\)<br>    权重矩阵：\(θ=(-10,20,20)\)则：\(h</em>θ (x)=g(-10+20x_1+20x_2)\)，神经网络模型为及真值表：<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.6.6%E9%80%BB%E8%BE%91%E6%88%96.png?raw=true" alt="逻辑或"></p><p><strong>逻辑非：</strong>\(x<em>1∈{0,1} ; \  \  \ \ \      y=NOT \ x_1\)<br>    权重矩阵：\(θ=(10,-20)\)则：\(h</em>θ (x)=g(10-20x_1)\)，神经网络模型为为及真值表：<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.6.7%E9%80%BB%E8%BE%91%E9%9D%9E.png?raw=true" alt="逻辑非"></p><p><strong>都取零结果才为1逻辑：</strong>\(x<em>1,x_2∈{0,1} ; \ \ \  y=(NOT \ x_1 )AND (NOT \ x_2 )\)<br>    权重矩阵：\(θ=(10,-20,-20)\)则：\(h</em>θ (x)=g(10-20x_1-20x_2)\)<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.6.8%E9%83%BD%E5%8F%96%E9%9B%B6%E6%89%8D%E4%B8%BA%E4%B8%80.png?raw=true" alt="都取零结果才为1逻辑"></p><p><strong>逻辑异或取反：</strong>\(x_1,x_2∈{0,1} \ \ \  \   y=x_1 \  XNOR \ x_2\)<br>权重矩阵：</p><script type="math/tex; mode=display">Θ^{(1)}=\left[  \begin{matrix}    -30&20&20 \\    10&-20&-10 \\  \end{matrix} \right];θ^{(2)}=(-10,20,20)</script><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.6.9%E9%80%BB%E8%BE%91%E5%BC%82%E6%88%96%E5%8F%96%E5%8F%8D.png?raw=true" alt="逻辑异或取反"></p><h1 id="2-6-2-神经网络梯度下降算法"><a href="#2-6-2-神经网络梯度下降算法" class="headerlink" title="2.6.2 神经网络梯度下降算法"></a><font color=blue>2.6.2 神经网络梯度下降算法</font></h1><h3 id="神经网络代价函数："><a href="#神经网络代价函数：" class="headerlink" title="神经网络代价函数："></a>神经网络代价函数：</h3><script type="math/tex; mode=display">J(θ)=-[1/m ∑_{i=1}^m∑_{k=1}^K(y_k^{(i)}log(h_Θ (x^{(i)} ))_k +\\(1-y_k^{(i)})log(1-(h_Θ (x^{(i)}))_k)]+\\λ/2m ∑_{l=1}^{L-1}∑_{i=1}^{S_l}∑_{j=1}^{S_l+1}(θ_{ji}^{(l)})^2</script><ul><li>\(H<em>θ (x)ϵR^K, (H</em>θ (x))_k\) 为神经网络最后一层第\(k\)个输出。</li><li>\(ι\)代表目前所计算的是第几层。</li><li>\(j\)代表目前计算层中的激活单元的下标，也将是下一层的第\(j\)个输入变量的下标。</li><li>\(i\)代表下一层中误差单元的下标，是受到权重矩阵中第\(i\)行影响的下一层中的误差单元的下标。</li></ul><h3 id="前向传播算法："><a href="#前向传播算法：" class="headerlink" title="前向传播算法："></a>前向传播算法：</h3><p>计算每一层激活单元的输出，每一个\(a\)都是由上一层所有的输入和每一个输入所对应的权重矩阵决定，把这样从左到右的算法称为前向传播算法。</p><script type="math/tex; mode=display">a^{(1)}=x\\z^{(2)}=θ^{(1)} a^{(1)}\\a^{(2)}=g(z^{(2)}) \ \ \ \ \ \ (remember\ add\ a_0^{(2)})\\z^{(3)}=θ^{(2)} a^{(2)}\\a^{(3)}=g(z^{(3)}) \ \ \ \ \ \ (remember\ add\ a_0^{(3)})\\z^{(4)}=θ^{(3)}a^{(3)}\\a^{(4)}=h_Θ (x)=g(z^{(4)} )</script><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.6.13%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95-1.png?raw=true" alt="前向传播算法"><br><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.6.14%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95-2.png?raw=true" alt="前向传播算法"></p><h3 id="反向传播算法："><a href="#反向传播算法：" class="headerlink" title="反向传播算法："></a>反向传播算法：</h3><p>计算代价函数的偏导数\(∂/(∂Θ_{ij}^{(l)}) J(θ)\)</p><p><strong>算法思想：</strong>首先计算最后一层的误差，然后再一层一层反向求出各层的误差，直到倒数第二层。&lt;/br&gt;<br><strong>算法演示：</strong>最后一层的误差是激活单元 \( a<em>k^{(4)} \) 与实际值 \( y_k \) 之间的误差，记为 \( δ^{(4)} \) ，\( δ_j^{(l)} \)表示第\(ι\)层神经网络的第\(j\)个神经元的误差 &lt;/br&gt;<br>第四层第j个神经元误差: $$δ_j^{(4)}=a_j^{(4)}-y_j\第四层误差:δ^{(4)}=a^{(4)}-y\<br>第三层误差:δ^{(3)}=(θ^{(3)})^Tδ^{(4)}.<em>g^{‘}(z^{(3)})&lt;=&gt;δ^{(3)}=a^{(3)}.</em>(1-a^{(3)})\第二层误差:δ^{(2)}=(θ^{(2)})^T δ^{(3)}.<em>g^{‘} (z^{(2)})&lt;=&gt;δ^{(2)}=a^{(2)}.</em>(1-a^{(2)})\第一层是输入变量，不存在误差:<br>δ^{(1)} 不存在\<br>\frac {∂}{∂θ</em>{ij}^{(l)}}  J(θ)=a_j^l δ_i^{l+1} 假设λ=0$$</p><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.6.15%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95.png?raw=true" alt="反向传播算法"></p><h3 id="神经网络梯度算法实现："><a href="#神经网络梯度算法实现：" class="headerlink" title="神经网络梯度算法实现："></a>神经网络梯度算法实现：</h3><ol><li>首先用正向传播方法计算出每一层的激活单元输出\(a^{(l)}\)。</li><li>其次利用训练集的结果与神经网络预测结果求出最后一层的误差。</li><li>然后利用该误差运用反向传播法计算出直至第二层的所有误差。</li><li>最后求得求得<script type="math/tex">\frac {∂}{∂θ_{ij}^{(l)}} J(θ)=a_j^l δ_i^{l+1}</script></li></ol><h1 id="2-6-3-神经网络算法应用"><a href="#2-6-3-神经网络算法应用" class="headerlink" title="2.6.3 神经网络算法应用"></a><font color=blue>2.6.3 神经网络算法应用</font></h1><h2 id="神经网络算法应用注意点："><a href="#神经网络算法应用注意点：" class="headerlink" title=" 神经网络算法应用注意点："></a><font color=DodgerBlue> 神经网络算法应用注意点：</font></h2><h3 id="梯度检验："><a href="#梯度检验：" class="headerlink" title="梯度检验："></a>梯度检验：</h3><p><strong>梯度检验的必要性：</strong>当对一个较为复杂的模型使用梯度下降算法时，虽然代价看上去在不断减小，但最终的结果可能并不是最优解。可采取梯度数值检验的方法来计算的导数值是否和通过反向传播算法计算出的\( \frac {∂}{∂θ_{ij}^{(l)}} J(θ)\)大致相同。 &lt;/br&gt;<br><strong>梯度检验实现方法：</strong>对梯度的估计采用的方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。即对于某个特定的\(θ\)，计算出在\(θ-ε\)和\(θ+ε\)代价值,\(ε\)是一个非常小的值，通常选取0.0001，然后求两个代价的平均，用以估计在\(θ\)处的代价值:<script type="math/tex">\frac {d}{dθ}J(θ)=\frac{J(θ+ε)-J(θ-ε)}{2ε}</script></p><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.6.17%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A2%AF%E5%BA%A6%E6%A3%80%E9%AA%8C%E5%AE%9E%E7%8E%B0.png?raw=true" alt="梯度检验"></p><h3 id="参数随机初始化："><a href="#参数随机初始化：" class="headerlink" title="参数随机初始化："></a>参数随机初始化：</h3><ol><li>任何优化算法都需要一些初始的参数，初始所有参数为0对于逻辑回归来说是可行的。</li><li>对于神经网络若令所有的初始参数都为0，这将意味着第二层的所有激活单元都会有相同的值。同理，如果初始所有的参数都为一个非0 的数，结果也是一样的。</li><li>通常初始参数为正负\(ε\)之间的随机值。</li></ol><h2 id="神经网络算法应用步骤："><a href="#神经网络算法应用步骤：" class="headerlink" title=" 神经网络算法应用步骤： "></a><font color=DodgerBlue> 神经网络算法应用步骤： </font></h2><h3 id="第一步：选择网络结构决定选择多少层以及决定每层分别有多少个单元"><a href="#第一步：选择网络结构决定选择多少层以及决定每层分别有多少个单元" class="headerlink" title="第一步：选择网络结构决定选择多少层以及决定每层分别有多少个单元"></a>第一步：选择网络结构决定选择多少层以及决定每层分别有多少个单元</h3><ol><li>确定第一层的单元数：训练集的特征数量。</li><li>确定最后一层的单元数：训练集的结果类的数量。</li><li>确定隐藏层的层数和每个隐藏层的单元数：若隐藏层数大于1，确保每个隐藏层的单元个数相同；隐藏层单元数取决于训练集的特征数量、输出的结果类的数量，一般大于训练集的特征数量；通常情况下隐藏层单元的个数越多越好。</li></ol><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.6.19%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%E9%80%89%E6%8B%A9.png?raw=true" alt="神经网络结构选择"></p><h3 id="第二步：训练神经网络"><a href="#第二步：训练神经网络" class="headerlink" title="第二步：训练神经网络"></a>第二步：训练神经网络</h3><ol><li>参数的随机初始化。</li><li>利用正向传播方法计算所有的\(x^{(i)}\)的\(H_θ (x^{(i)})\)。</li><li>编写计算代价函数\(J(θ)\)的代码。</li><li>利用反向传播方法计算所有偏导数，并利用梯度检验这些偏导数。</li><li>使用优化算法来最小化代价函数。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 2. 监督学习算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.5 支持向量机</title>
      <link href="/2017/06/23/2.5%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
      <url>/2017/06/23/2.5%20%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<p>非线性假设能够更好的建立分类模型，更好的拟合现实数据，但当特征量太多时非线性的计算量会非常的大，普通的逻辑回归模型不能有效地处理这么多的特征，支持向量机可以有效的解决此类的非线性问题。<br>支持向量机（Support Vector Machine）是一种二类分类模型：基本模型定义为特征空间上的间隔最大的线性（或非线性）分类器，学习策略便是间隔最大化，将问题最终转化为一个凸二次规划问题的求解。</p><a id="more"></a><h1 id="2-5-1-了解支持向量机"><a href="#2-5-1-了解支持向量机" class="headerlink" title="2.5.1 了解支持向量机"></a><font color=blue>2.5.1 了解支持向量机</font></h1><h2 id="线性分类起源："><a href="#线性分类起源：" class="headerlink" title=" 线性分类起源："></a><font color=DodgerBlue> 线性分类起源：</font></h2><p><strong>逻辑回归：</strong>将特性的线性组合作为自变量，由于自变量的取值范围是负无穷到正无穷。因此使用Sigmoid函数将自变量映射到(0,1)上，映射后的值被认为是属于y=1的概率：</p><pre><code>$$h_θ (x)=g(z)=g(θ^T X)=\frac &#123;1&#125;&#123;1+e^&#123;-θ^T X&#125;&#125;=\begin&#123;cases&#125;    1\ \ \ \ \ \ 若y=1希望h_θ (x)≈1,θ^T X≫0 )     \\    0\ \ \ \ \ \ 若y=0希望h_θ (x)≈0,θ^T X≪0)\end&#123;cases&#125;$$</code></pre><p><strong>线性分类函数：</strong>将结果标签\(y=0\)和\(y=1\)替换为\(y=-1, y=1\)，然后将\(θ_1 x_1+θ_2 x_2+⋯θ_n x_n\)替换成\(W^T X\)，将\(θ_0\)替换成\(b\):</p><pre><code> $$g(z)=g(W^T X+b)=\frac &#123;1&#125;&#123;1+e^&#123;W^T X+b&#125;&#125;=\begin&#123;cases&#125;    1\ \ \ \ \ \ \ 若y=1,W^T X+b≫0\    \\    -1\ \ \ \ \ \ 若y=0,W^T X+b≪0) \end&#123;cases&#125;$$</code></pre><font color=OrangeRed>线性函数在一维空间里就是一个点，在二维空间里就是一条直线，三维空间里就是一个平面，如果不关注空间的维数，这种线性函数还有一个统一的名称“超平面”。</font><h2 id="线性分类器："><a href="#线性分类器：" class="headerlink" title=" 线性分类器："></a><font color=DodgerBlue> 线性分类器：</font></h2><p><strong>描述：</strong>给定一些数据点，它们分别属于两个不同的类，现在要找到一个线性分类器把这些数据分成两类。若用\(x\)表示数据点，用\(y\)表示类别（y可以取1或者-1代表两个类），一个线性分类器的学习目标便是要在n维的数据空间中找到一个超平面，其方程表示为：<script type="math/tex">W^T X+b=0</script></p><p><strong>直观理解</strong>：</p><ul><li>假设有一个二维平面，平面上有两种不同的数据，分别用圈和叉表示，假设可以用一条直线将这两类数据分开，这条直线就相当于一个超平面，超平面一边的数据点所对应的y全是-1，另一边所对应的\(y\)全是1。</li><li>这个超平面可以用分类函数\(f(x)=w^T x+b\)表示，当\(f(x)=0\)时，\(x\)便是位于超平面上的点，而\(f(x)&gt;0\)的点对应\(y=1\)的数据点，\(f(x)&lt;0\)的点对应\(y=-1\)的点。</li><li>在进行分类的时候，遇到一个新的数据点\(x\)，将\(x\)代入\(f(x)\)中，如果\(f(x)<0\\)则将x的类别赋为-1，如果\\(f(x)>0\)则将x的类别赋为1。</li><li>直观而言这个超平面应该是最适合分开两类数据的直线。而判定“最适合”的标准就是这条直线离直线两边的数据的间隔最大。所以得寻找有着最大间隔的超平面。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.5.3%E8%B6%85%E5%B9%B3%E9%9D%A2.png?raw=true" alt="超平面">  </p><h2 id="函数间隔："><a href="#函数间隔：" class="headerlink" title=" 函数间隔："></a><font color=DodgerBlue> 函数间隔：</font></h2><script type="math/tex; mode=display">γ ̂=y(w^T x+b)=yf(x)</script><ul><li>在超平面\(w^T x+b=0\)确定的情况下，\(|w^T x+b|\)能够表示点\(x\)到距离超平面的远近.</li><li>通过观察\(w^T x+b\)的符号与类标记\(y\)的符号是否一致可判断分类是否正确，可以用\(y(w^T x+b)\)的正负性来判定或表示分类的正确性。</li><li>超平面\((w,b)\)关于所有样本点\( (x^{(i)} , y^{(i)}) \)的函数间隔最小值，便为超平面关于训练数据集T的函数间隔：<script type="math/tex">γ ̂=min_{i=1,2…n} γ ̂^{(i)}</script></li></ul><h2 id="几何间隔："><a href="#几何间隔：" class="headerlink" title=" 几何间隔："></a><font color=DodgerBlue> 几何间隔：</font></h2><script type="math/tex; mode=display">γ ̃=γ ̂/‖w‖</script><ul><li>函数间隔存在的问题：若成比例的改变\(w\)和\(b\)（如将它们改成\(2w\)和\(2b\)），则函数间隔的值\(f(x)\)却变成了原来的2倍，虽然此时超平面没有改变。</li><li>几何间隔就是函数间隔除以\(‖w‖\)，而且函数间隔\(y(w^T x+b)=yf(x)\)实际上就是\(|f(x)|\)，几何间隔\(|f(x)|⁄‖w‖ \)才是直观上的点到超平面的距离。</li></ul><h2 id="大间距分类器："><a href="#大间距分类器：" class="headerlink" title=" 大间距分类器："></a><font color=DodgerBlue> 大间距分类器：</font></h2><p><strong>描述：</strong>当超平面离数据点的“间隔”越大，分类的确信度也越大。所以，为了使得分类的确信度尽量高，需要让所选择的超平面能够最大化这个“间隔”值。这个间隔就是图中Gap的一半。</p><p><strong>公式推导：</strong>最大间隔分类器的目标函数可以定义为</p><pre><code>$$    \begin &#123;cases&#125;    max_i γ ̃= max_i γ ̂/‖w‖ \\    γ ̂^&#123;(i)&#125; = y^&#123;(i)&#125; (w^T x^&#123;(i)&#125;+b)≥γ ̂\ \ \ \ \ \  (i=1,…n)    \end &#123;cases&#125;$$为了方便推导和优化，令γ ̂=1，且这样做对目标函数的优化没有影响：$$    \begin &#123;cases&#125;    max_i\ γ ̃= max\ 1/‖w‖ =min\ ⁡   w^2    \\    γ ̂^&#123;(i)&#125; = y^&#123;(i)&#125; (w^T x^&#123;(i)&#125;+b)≥1 \ \ \ \ \  (i=1,…n)    \end &#123;cases&#125;$$</code></pre><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.5.4%E6%9C%80%E5%A4%A7%E5%88%86%E7%B1%BB%E5%99%A8.png?raw=true" alt="最大分类器"></p><p><strong>直观理解：</strong>中间的实线便是寻找到的最优超平面，其到两条虚线边界的距离相等，这个距离便是几何间隔\(γ ̃\)，两条虚线间隔边界之间的距离等于\(2γ ̃\)，而虚线间隔边界上的点则是支持向量。由于这些支持向量刚好在虚线间隔边界上，所以它们满足\(y(w^T x+b)=1\)，而对于所有不是支持向量的点，则显然有\(y(w^T x+b)&gt;1\)<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.5.5%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E6%9C%80%E4%BC%98%E8%B6%85%E5%B9%B3%E9%9D%A2.png?raw=true" alt="支持向量机最优超平面"></p><h1 id="2-5-2-支持向量机推导"><a href="#2-5-2-支持向量机推导" class="headerlink" title="2.5.2 支持向量机推导"></a><font color=Blue>2.5.2 支持向量机推导</font></h1><h2 id="逻辑回归代价函数："><a href="#逻辑回归代价函数：" class="headerlink" title=" 逻辑回归代价函数："></a><font color=DodgerBlue> 逻辑回归代价函数：</font></h2><script type="math/tex; mode=display">Cost(h_θ (x),y)=-y logh_θ (x)-(1-y)  log(1-h_θ (x))\\=-y log(1/(1+e^{-θ^T X} ))-(1-y)log(1-1/(1+e^{-θ^T X}))</script><script type="math/tex; mode=display">    \begin {cases}    若y=1，希望Ζ=θ^T X≫0：Cost(h_θ (x),y)=-log(1/(1+e^{-θ^T X} )) \\    若y=0，希望Ζ=θ^T X≪0：Cost(h_θ (x),y)=- log(1-1/(1+e^{-θ^T X}))    \end {cases}</script><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.5.6%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0.png?raw=true" alt="逻辑回归代价函数"></p><h2 id="支持向量机代价函数："><a href="#支持向量机代价函数：" class="headerlink" title=" 支持向量机代价函数："></a><font color=DodgerBlue> 支持向量机代价函数：</font></h2><p>由逻辑回归到支持向量机，修改代价函数曲线如下：<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.5.7%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0.png?raw=true" alt="SVM代价函数"></p><script type="math/tex; mode=display">h_θ (x)=         \begin {cases}         1\ \ \ \ \ if\ Ζ= θ^T X≥1 \\         0\ \ \ \ \ if Ζ= θ^T X<-1        \end {cases}</script><script type="math/tex; mode=display">min_θ C ∑_{i=1}^m (y^{(i)} cost_1 (θ^T x^{(i) } )+(1-y^{(i)}) cost_0 (θ^T x^{(i)})) + \frac {1}{2} ∑_{j=1}^n θ_j^2</script><p><strong>支持向量机可以看作是大间距分类器：</strong>若C非常大，则最小化代价函数的时候，将会很希望找到一个使第一项为0 的最优解：\(y^{(i)} cost_1 (θ^T x^{(i) } )+(1-y^{(i)}) cost_0 (θ^T x^{(i)})≈0\)</p><ul><li>训练样本\(y=1\)，需要找到一个\(θ\)，使得\(θ^T x≥1\)； \(y=0\)，需要找到一个\(θ\)，使得\(θ^T x≤-1\)。</li><li>当求解这个优化问题的时候，会得到一个非常有趣的决策边界。<script type="math/tex; mode=display">y^{(i)} cost_1 (θ^T x^{(i) } )+(1-y^{(i)}) cost_0 (θ^T x^{(i)})≈0\\   → min\ 1/2 ∑_{j=1}^n θ_j^2</script></li></ul><p><strong>关于C值选择：</strong></p><ul><li>\(C\)不要设置太大，当 \(C\)不是非常非常大时，可忽略掉一些异常点的影响，得到更好的决策界</li><li>\(C≈1/λ\)：\(C\)较大时，相当于\(λ \)较小，可能会导致过拟合，高方差。\(C\)较小时，相当于\(λ \)较大，可能会导致低拟合，高偏差。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.5.9C%E5%80%BC%E9%80%89%E6%8B%A9.png?raw=true" alt="C值选择"></p><h1 id="2-5-4-核函数"><a href="#2-5-4-核函数" class="headerlink" title="2.5.4 核函数"></a><font color=blue>2.5.4 核函数</font></h1><h2 id="什么是核函数？"><a href="#什么是核函数？" class="headerlink" title=" 什么是核函数？ "></a><font color=DodgerBlue> 什么是核函数？ </font></h2><p>SVM在处理非线性分类边界问题时， 通过选择一个核函数，将数据映射到高维空间，来解决在原始空间中线性不可分的问题。</p><p>在线性不可分的情况下，支持向量机首先在低维空间中完成计算，然后通过核函数将输入空间映射到高维特征空间，最终在高维特征空间中构造出最优分离超平面，从而把平面上本身不好分的非线性数据分开。将\(γ ̂^{(i)} = y^{(i)} (w^T x^{(i)} + b)≥1\ \ \ \ \  (i=1,…n)\)中的\(x\)换为高维空间。</p><h2 id="核函数解决非线性分类边界问题的思路："><a href="#核函数解决非线性分类边界问题的思路：" class="headerlink" title=" 核函数解决非线性分类边界问题的思路： "></a><font color=DodgerBlue> 核函数解决非线性分类边界问题的思路： </font></h2><ol><li><p>给定一个训练实例\(x\)，利用x的各个特征与预先选定的地标 \( ι^{(1)}, ι^{(2)}, ι^{(3)} \) 的近似程度来选取新的特征\(f_1,f_2,f_3\)</p><script type="math/tex; mode=display">f_1=similarity(x,ι^{(1)})</script><script type="math/tex; mode=display">f_2=similarity(x,ι^{(2)})</script><script type="math/tex; mode=display">f_3=similarity(x,ι^{(3)})</script><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.5.11%E6%A0%B8%E5%87%BD%E6%95%B0.png?raw=true" alt="核函数"></p></li><li><p>\(similarity(x,ι^{(i)} )\)就是核函数，选择一核函数，记为\(K(x,ι^{(i)})\)，利用核函数模型计算新特征，若选用的核函数为高斯核函数，则：</p><script type="math/tex; mode=display">f_1=similarity(x,ι^{(1)})=K(x,ι^{(1)})=exp(-||x-ι^{(1)}||^2/(2σ^2 ))</script><script type="math/tex; mode=display">f_2=similarity(x,ι^{(2)})=K(x,ι^{(2)})=exp(-||x-ι^{(2)}||^2/(2σ^2 ))</script><script type="math/tex; mode=display">f_3=similarity(x,ι^{(3)})=K(x,ι^{(3)})=exp(-||x-ι^{(3)}||^2/(2σ^2 ))</script></li></ol><p>含义： 若\(x≈ι^{(1)}，f_1≈1\)；若\(x\ \ far \ \ from \ \ \ ι^{(1)},f_1≈0\) 随着\(x\)的改变\( f \)值改变的速率受到\( σ^2\)的控制。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.5.12G%E7%9A%84%E7%90%86%E8%A7%A3.png?raw=true" alt="\\( σ^2\\)"><br><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.5.12%E6%A0%B8%E5%87%BD%E6%95%B0%E7%A4%BA%E4%BE%8B.png?raw=true" alt="核函数示例"></p><h2 id="核函数实现支持向量机新特性："><a href="#核函数实现支持向量机新特性：" class="headerlink" title=" 核函数实现支持向量机新特性： "></a><font color=DodgerBlue> 核函数实现支持向量机新特性： </font></h2><p><strong>选择地标：</strong>通常是根据训练集的数量选择地标的数量，即如果训练集中有\(m\)个实例 \( (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}) ,…, (x^{(m)}, y^{(m)}) \) ，则选取\(m\)个地标，并且令： \(ι^{(1)} = x^{(1)}, ι^{(2)} = x^{(2)} ,…, ι^{(m)} = x^{(m)}\) ,得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上。</p><p><strong>计算新的特征向量：</strong>对于特定的训练实例 \(x^{(i)}\)计算新的特征向量 \(f^{(i)}\)</p><pre><code>$$    f^&#123;(i)&#125;=    \begin &#123;cases&#125;        f_0^&#123;(i)&#125;=1     \\        f_1^&#123;(i)&#125;=similarity(x^&#123;(i)&#125;,ι^&#123;(1)&#125; )  \\        f_2^&#123;(i)&#125;=similarity(x^&#123;(i)&#125;,ι^&#123;(2)&#125; )  \\        ... \\        f_i^&#123;(i)&#125;=similarity(x^&#123;(i)&#125;,ι^&#123;(i)&#125; )=e^0=1  \\        ... \\        f_m^&#123;(i)&#125;=similarity(x^&#123;(i)&#125;,ι^&#123;(m)&#125; )  \\    \end &#123;cases&#125;$$</code></pre><p><strong>SVM代价函数：</strong></p><script type="math/tex; mode=display">min_θ C∑_{i=1}^m (y^{(i)} cost_1 (θ^T f^{(i) } )+(1-y^{(i)}) cost_0 (θ^T f^{(i)})) + 1/2 ∑_{j=1}^n θ_j^2</script><ul><li>在具体实施过程中，需要对最后的归一化项进行些微调整：\(∑_{j=1}^{n=m} θ_j^2 =θ^T θ\)</li><li>根据不同的核函数选择矩阵\(M\)：\(θ^T Mθ\)代替\(θ^T θ\)，简化计算。</li><li>理论上讲，可以在逻辑回归中使用核函数，但使用\(M\)来简化计算的方法不适用与逻辑回归，因此计算将非常耗费时间。</li></ul><h1 id="2-5-5-使用支持向量机"><a href="#2-5-5-使用支持向量机" class="headerlink" title="2.5.5 使用支持向量机"></a><font color=blue>2.5.5 使用支持向量机</font></h1><p>最小化支持向量机的代价函数可以使用现有的软件包，如liblinear，libsvm 等.</p><h2 id="选择支持向量机求解代价函数参数-θ-，需要做的工作："><a href="#选择支持向量机求解代价函数参数-θ-，需要做的工作：" class="headerlink" title=" 选择支持向量机求解代价函数参数\(θ\)，需要做的工作： "></a><font color=DodgerBlue> 选择支持向量机求解代价函数参数\(θ\)，需要做的工作： </font></h2><p><strong>选择合适的代价函数中的参数C：</strong></p><ul><li>\(C\)较大时，相当于\(λ \)较小，可能会导致过拟合，高方差。</li><li>\(C\)较小时，相当于\(λ \)较大，可能会导致低拟合，高偏差。</li></ul><p><strong>选择合适的核函数：</strong>核函数需要满足Mercers定理，才能被支持向量机的优化软件正确处理</p><ul><li>线性核函数（无核函数）：当\(θ^T x≥0\)时，预测\(y=1 \)。当不需要采用非常复杂的核函数，或训练集特征非常多，实例非常少的时候，可以采用这种不带核函数的支持向量机。</li><li>高斯核函数：<script type="math/tex">f_i=exp(-||x-ι^{(i)}||^2/2σ^2 ),where\ ι^{(i)}=x^{(i)}</script><ul><li>需要选择合适的\(σ\)：\(σ\)较大时，导致高方差；\(σ\)较小时，导致高偏差。</li><li>需要进行特征缩放：<script type="math/tex">||x-ι||^2=(x_1-ι_1)^2+(x_2-ι_2)^2+⋯+(x_n-ι_n)^2</script></li></ul></li><li>多项式核函数（Polynomial Kernel）：&lt;/br&gt;\(f=similarity(x,ι)=K(x,ι)=(x^T ι+constant)^{degree}\)</li><li>字符串核函数（String kernel）、卡方核函数（chi-square kernel）、直方图交集核函数（histogram<pre><code> intersection kernel）</code></pre></li></ul><p><strong>支持向量机多类分类问题：</strong></p><ul><li>多类分类问题：若一共有\(k\)个类，则需要\(k\)个模型，以及\(k\) 个参数向量\(θ\)。</li><li>同样也可以训练\(k\)个支持向量机来解决多类分类问题。但是大多数支持向量机软件包都有内置的多类分类功能，只要直接使用即可。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.5.13SVM%E5%A4%9A%E5%88%86%E7%B1%BB.png?raw=true" alt="SVM多分类"></li></ul>]]></content>
      
      
      <categories>
          
          <category> 2. 监督学习算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.4 贝叶斯分类器</title>
      <link href="/2017/06/17/2.4%20%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/"/>
      <url>/2017/06/17/2.4%20%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<h2 id="基本概念："><a href="#基本概念：" class="headerlink" title=" 基本概念："></a><font color=DodgerBlue> 基本概念：</font></h2><ul><li>客观概率：指一件事已经发生的频率，即频率流派。</li><li>主观概率：个人主观概率， 表明对某个事物发生的相信程度，即贝叶斯流派。</li><li>贝叶斯统计三要素：由先验概率\(P(A)\)和条件概率\(P(B|A)\)， 得到后验概率\(P(A|B)\).</li></ul><a id="more"></a><h2 id="贝叶斯分类器决策步骤："><a href="#贝叶斯分类器决策步骤：" class="headerlink" title=" 贝叶斯分类器决策步骤："></a><font color=DodgerBlue> 贝叶斯分类器决策步骤：</font></h2><ul><li>定义并区分现象和规律。</li><li>获取整个规律空间，得到某一规律的概率分布: \(P(规律)\)。</li><li>获取整个现象空间，得到某一现象的概率分布: \(P(现象)\)。</li><li>获取某一现象先验概率: \(P(现象|规律)\)。</li><li>根据贝叶斯概率公式求解后验概率， 得到假设空间的后验概率分布:<script type="math/tex; mode=display">P(规律|现象) = P(现象|规律) P(规律)/ P(现象)</script></li><li>找到有最大值的后验概率的规律，完成分类操作。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.4.1%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%9C%AC%E8%B4%A8.jpg?raw=true" alt="贝叶斯本质"></p><h2 id="对贝叶斯分类器的理解："><a href="#对贝叶斯分类器的理解：" class="headerlink" title=" 对贝叶斯分类器的理解： "></a><font color=DodgerBlue> 对贝叶斯分类器的理解： </font></h2><p>两个一模一样的碗，一号碗有30颗水果糖和10颗巧克力糖，二号碗有水果糖和巧克力糖各20颗。现在随机选择一个碗，从中摸出一颗糖，发现是水果糖。请问这颗水果糖最可能来自几号碗？</p><ul><li>定义和区分现象和规律：两个规律（从一号碗来的规律，从二号碗来的规律）、两种现象（水果糖现象，巧克力现象）。</li><li>获取整个规律空间概率分布：因为是两个一模一样的碗，所以两个规律的概率一样，\(P(一号碗)＝P(二号碗)＝0.5\)。</li><li>获取整个现象空间的概率分布：<script type="math/tex; mode=display">P(水果糖现象)＝(30+20)/(30+10+20+20)=0.625</script><script type="math/tex; mode=display">P(巧克力现象)=(10+20)/(30+10+20+20)=0.375</script></li><li>获取现象的先验概率：<script type="math/tex; mode=display">P(水果糖现象|一号碗)＝30/(30+10)=0.75</script><script type="math/tex; mode=display">P(巧克力现象|一号碗)=10/(30+10)=0.25</script><script type="math/tex; mode=display">P(水果糖现象|二号碗)=20/(20+20)=0.5</script><script type="math/tex; mode=display">P(巧克力现象|二号碗)=20/(20+20)=0.5</script></li><li>根据贝叶斯公式计算出现象的后验概率：<script type="math/tex; mode=display">P(一号碗|水果糖现象)＝\frac {P(水果糖现象|一号碗)P(一号碗)}{P(水果糖现象)}\\=\frac {0.75*0.5}{0.625}=0.6</script><script type="math/tex; mode=display">P(二号碗|水果糖现象)＝\frac {P(水果糖现象|二号碗)P(二号碗)}{P(水果糖现象)}\\=\frac {0.5*0.5}{0.625}=0.4</script></li><li>找到有最大值的后验概率的规律，完成分类操作：水果糖最可能来自一号碗。</li></ul><h2 id="机器学习算法流程："><a href="#机器学习算法流程：" class="headerlink" title=" 机器学习算法流程："></a><font color=DodgerBlue> 机器学习算法流程：</font></h2><ol><li>进行特征工程：确定特征属性，获取训练样本，\(y_i\) 表示训练集中的第\(i\)种标签。</li><li>训练算法：<ul><li>对每个类别计算分类概率（规律空间概率）：\(P(y_i)\);</li><li>对每个特征属性计算所有划分的条件概率（现象的先验概率）：\(P(x|y_i)\)</li></ul></li><li>预测：对每个类别分别计算 \(P(x|y_i)*P(y_i)\) 获取值中的最大值中的类别，为特征的所属类别.</li></ol><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.4.2%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%B5%81%E7%A8%8B.png?raw=true" alt="贝叶斯机器学习流程"></p>]]></content>
      
      
      <categories>
          
          <category> 2. 监督学习算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.3 决策树</title>
      <link href="/2017/06/17/2.3%20%E5%86%B3%E7%AD%96%E6%A0%91/"/>
      <url>/2017/06/17/2.3%20%E5%86%B3%E7%AD%96%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<p>决策树是通过一系列规则对数据进行分类的过程。决策树分为分类树和回归树两种，分类树对离散变量做决策树，回归树对连续变量做决策树。作为分类、预测问题的典型支持技术，它在用户划分、行为预测、规则梳理等方面具有广泛的应用前景。缺点：趋向过拟合，可能或陷于局部最小值中，没有在线学习。</p><a id="more"></a><h2 id="决策树的分类过程："><a href="#决策树的分类过程：" class="headerlink" title=" 决策树的分类过程： "></a><font color=DodgerBlue> 决策树的分类过程： </font></h2><ol><li>特征选择：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准标准，从而衍生出不同的决策树算法。</li><li>决策树生成：根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则停止决策树生长。</li><li>剪枝：在决策树构造时，由于训练数据中的噪音或孤立点，许多分枝反映的是训练数据中的异常，使用这样的判定树对类别未知的数据进行分类，分类的准确性不高。因此试图检测和减去这样的分支，检测和减去这些分支的过程被称为树剪枝。<ul><li>作用：剪枝可缩小树结构规模，消除过拟合。</li><li>剪枝评测标准：在决策树的不断剪枝操作过程中，将原样本集合或新数据集合作为测试数据，检验决策树对测试数据的预测精度，并计算出相应的错误率，若剪掉某个子树后的决策树对测试数据的预测精度或其他测度不降低，那么剪掉该子树。</li><li>预剪枝：根据一些原则及早的停止树增长，如树的深度达到用户所要的深度、节点中样本个数少于用户指定个数、不纯度指标下降的最大幅度小于用户指定的幅度等。</li><li>后剪枝：通过在完全生长的树上剪去分枝实现的，通过删除节点的分支来剪去树节点，可以使用的后剪枝方法有多种，比如：代价复杂性剪枝、最小误差剪枝、悲观误差剪枝等等。</li></ul></li></ol><h2 id="基于信息论的决策树算法："><a href="#基于信息论的决策树算法：" class="headerlink" title=" 基于信息论的决策树算法："></a><font color=DodgerBlue> 基于信息论的决策树算法：</font></h2><ol><li>ID3算法：算法建立在“奥卡姆剃刀”的基础上，越是小型的决策树越优于大的决策树；根据信息论的信息增益评估和选择特征，每次选择信息增益最大的特征做判断模块；没有剪枝的过程，为了避免过拟合，可通过裁剪合并相邻的无法产生大量信息增益的叶子节点。<ul><li>在训练集中，某个属性所取的不同值的个数越多，那么越有可能拿它来作为分裂属性。</li><li>不能处理连续分布的数据特征。</li></ul></li><li>C4.5算法：ID3的改进算法， 算法用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足在树构造过程中进行剪枝；能够完成对连续属性的离散化处理；能够对不完整数据进行处理。<ul><li>优点：分类规则易于理解、准确率较高。</li><li>缺点：效率低。因树构造过程中，需要对数据集进行多次的顺序扫描和排序。也是因为必须多次数据集扫描，C4.5只适合于能够驻留于内存的数据集。</li></ul></li><li>CART算法：Classification And Regression Tree<ul><li>采用Gini指数（选Gini指数最小的特征）作为分裂标准，同时它也是包含后剪枝操作。</li><li>ID3算法和C4.5算法虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但其生成的决策树分支较大，规模较大。</li></ul></li></ol><h2 id="Python算法决策树："><a href="#Python算法决策树：" class="headerlink" title=" Python算法决策树："></a><font color=DodgerBlue> Python算法决策树：</font></h2><p>不需要构造新的数据类型来存储决策树，使用字典dict即可方便的存储节点信息，永久存储则可以通过pickle或JSON将其写入文件中。trees模块定义了一个decisionTree对象，同时支持ID3和C4.5两种算法（C4.5算法连续特征的处理和后剪枝没有实现）。</p><h2 id="决策树算法实现与应用："><a href="#决策树算法实现与应用：" class="headerlink" title=" 决策树算法实现与应用： "></a><font color=DodgerBlue> 决策树算法实现与应用： </font></h2><p>具体请参照：<a href="https://rocskyfly.github.io/2017/08/20/IrisClassifier/">决策树算法实现与应用</a>&lt;/br&gt;</p>]]></content>
      
      
      <categories>
          
          <category> 2. 监督学习算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.2 KNN算法</title>
      <link href="/2017/06/17/2.2%20KNN%E7%AE%97%E6%B3%95/"/>
      <url>/2017/06/17/2.2%20KNN%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>KNN全名是k-Nearest Neighbors，实际上KNN算法也可以用来做回归，这里只讨论分类。<br>算法基本思想：若一个样本在特征空间中的k个最相似的样本中，属于某一个类别最多，则该样本也属于这个类别，通过计算欧氏距离来选定相似样本。</p><a id="more"></a><h2 id="算法过程："><a href="#算法过程：" class="headerlink" title=" 算法过程： "></a><font color=DodgerBlue> 算法过程： </font></h2><ol><li>将已经分好类的样本的特征输入，作为“参考样本”，也就是下一步的输入的未知样本的“邻居”。</li><li>输入待分类样本，通过这个样本的周围最近的K个已经分好类的邻居的类别来判断它的类别。若在这K个邻居中，某个类的数量最多，那么判断该样本为此类。</li></ol><h2 id="算法特点："><a href="#算法特点：" class="headerlink" title=" 算法特点： "></a><font color=DodgerBlue> 算法特点： </font></h2><ul><li>属于监督学习（数据集是带label的数据），但没有明显的前期训练过程。</li><li>被分好类的新样本，不作为下一个需要分类的样本的参照样本。</li></ul><h2 id="K值选择："><a href="#K值选择：" class="headerlink" title=" K值选择： "></a><font color=DodgerBlue> K值选择： </font></h2><ul><li>参数K取奇数为佳。</li><li>K取值不宜太小，以消除噪声影响，但过大又会使分类边界模糊，故 \(K\) 值需要进行参数优化。</li></ul><h2 id="算法举例："><a href="#算法举例：" class="headerlink" title=" 算法举例： "></a><font color=DodgerBlue> 算法举例： </font></h2><ul><li>图中的数据集是良好的数据，即都打好了label，一类是蓝色的正方形，一类是红色的三角形，那个绿色的圆形是待分类的数据。</li><li>如果K=3，那么离绿色点最近的有2个红色三角形和1个蓝色的正方形，这3个点投票，于是绿色的这个待分类点属于红色的三角形。</li><li>如果K=5，那么离绿色点最近的有2个红色三角形和3个蓝色的正方形，这5个点投票，于是绿色的这个待分类点属于蓝色的正方形。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.2KNN%E7%AE%97%E6%B3%95%E4%B8%BE%E4%BE%8B.png?raw=true" alt="KNN算法举例"></p><h2 id="KNN算法实现与应用："><a href="#KNN算法实现与应用：" class="headerlink" title=" KNN算法实现与应用："></a><font color=DodgerBlue> KNN算法实现与应用：</font></h2><p>具体请参照：<a href="https://rocskyfly.github.io/2017/08/20/IrisClassifier/">KNN算法实现与应用</a>&lt;/br&gt;</p>]]></content>
      
      
      <categories>
          
          <category> 2. 监督学习算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2.1 回归算法</title>
      <link href="/2017/06/10/2.1%20%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/"/>
      <url>/2017/06/10/2.1%20%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>在监督学习中有一个有标签的训练集，目标是找到能够区分正样本和负样本的决策边界，通过一系列输入数据的特征和输入数据的标签，拟合一个假设函数。</p><h1 id="2-1-1-线性回归算法"><a href="#2-1-1-线性回归算法" class="headerlink" title="2.1.1 线性回归算法"></a><font color=blue>2.1.1 线性回归算法</font></h1><h2 id="2-1-1-1-单变量线性回归"><a href="#2-1-1-1-单变量线性回归" class="headerlink" title="2.1.1.1 单变量线性回归"></a><font color=DodgerBlue>2.1.1.1 单变量线性回归</font></h2><p>Linear regression with one variable，以房价模型为例：</p><a id="more"></a><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.1%E6%88%BF%E4%BB%B7%E5%8D%95%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B.png?raw=true" alt="房价单变量模型"></p><p><strong>单变量线性回归模型假设：\(h_θ (x)=θ_0+θ_1 x\)</strong></p><p>\(M\)为训练集中实例的数量；\(x\)为特征/输入变量；\(y\)为目标变量/输出变量；\((x,y)\)代表训练集中的某个实例；\(((x^{(i)} ,y^{(i)})\)为训练集中的第\(i\)个实例；\(h\)表示假设函数；\(θ_0, θ_1 \)为模型参数.</p><p><strong>平方误差代价函数：</strong></p><script type="math/tex; mode=display">J(θ_0,θ_1)=1/2m ∑_{i=1}^m(h_θ (x^{(i)})-y^{(i)})^2</script><ul><li>建模误差：模型所预测的值与训练集中实际值之间的差距。</li><li>实现建模目标：寻找可以使得建模误差的平方和能够最小的模型参数\(θ<em>0, θ_1\)，获取平方误差代价函数的值最小\(minimize</em>{θ<em>0,θ_1} J(θ_0,θ_1 )\)，实现训练集与最优的直线相拟合，使训练集实例\((x,y)\)的预测值\(h</em>θ (x)\)接近实际值\(y\).</li></ul><p><strong>梯度下降算法：</strong></p><ul><li>作用：用来计算函数最小值的一种算法。</li><li>基本思想：以求代价函数\(minimize_{θ_0,θ_1} J(θ_0,θ_1 )\)为例。首先随机选择一个参数的组合\(θ_0, θ_1\)，可以是一个全零的向量，计算代价函数的值；    然后寻找下一组能让代价函数值下降的参数组合；如此往复，直到找到一个局部最小值。由于并没有尝试完所有的参数组合，所以不能确定得到的局部最小值是否是全局最小值，选择不同的初始参数组合，可能会找到不同的局部最小值。<br>  <img src= "/img/loading.gif" data-lazy-src="/img/ai/2.2%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E4%B8%89%E7%BB%B4%E5%9B%BE.png?raw=true" alt="梯度下降三维图"></li><li>梯度的几何意义：梯度是函数变化增加最快的地方，或者说沿着梯度向量的方向，更加容易找到函数的最大值。反过来说，沿着梯度向量相反的方向，梯度减少最快，更容易找到函数的最小值。</li><li>计算公式：<br>\(repeat\ until\ convergence\ for(j=0 and j=1) :\{\)<script type="math/tex; mode=display">{θ_j≔θ_j-α \frac {∂}{∂θ_j} J(θ_0,θ_1 ) }</script>\(\}\) &lt;/br&gt;<br>α 是学习率，它决定了沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。若α 太小一点点地挪动，去努力接近最低点，这样就需要很多步才能到达局部最低点；若α 太大，那么梯度下降法可能会越过最低点，可能造成无法收敛，甚至是发散。</li></ul><p><strong>运用梯度下降法解决单变量线性回归问题：</strong><br>关键在于求出代价函数的偏导数</p><script type="math/tex; mode=display">h_θ (x)=θ_0+θ_1 x</script><script type="math/tex; mode=display">J(θ_0,θ_1)=\frac {1}{2m} ∑_{i=1}^m(h_θ (x^{(i)})-y^{(i)})^2</script><p>\(repeat\ until\ convergence\ for(j=0 and j=1) :\{\)</p><script type="math/tex; mode=display">{θ_j≔θ_j-α \frac {∂}{∂θ_j} J(θ_0,θ_1 ) }</script><p>\(\}\)</p><script type="math/tex; mode=display">\frac {∂}{∂θ_j} J(θ_0,θ_1)=\frac {∂}{∂θ_j} \frac {1}{2m} ∑_{i=1}^m(h_θ (x^{(i)})-y^{(i)})^2</script><p>\(repeat\ until\ convergence\ and\ update θ_0, θ_1\ simultaneously :\{\)</p><script type="math/tex; mode=display">θ_0:=θ_0- \frac {α}{m} ∑_{i=1}^m (h_θ (x^{(i)})-y^{(i)})</script><script type="math/tex; mode=display">θ_1:=θ_1- \frac {α}{m} ∑_{i=1}^m (h_θ (x^{(i)})-y^{(i)}) x^{(i)}</script><p>\(\}\)</p><p><strong>单变量线性回归算法另类数学理解：\(h_θ (x)=θ_0+θ_1 x\)</strong></p><script type="math/tex; mode=display">Slope:θ_1=(\overline{X} \overline{Y} -\overline{XY})/(\overline{X}^2 -\overline{X^2})</script><script type="math/tex; mode=display">Interception:θ_0=\overline Y - θ_1 \overline{X}̅</script><script type="math/tex; mode=display">Squrared error:SE=(Y-h_θ (X))^2</script><h2 id="2-1-1-2-多变量线性回归"><a href="#2-1-1-2-多变量线性回归" class="headerlink" title="2.1.1.2 多变量线性回归"></a><font color=DodgerBlue>2.1.1.2 多变量线性回归</font></h2><p>Linear Regression with Multiple Variables，房价模型：<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.3%E6%88%BF%E4%BB%B7%E5%A4%9A%E5%8F%98%E9%87%8F%E6%A8%A1%E5%9E%8B.png?raw=true" alt="房价多变量模型"><br><strong>多维特征模型假设：\(h_θ (x)=θ_0 x_0+θ_1 x_1+θ_2 x_2+⋯+θ_n x_n\)</strong></p><p>\((x<em>1,x_2,…,x_n)\) 为模型特征；\(n\) 代表特征的数量；\(x^{(i)}\) 代表第 \(i\) 个训练实例（一个向量）；\(x_j^{(i)}\)代第\(i\)个训练实例的第 \(j\)个特征；引入\(x_0=1\)。多维特征模型向量表示：\(h</em>θ (x)=θ^T X\)</p><script type="math/tex; mode=display">X=\left[  \begin{matrix}   1  \\   x_1 \\   x_2 \\   ... \\   x_n  \end{matrix}  \right] θ =\left[  \begin{matrix}    θ_0  \\     θ_1 \\    θ_2 \\    ... \\    θ_n  \end{matrix} \right]  h_θ (x)=θ^T X</script><p><strong>多变量梯度下降算法：</strong></p><script type="math/tex; mode=display">Hypothesis:  h_θ (x)=θ_0 x_0+θ_1 x_1+θ_2 x_2+⋯+θ_n x_n</script><script type="math/tex; mode=display">Parameters:   θ_0,θ_1,…,θ_n</script><script type="math/tex; mode=display">Cost Function:     J(θ_0,θ_1,…,θ_n )= \frac {1}{2m} ∑_{i=1}^m(h_θ (x^{(i)})-y^{(i)})^2</script><p>\(repeat\ until\ convergence\ update\ θ_0, θ_1,…,θ_n\ simultaneously :\{\)<br>\({θ_j≔θ_j-α \frac {∂}{∂θ_j} J(θ_0,θ_1,…,θ_n)<br>}\)<br>\(\}\)&lt;/br&gt;<br>当代表特征的数量\(n&gt;1\)时，梯度下降算法变为：首先随机选择一系列的参数值，计算所有的预测结果后，再给所有的参数一个新的值，如此循环直到收敛。&lt;/br&gt;<br>\(repeat\ until\ convergence\ and\ update\ θ_0, θ_1,…,θ_n\ simultaneously:\{ \)</p><script type="math/tex; mode=display">θ_0≔θ_0-\frac {α}{m} ∑_{i=1}^m (h_θ (x^{(i)})-y^{(i)}) x_0^{(i)}</script><script type="math/tex; mode=display">θ_1≔θ_1-\frac {α}{m} ∑_{i=1}^m (h_θ (x^{(i)})-y^{(i)}) x_1^{(i)}</script><script type="math/tex; mode=display">...</script><script type="math/tex; mode=display">θ_n≔θ_n-\frac {α}{m} ∑_{i=1}^m (h_θ (x^{(i)})-y^{(i)}) x_n^{(i)}</script><p>\(\}\)</p><p><strong>提升多维特征梯度下降算法收敛速度：特征缩放</strong></p><ul><li>现象：假设使用两个特征，房屋的尺寸和房间的数量，尺寸的值为0-2000 平方英尺，而房间数量的值则是0-5，以两个参数分别为横纵坐标，绘制代价函数的等高线图能，看出图像会显得很扁，梯度下降算法需要非常多次的迭代才能收敛。</li><li>解决方法：保证这些特征都具有相近的尺度，尝试将所有特征的尺度都尽量缩放到-1到1 之间。</li><li>表达式：\(x_n= \frac {x_n-μ_n}{s_n}\) 其中\(μ_n\)是平均值，\(s_n\)是标准差。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.4%E6%95%B0%E6%8D%AEscaling.png?raw=true" alt="数据scaling"></li></ul><p><strong>多维特征梯度下降算法学习率：</strong></p><ul><li>若学习率α 过小，则达到收敛所需的迭代次数会非常高；如果学习率α 过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。</li><li>通常可以考虑尝试的学习率：α=0.001， 0.003，0.01，0.03，0.1，0.3，1，3，10 </li></ul><p><strong>高次多项式回归：</strong></p><ul><li>线性回归并不适用于所有数据，比如一个三次方模型：<script type="math/tex">h_θ (x)=θ_0+θ_1 x_1+θ_2 x_2^2+θ_3 x_3^3</script></li><li>多项式回归处理方法：通常需要先观察数据然后再决定准备尝试怎样的模型<ul><li>直接用曲线来拟合数据集，非线性回归算法（支持向量机）</li><li>将模型转化为线性回归模型：\(x_2 = x_2^2;x_3 = x_3^3\)</li></ul></li><li>若采用多项式回归模型，在运行梯度下降算法前，特征缩放非常有必要。</li></ul><p><strong>最小二乘法（正规方程算法）：解决某些线性回归问题</strong></p><ul><li>基本思想：求代价函数的极值为零时的参数，即:\(\frac {∂}{∂θ_j} J(θ_j )=0\)</li><li>假设训练集特征矩阵为 X（包含了x_0=1）并且训练集的结果为向量 Y，利用正规方程解出向量：\(θ={(X^T X)}^{-1} X^T Y\)</li><li>适用范围：<ul><li>对于不可逆的矩阵，正规方程方法是不能用的（通常是因为特征之间不独立，如同时包含英尺为单位的尺寸和米为单位的尺寸两个特征，也有可能是特征数量大于训练集的数量）。</li><li>如果特征数量 n 较大则运算代价大，因为矩阵逆的计算时间复杂度为 O(n3)，通常来说当 n 小于 10000 时还是可以接受的</li></ul></li></ul><h1 id="2-1-2-逻辑回归算法"><a href="#2-1-2-逻辑回归算法" class="headerlink" title=" 2.1.2 逻辑回归算法"></a><font color=blue> 2.1.2 逻辑回归算法</font></h1><p>Logistic regression是用来解决分类问题，如邮件是垃圾邮件还是正常邮件、肿瘤是是良性的还是恶性的、线上交易是欺诈的还是正常的等等。</p><h2 id="逻辑回归模型假设："><a href="#逻辑回归模型假设：" class="headerlink" title=" 逻辑回归模型假设："></a><font color=DodgerBlue> 逻辑回归模型假设：</font></h2><ul><li>二元分类：输出变量\(y∈\{0,1\}\)，其中0 表示负向类，1 表示正向类。处理二元分类问题的算法的输出值永远在0 到1 之间，代表y为正向类的概率。<script type="math/tex; mode=display">\begin{cases}  Classification:  y=0\ or\ 1 \\                    Logistic  regression:   0≤h _θ (x)≤1\end{cases}</script></li><li>S型函数： \(g(z)=\frac {1}{1+e^{-z}}\)函数输出值永远在0 到1 之间。输入值从负无穷到正无穷，映射到0和1之间，将这样的输出值表达为“可能性”。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.5S%E5%87%BD%E6%95%B0.png?raw=true" alt="S型函数"></li><li>逻辑回归模型假设：<script type="math/tex; mode=display">h_θ (x)=g(θ^T X)=\frac {1}{1+e^{-θ^T X}}=P(y=1 | x;θ)</script><ul><li>含义：对于给定的输入变量X，根据选择的参数θ计算输出变量等于1 的可能性。若对于给定的x，计算出\(h_θ (x)=0.7\)，则表示y有 70%的几率为正向类，有30%的几率为负向类\(P(y=0 |x;θ)=1-P(y=1 |x;θ)\)</li><li>判定边界：根据不同的数据呈现，可得到不同的决策边界：线性决策边界和非线性边界</li></ul></li></ul><h2 id="逻辑回归代价函数："><a href="#逻辑回归代价函数：" class="headerlink" title=" 逻辑回归代价函数："></a><font color=DodgerBlue> 逻辑回归代价函数：</font></h2><ul><li>误差平方和代价函数在逻辑回归模型中存在的问题：误差平方和代价函数将是一个非凸函数（Non-convex function）。<br><img src= "/img/loading.gif" data-lazy-src="https://raw.githubusercontent.com/rocskyfly/test/master/2.1.2.1%E9%9D%9E%E5%87%B8%E5%87%BD%E6%95%B0.png" alt="非凸函数"></li><li>逻辑回归的代价函数为：<script type="math/tex; mode=display">J(θ)=\frac {1}{m} ∑_1^m Cost(h_θ (x^{(i)} ),y^{(i)})</script><script type="math/tex; mode=display">Cost(h_θ (x),y)=\begin{cases}-log\ h_θ (x)\ \ \ \ \ \ \ \ \ \ \ \ \ \ if\ y=1 \\-log\ (1-h_θ (x))\ \ \ \ if\ y=0\end{cases}</script><ul><li>当实际的\( y=1 \)且\(h<em>θ (x\))也为 1 时误差为\(0\)，当 \(y=1\)但\(h</em>θ (x)\)不为1时误差随着hθ的变小而变大；</li><li>当实际的 \(y=0\) 且\(h<em>θ (x)\)也为\( 0 \)时代价为\( 0\)，当\(y=0\) 但\(h</em>θ (x)\)不为\( 0 \)时误差随着\(hθ\)的变大而变大<script type="math/tex; mode=display">Cost(h_θ (x),y)=-y log(h_θ (x)) -(1-y)log(1-h_θ (x))</script><script type="math/tex; mode=display">J(θ)=-\frac {1}{m} ∑_{i=1}^m [y^{(i)}  logh_θ (x^{(i)} )+(1-y^{(i)} )log(1-h_θ (x^{(i)} ))]</script></li></ul></li></ul><font color=OrangeRed>逻辑回归（对数回归）本质上是线性回归，只是在特征到结果的映射中加入了一层函数映射，即先把特征线性求和，然后使用函数 \\(g(z)\\)做为假设函数来预测，将连续值的特征值映射到0 和 1上。</font><h2 id="应用梯度下降算法："><a href="#应用梯度下降算法：" class="headerlink" title=" 应用梯度下降算法："></a><font color=DodgerBlue> 应用梯度下降算法：</font></h2><p>\(repeat\ until\ convergence\ and\ update θ_0, θ_1,…,θ_n\ simultaneously:\{ \)</p><script type="math/tex; mode=display">θ_0≔θ_0-\frac {α}{m} ∑_{i=1}^m (h_θ (x^{(i)})-y^{(i)}) x_0^{(i)}</script><script type="math/tex; mode=display">θ_1≔θ_1-\frac {α}{m} ∑_{i=1}^m (h_θ (x^{(i)})-y^{(i)}) x_1^{(i)}</script><script type="math/tex; mode=display">...</script><script type="math/tex; mode=display">θ_n≔θ_n-\frac {α}{m} ∑_{i=1}^m (h_θ (x^{(i)})-y^{(i)}) x_n^{(i)}</script><p>\(\}\)</p><ul><li>在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。</li><li>高级优化算法：共轭梯度法、BFGS (变尺度法) 、L-BFGS (限制变尺度法)等。使用比梯度下降更复杂的算法来最小化代价函数，通常不需要手动选择学习率α。</li></ul><h2 id="多类别分类（一对多）："><a href="#多类别分类（一对多）：" class="headerlink" title=" 多类别分类（一对多）："></a><font color=DodgerBlue> 多类别分类（一对多）：</font></h2><p>假设数据集有三个类别，\(三角形y=1、方框y=2、叉叉y=3\)</p><ul><li>分类思路：将其分成三个二元分类问题<ul><li>先从用三角形代表的类别1 开始，实际上可以创建一个新的”伪”训练集，类型2 和类型3 定为负类，类型1 设定为正类，创建一个新的训练集，拟合出一个合适的分类器（这里的三角形是正样本，而圆形代表负样本），就得到一个正边界。</li><li>将多个类中的一个类标记为正向类\(（ y=1）\)，然后将其他所有类都标记为负向类，这个模型记作\(h<em>θ^{(1)} (x)\), 接着，类似地第我们选择另一个类标记为正向类\(（ y=2）\)，再将其它类都标记为负向类，将这个模型记作\(h</em>θ^{(2)} (x)\)，依次类推最后得到一系列的模型简记为：\(h_θ^{(i)} (x) = P(y=i| x;θ)\ \ \ \i=(1,2,3…k)\)</li><li>最后，在需要做预测时，将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。</li></ul></li><li>多类别分类问关键点：<ul><li>挑选分类器：选择出哪一个分类器是可信度最高效果最好的，那么就可认为得到一个正确的分类，无论i值是多少，都有最高的概率值。</li><li>训练逻辑回归分类器：\(h<em>θ^{(i)} (x)\)其中\(i\)对应每一个可能的 \(y=i\)，最后为了做出预测，给出输入一个新的\( x \)值，用这个做预测，要做的就是在三个分类器里面输入 \(x\)，然后我们选择一个让\(h</em>θ^{(i)} (x)\)最大的\(i\)，即：\(max<em>i h</em>θ^{(i)} (x)\)。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.6%E5%A4%9A%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B.png?raw=true" alt="多类分类器"></li></ul></li></ul><h2 id="逻辑回归算法实现与应用："><a href="#逻辑回归算法实现与应用：" class="headerlink" title=" 逻辑回归算法实现与应用："></a><font color=DodgerBlue> 逻辑回归算法实现与应用：</font></h2><p>具体请参照：<a href="https://rocskyfly.github.io/2017/08/22/SpamEmailClassifier/">逻辑回归算法实现与应用</a>&lt;/br&gt;</p><h1 id="2-1-3-回归算法正则化"><a href="#2-1-3-回归算法正则化" class="headerlink" title="2.1.3 回归算法正则化"></a><font color=blue>2.1.3 回归算法正则化</font></h1><h2 id="2-1-3-1-过度拟合问题"><a href="#2-1-3-1-过度拟合问题" class="headerlink" title="2.1.3.1 过度拟合问题"></a><font color=DodgerBlue>2.1.3.1 过度拟合问题</font></h2><p>过度拟合也称为高方差，指的是如果有非常多的特征，通过学习得到的假设可能能够非常好地适应训练集（代价函数几乎为 0），但是可能会不能推广到新的数据，即不能做出相对准确的预测。以多项式理解， x 的次数越高，拟合的越好，但相应的预测的能力就可能变差。</p><p><strong>问题描述：</strong></p><ul><li>线性回归过度拟合问题：第一个模型是一个线性模型，不能很好地适应训练集，欠拟合高偏差；    第二个模型能比较好地适应训练集，在新输入变量进行预测时效果也比较好；    第三个模型过于强调拟合原始数据，丢失了算法预测新数据的本质，过拟合高方差。</li></ul><p><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.8%E7%BA%BF%E6%80%A7%E8%BF%87%E6%8B%9F%E5%90%88.png?raw=true" alt="线性过拟合"></p><ul><li>逻辑回归过度拟合问题：<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/2.9%E9%80%BB%E8%BE%91%E8%BF%87%E6%8B%9F%E5%90%88.png?raw=true" alt="逻辑过度拟合问题"> </li></ul><p><strong>解决方案：</strong></p><ol><li>降维：丢弃不能帮助做出正确预测的特征，手工选择保留哪些特征，或使用主成分分析算法。</li><li>正则化：保留所有的特征，但减小特征参数 \(θ_j\) ，即减少某些特征对最终预测值的影响。</li></ol><h2 id="2-1-3-2-回归正则化"><a href="#2-1-3-2-回归正则化" class="headerlink" title="2.1.3.2 回归正则化"></a><font color=DodgerBlue>2.1.3.2 回归正则化</font></h2><p><strong>正则化模型推导：</strong></p><ul><li>正则化代价函数推导：<ul><li>减少\(θ_3\)和\(θ_4\)的大小：正是那些高次项导致了过拟合的产生，所以若能让这些高次项的系数接近于0的话，就能很好的拟合，即可减少\(θ_3\)和\(θ_4\)的大小（设置一点惩罚）：<script type="math/tex; mode=display">min_θ \frac {1}{2m} ∑_{i=1}^m(h_θ (x^{(i)})-y^{(i)})^2 +1000θ_3^2+10000θ_4^2</script></li><li>减小所有\(θ_j\)的大小：若有非常多的特征，并不知道哪些特征需要惩罚，对所有的特征进行惩罚，并让代价函数最优化的软件来选择惩罚的程度，根据惯例一般不对\(θ_0\)进行惩罚，得到了一个较为简单的能防止过拟合问题的假设：<script type="math/tex; mode=display">min_θ \frac {1}{2m} [∑_{i=1}^m(h_θ (x^{(i)})-y^{(i)})^2 +λ∑_{j=1}^n θ_j^2] \ \ \ \ (λ 又称为正则化参数 )</script>保证很好的拟合数据项：    \( ∑<em>{i=1}^m(h</em>θ (x^{(i)})-y^{(i)})^2\)<br>保证更好的预测数据，防止过拟合项：    \(λ∑_{j=1}^nθ_j^2\)</li></ul></li><li>归一化参数λ特点：&lt;/br&gt;<br>λ过大会最小化所有的参数，导致模型变成 \(h_θ (x)=θ_0\)，欠拟合。&lt;/br&gt;λ过小会最大化所有的参数，导致模型过拟合。</li></ul><p><strong>正则化线性回归：</strong></p><ul><li><p>正则化梯度下降算法：&lt;/br&gt;<br>\(repeat\ until\ convergence\ and\ update\ θ_0, θ_1,…,θ_n\ simultaneously:\{ \)</p><script type="math/tex; mode=display">θ_0≔θ_0-\frac {α}{m} ∑_{i=1}^m (h_θ (x^{(i)})-y^{(i)}) x_0^{(i)}</script><script type="math/tex; mode=display">θ_j≔θ_j-\frac {α}{m} ∑_{i=1}^m ((h_θ (x^{(i)})-y^{(i)}) x_n^{(i)}+\frac {λ}{m} θ_j)\ \ (j=1,2,…,n)</script><p>\(\}\)&lt;/br&gt;<br>合并同类项有：&lt;/br&gt;<br>\(repeat\ until\ convergence\ and\ update\ θ_0, θ_1,…,θ_n\ simultaneously:\{ \)</p><script type="math/tex; mode=display">θ_0≔θ_0-\frac {α}{m} ∑_{i=1}^m (h_θ (x^{(i)})-y^{(i)}) x_0^{(i)}</script><script type="math/tex; mode=display">θ_j≔θ_j (1-α \frac {λ}{m}) - \frac {α}{m} ∑_{i=1}^m (h_θ (x^{(i)})-y^{(i)}) x_n^{(i)}\ \ (j=1,2,…,n)</script><p>\(\}\)&lt;/br&gt;</p></li><li><p>正则化正规方程算法：</p><script type="math/tex; mode=display">θ=(X^T X+λ\left[\begin{matrix}  0&0&0&0&0&0  \\  0&1&0&0&0&0  \\  0&0&1&0&0&0  \\  .&.&.&.&.&.\\  0&0&0&0&0&1  \\\end{matrix}\right])^{-1}X^T Y</script></li></ul><p><strong>正则化逻辑回归：</strong></p><script type="math/tex; mode=display">J(θ)=-\frac {1}{m} ∑_{i=1}^m [y^{(i)}  logh_θ (x^{(i)} )+(1-y^{(i)} )log(1-h_θ (x^{(i)} ))]+\frac {λ}{2m} ∑_{j=1}^nθ_j^2</script><p>\(repeat\ until\ convergence\ and\ update\ θ_0, θ_1,…,θ_n\ simultaneously:\{ \)</p><script type="math/tex; mode=display">θ_0≔θ_0-\frac {α}{m} ∑_{i=1}^m (h_θ (x^{(i)})-y^{(i)}) x_0^{(i)}</script><script type="math/tex; mode=display">θ_j≔θ_j (1-α \frac {λ}{m} - \frac {α}{m} ∑_{i=1}^m (h_θ (x^{(i)})-y^{(i)}) x_n^{(i)}\ \ (j=1,2,…,n)</script><p>\(\}\)</p><p><em><font color=OrangeRed>看上去同线性回归一样，但是知道\(h_θ(x)=g(θ^T X)\)，所以与线性回归不同。&lt;/br&gt;虽然正则化的逻辑回归中的梯度下降和正则化的线性回归中的表达式看起来一样。</font></em></p>]]></content>
      
      
      <categories>
          
          <category> 2. 监督学习算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人工智能与机器学习</title>
      <link href="/2017/06/03/1.%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
      <url>/2017/06/03/1.%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="1-1-人工智能"><a href="#1-1-人工智能" class="headerlink" title=" 1.1 人工智能"></a><font color=blue> 1.1 人工智能</font></h1><h2 id="1-1-1-人工智能的基本概念"><a href="#1-1-1-人工智能的基本概念" class="headerlink" title="1.1.1 人工智能的基本概念"></a><font color=DodgerBlue>1.1.1 人工智能的基本概念</font></h2><p>人工智能指的是由机器或计算机程序学习如何完成通常由人类完成的需要智能形式的任务。<br>智能机器可以是一个虚拟的或者物理的机器人，有自主的感知、认知、决策、学习、执行和社会协作能力，符合人类情感、伦理与道德观念。</p><h2 id="1-1-2-人工智能涵盖的学科"><a href="#1-1-2-人工智能涵盖的学科" class="headerlink" title=" 1.1.2 人工智能涵盖的学科"></a><font color=DodgerBlue> 1.1.2 人工智能涵盖的学科</font></h2><ol><li>计算机视觉：模式识别，图像处理等。感知的图像往往只占5%，提供一些蛛丝马迹；而后面的95%，包括功能、物理、因果、动机等等是要靠人的想象和推理过程来完成的。<a id="more"></a></li><li>自然语言理解与交流：语音识别、合成、对话等。语言产生的基础是人要寻求合作。在语言产生之前，人类就已经有了十分丰富的认知基础，也就是上一节谈的那些表达。没有这样的认知基础，语言是空洞的符号，对话也不可能发生。</li><li>认知与推理：包含各种物理和社会常识。当有了共同价值观的时候，就有社会道德和伦理规范，这都可以推导出来了。机器人要与人共生共存必须理解人的团体的社会道德和伦理规范。这个认识论是机器人发展的必经之道。</li><li>机器人学：机械、控制、设计、运动规划、任务规划等。</li><li>博弈与伦理：多代理人agents的交互、对抗与合作，机器人与社会融合等议题。获取、共享人类的价值观。</li><li>机器学习：各种统计的建模、分析工具和计算的方法。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E6%B6%B5%E7%9B%96%E7%9A%84%E5%AD%A6%E7%A7%91.png" alt="人工智能涵盖的学科"></li></ol><p><em><font color=OrangeRed>传统人工智能领域：逻辑推理、搜索博弈、专家系统等。<br/><br>人脑的功耗大约是10-25瓦。<br/>现在的人工智能和机器人，关键问题是缺乏物理的常识和社会的常识。</font> </em></p><h2 id="1-1-3-人工智能-amp-机器学习-amp-深度学习之间的关系"><a href="#1-1-3-人工智能-amp-机器学习-amp-深度学习之间的关系" class="headerlink" title="1.1.3 人工智能&amp;机器学习&amp;深度学习之间的关系"></a><font color=DodgerBlue>1.1.3 人工智能&amp;机器学习&amp;深度学习之间的关系</font></h2><p><strong>机器学习（Machine Learning）：机器学习是对能通过经验自动改进的计算机算法的研究。机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。</strong></p><ul><li>与传统的为解决特定任务、硬编码的软件程序不同，机器学习是用大量的数据来“训练”，通过各种算法从数据中学习如何完成任务。</li><li>机器学习传统算法包括决策树学习、推导逻辑规划、聚类、强化学习和贝叶斯网络等等。</li></ul><p><strong>深度学习（Deep Learning）：一种实现机器学习的技术（深度神经网络）</strong></p><ul><li>“深度”指的是一个神经网络中的层的数量。浅层神经网络有一个所谓的隐藏层，而深度神经网络则不止一个隐藏层。多个隐藏层让深度神经网络能够以分层的方式学习数据的特征，因为简单特征可逐层叠加，形成更为复杂的特征。</li><li>深度学习是一种基于对数据进行表征学习的方法。深度学习的好处是用非监督式或半监督式的特征学习和分层特征提取高效算法来替代手工获取特征。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD&amp;%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0&amp;%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB.png?raw=true" alt="人工智能&amp;机器学习&amp;深度学习之间的关系"></li></ul><p><em><font color=OrangeRed>任何一个模型由数据与任务来共同塑造。<br/>复杂的算法 ≤ 简单的学习算法 + 好的训练数据。<br/>当前的深度学习方法属于一个“大数据、小任务范式（big data for small task）”。针对某个特定的任务，如图像识别，设计一个简单的价值函数Loss function，用大量数据训练特定的模型。</font></em></p><h1 id="1-2-机器学习"><a href="#1-2-机器学习" class="headerlink" title="1.2 机器学习"></a><font color=blue>1.2 机器学习</font></h1><h2 id="1-2-1-机器学习定义"><a href="#1-2-1-机器学习定义" class="headerlink" title="1.2.1 机器学习定义"></a><font color=DodgerBlue>1.2.1 机器学习定义</font></h2><p>Arthur Samuel： Machine Learning is the field of study that gives computers the ability to learn without being explicitly programmed (1959)。<br>Tom Mitchell：一个程序被认为能从经验E 中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E 后，经过P 评判，程序在处理T 时的性能有所提升。</p><h2 id="1-2-2-机器学习数据集"><a href="#1-2-2-机器学习数据集" class="headerlink" title="1.2.2 机器学习数据集"></a><font color=DodgerBlue>1.2.2 机器学习数据集</font></h2><ul><li>训练数据集：当机器学习程序开始运行时，使用训练样本集作为算法的输人，完成算法训练。</li><li>测试数据集：训练完成之后输人测试样本。比较测试样本预测的目标变量值与实际样本类别之间的差别，就可以得出算法的实际精确度。</li></ul><h2 id="1-2-3-机器学习类型"><a href="#1-2-3-机器学习类型" class="headerlink" title="1.2.3 机器学习类型"></a><font color=DodgerBlue>1.2.3 机器学习类型</font></h2><p><strong>1. 监督学习： </strong></p><ul><li>工作机制：通过学习被标识的数据集，发现数据集中输入数据与输出数据之间的对应关系；通过这种对应关系，将数据的输入映射到合适的输出。</li><li>常见算法：线性回归、逻辑回归、决策树、kNN、SVM。<br><img src= "/img/loading.gif" data-lazy-src="/img/ai/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0.png?raw=true" alt="监督学习"></li></ul><p><strong>2. 无监督学习： </strong></p><ul><li>工作机制：通过学习未被标识的数据集，发现数据集的一些内在结构或某种规律；通过这种内在结构或某种规律，将数据集自行分类，从而可以标识数据集。</li><li>常见算法：关联算法、聚类（k-means）、降维、推荐系统等。</li></ul><p><strong>3. 半监督式学习： </strong></p><ul><li>工作机制：首先试图对未标识数据进行建模，在此基础上再对标识的数据进行预测。</li><li>特征：综合利用少量有标记的数据和大量没有标记的数据，来生成合适的分类函数。</li></ul><p><strong>4. 增强学习： </strong></p><ul><li>工作机制：机器被放在一个能让它通过反复试错来训练自己的环境中<ul><li>输入数据直接反馈到模型，模型必须对此立刻作出调整。</li><li>不断与环境进行交互，通过试错的方式来获得最佳策略。</li><li>延迟奖惩的学习方式，从过去的经验中进行学习，非直接判断对错。</li></ul></li><li>常见算法：马尔可夫决策过程、Q-Learning、时间差学习。</li></ul><h2 id="1-2-4-机器学习算法选择"><a href="#1-2-4-机器学习算法选择" class="headerlink" title="1.2.4 机器学习算法选择"></a><font color=DodgerBlue>1.2.4 机器学习算法选择</font></h2><p>算法是从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法</p><ol><li><p>明确使用机器学习算法的目的，想要算法完成何种任务：</p><ul><li>若想要预测目标变量的值，则可以选择监督学习算法：需要进一步确定目标变量类型，若目标变量是离散型，则可以选择分类器算法；若目标变量是连续型的数值，则需要选择回归算法。</li><li>若不是预测目标变量的值，可以选择无监督学习算法：进一步分析，若需要将数据是离散的，则使用聚类算法；若需要估计数据与每个分组的相似程度，则需要使用密度估计算法。</li></ul></li><li><p>明确需要分析或收集的数据：对实际数据了解得越充分，越容易创建符合实际需求的应用程序</p><ul><li>特征值是离散型变量还是连续型变量。</li><li>特征值中是否存在缺失的值，何种原因造成缺失值。</li><li>数据中是否存在异常值，某个特征发生的频率如何等。</li></ul></li></ol><h2 id="1-2-5-开发机器学习应用程序的步骤"><a href="#1-2-5-开发机器学习应用程序的步骤" class="headerlink" title="1.2.5 开发机器学习应用程序的步骤"></a><font color=DodgerBlue>1.2.5 开发机器学习应用程序的步骤</font></h2><ol><li>收集并处理数据：使用多种方法获取样本数据，分析数据确保数据集中没有垃圾数据，按算法要求将数据转换成特定的格式。</li><li>训练算法：将格式化数据输入到算法，从中学习数据规律。</li><li>测试算法：对于监督学习预测评估算法的目标变量值，对于无监督学习用其他的评测手段来检验算法的成功率。</li><li>使用算法：执行实际任务，以检验上述步骤是否可以在实际环境中正常工作。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 1. 人工智能与机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
